{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM, Activation\n",
    "from keras.utils import to_categorical\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/18wh70fx\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/18wh70fx</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - 'BAM': 100%|███████████████████████████████████████████████| 186/186 [00:01<00:00, 94.62it/s]\n",
      "Saving vectors of label - 'BBI': 100%|█████████████████████████████████████████████| 1012/1012 [00:15<00:00, 63.87it/s]\n",
      "Saving vectors of label - 'BIN': 100%|███████████████████████████████████████████████| 133/133 [00:01<00:00, 96.49it/s]\n",
      "Saving vectors of label - 'BMA': 100%|████████████████████████████████████████████████| 11/11 [00:00<00:00, 129.71it/s]\n",
      "Saving vectors of label - 'BRA': 100%|████████████████████████████████████████████████| 12/12 [00:00<00:00, 120.30it/s]\n"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.max_len = 21\n",
    "config.buckets = 50\n",
    "\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
    "\n",
    "#labels=np.array([\"chirping_birds\", \"crickets\", \"crow\", \n",
    "#                 \"frog\", \"insects\"])\n",
    "labels=np.array([\"BRA\", \"BAM\", \"BBI\", \n",
    "                 \"BMA\", \"BIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train/test set\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting channels to 1 to generalize stereo sound to 1 channel\n",
    "channels = 1\n",
    "config.epochs = 50\n",
    "config.batch_size = 100\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 5\n",
    "\n",
    "# Reshape X_train and X_test to include a 4th dimension (channels)\n",
    "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len, channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len, channels)\n",
    "X_val = X_val.reshape(X_val.shape[0], config.buckets, config.max_len, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 50, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "# Spectrogram visualized of 0th element\n",
    "print(X_train.shape)\n",
    "#plt.imshow(X_train[500, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vector number where each number corresponds to a label\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_val_hot = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 48, 19, 24)        240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 9, 24)         0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 9, 24)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 22, 7, 48)         10416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 11, 3, 48)         0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 11, 3, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 3, 48)          6960      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 3, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1296)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1296)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                83008     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 100,949\n",
      "Trainable params: 100,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"model.add(Conv2D(32, (3, 3),\\n    input_shape=(config.buckets, config.max_len, channels),\\n    activation='relu'))\\n\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "\n",
    "input_shape= (config.buckets, config.max_len, channels)\n",
    "\n",
    "model.add(Conv2D(24, (3, 3), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (3, 3), padding=\"valid\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (3, 1), padding=\"valid\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(len(labels)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# Conv2D: \n",
    "#    Filters: 32\n",
    "#    Kernel_size: (3,3) (height/width of the 2D convolution window)     \n",
    "'''model.add(Conv2D(32, (3, 3),\n",
    "    input_shape=(config.buckets, config.max_len, channels),\n",
    "    activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CNN for training\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/y8r5mf51\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/y8r5mf51</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 5)\n",
      "(5,)\n",
      "(649, 50, 21, 1)\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 649 samples, validate on 434 samples\n",
      "Epoch 1/50\n",
      "649/649 [==============================] - ETA: 18s - loss: 4.5785 - accuracy: 0.250 - ETA: 9s - loss: 3.7529 - accuracy: 0.328 - ETA: 6s - loss: 3.4751 - accuracy: 0.43 - ETA: 4s - loss: 3.2928 - accuracy: 0.46 - ETA: 3s - loss: 2.9507 - accuracy: 0.52 - ETA: 3s - loss: 2.6885 - accuracy: 0.55 - ETA: 2s - loss: 2.4328 - accuracy: 0.58 - ETA: 2s - loss: 2.3398 - accuracy: 0.58 - ETA: 1s - loss: 2.1312 - accuracy: 0.58 - ETA: 1s - loss: 1.9995 - accuracy: 0.57 - ETA: 0s - loss: 1.9268 - accuracy: 0.57 - ETA: 0s - loss: 1.7802 - accuracy: 0.60 - ETA: 0s - loss: 1.6815 - accuracy: 0.60 - ETA: 0s - loss: 1.6417 - accuracy: 0.60 - ETA: 0s - loss: 1.6113 - accuracy: 0.60 - 2s 3ms/step - loss: 1.5630 - accuracy: 0.6225 - val_loss: 0.7253 - val_accuracy: 0.7350\n",
      "Epoch 2/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.8659 - accuracy: 0.65 - ETA: 1s - loss: 0.8175 - accuracy: 0.70 - ETA: 0s - loss: 0.8903 - accuracy: 0.71 - ETA: 0s - loss: 0.9474 - accuracy: 0.69 - ETA: 0s - loss: 0.9626 - accuracy: 0.68 - ETA: 0s - loss: 0.9002 - accuracy: 0.70 - ETA: 0s - loss: 0.8823 - accuracy: 0.72 - ETA: 0s - loss: 0.8663 - accuracy: 0.73 - ETA: 0s - loss: 0.8366 - accuracy: 0.73 - ETA: 0s - loss: 0.8283 - accuracy: 0.74 - ETA: 0s - loss: 0.8188 - accuracy: 0.74 - ETA: 0s - loss: 0.8163 - accuracy: 0.74 - 1s 2ms/step - loss: 0.8046 - accuracy: 0.7504 - val_loss: 0.6369 - val_accuracy: 0.7949\n",
      "Epoch 3/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.87 - ETA: 0s - loss: 0.4927 - accuracy: 0.81 - ETA: 1s - loss: 0.5692 - accuracy: 0.79 - ETA: 1s - loss: 0.5914 - accuracy: 0.79 - ETA: 1s - loss: 0.6884 - accuracy: 0.77 - ETA: 1s - loss: 0.6685 - accuracy: 0.78 - ETA: 1s - loss: 0.7050 - accuracy: 0.77 - ETA: 1s - loss: 0.7016 - accuracy: 0.78 - ETA: 1s - loss: 0.6733 - accuracy: 0.78 - ETA: 0s - loss: 0.6571 - accuracy: 0.78 - ETA: 0s - loss: 0.6611 - accuracy: 0.78 - ETA: 0s - loss: 0.6477 - accuracy: 0.78 - ETA: 0s - loss: 0.6497 - accuracy: 0.78 - ETA: 0s - loss: 0.6792 - accuracy: 0.78 - ETA: 0s - loss: 0.6864 - accuracy: 0.78 - ETA: 0s - loss: 0.7189 - accuracy: 0.78 - ETA: 0s - loss: 0.7382 - accuracy: 0.78 - ETA: 0s - loss: 0.7399 - accuracy: 0.78 - 2s 2ms/step - loss: 0.7342 - accuracy: 0.7843 - val_loss: 0.5574 - val_accuracy: 0.8433\n",
      "Epoch 4/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.6717 - accuracy: 0.84 - ETA: 1s - loss: 0.7052 - accuracy: 0.76 - ETA: 0s - loss: 0.7503 - accuracy: 0.75 - ETA: 0s - loss: 0.7077 - accuracy: 0.76 - ETA: 0s - loss: 0.6876 - accuracy: 0.76 - ETA: 0s - loss: 0.6479 - accuracy: 0.78 - ETA: 0s - loss: 0.6751 - accuracy: 0.77 - ETA: 0s - loss: 0.6412 - accuracy: 0.78 - ETA: 0s - loss: 0.6124 - accuracy: 0.79 - ETA: 0s - loss: 0.6090 - accuracy: 0.80 - ETA: 0s - loss: 0.6033 - accuracy: 0.80 - ETA: 0s - loss: 0.6043 - accuracy: 0.80 - ETA: 0s - loss: 0.6208 - accuracy: 0.80 - ETA: 0s - loss: 0.5966 - accuracy: 0.81 - ETA: 0s - loss: 0.5869 - accuracy: 0.81 - ETA: 0s - loss: 0.5968 - accuracy: 0.81 - ETA: 0s - loss: 0.6051 - accuracy: 0.81 - ETA: 0s - loss: 0.6010 - accuracy: 0.81 - 2s 3ms/step - loss: 0.5956 - accuracy: 0.8166 - val_loss: 0.5050 - val_accuracy: 0.8410\n",
      "Epoch 5/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.84 - ETA: 1s - loss: 0.5859 - accuracy: 0.82 - ETA: 1s - loss: 0.5940 - accuracy: 0.82 - ETA: 1s - loss: 0.5367 - accuracy: 0.84 - ETA: 0s - loss: 0.5692 - accuracy: 0.84 - ETA: 0s - loss: 0.5643 - accuracy: 0.84 - ETA: 0s - loss: 0.5627 - accuracy: 0.84 - ETA: 0s - loss: 0.5491 - accuracy: 0.85 - ETA: 0s - loss: 0.5490 - accuracy: 0.84 - ETA: 0s - loss: 0.5596 - accuracy: 0.84 - ETA: 0s - loss: 0.5766 - accuracy: 0.82 - ETA: 0s - loss: 0.5584 - accuracy: 0.83 - ETA: 0s - loss: 0.5569 - accuracy: 0.82 - ETA: 0s - loss: 0.5518 - accuracy: 0.83 - ETA: 0s - loss: 0.5511 - accuracy: 0.82 - 1s 2ms/step - loss: 0.5507 - accuracy: 0.8305 - val_loss: 0.4757 - val_accuracy: 0.8456\n",
      "Epoch 6/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.7303 - accuracy: 0.81 - ETA: 1s - loss: 0.7337 - accuracy: 0.79 - ETA: 1s - loss: 0.6324 - accuracy: 0.82 - ETA: 1s - loss: 0.6121 - accuracy: 0.82 - ETA: 1s - loss: 0.5691 - accuracy: 0.83 - ETA: 1s - loss: 0.5424 - accuracy: 0.83 - ETA: 1s - loss: 0.4927 - accuracy: 0.86 - ETA: 0s - loss: 0.4726 - accuracy: 0.86 - ETA: 0s - loss: 0.4848 - accuracy: 0.85 - ETA: 0s - loss: 0.5001 - accuracy: 0.84 - ETA: 0s - loss: 0.4957 - accuracy: 0.84 - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.5230 - accuracy: 0.84 - ETA: 0s - loss: 0.5149 - accuracy: 0.84 - 1s 2ms/step - loss: 0.5129 - accuracy: 0.8444 - val_loss: 0.4841 - val_accuracy: 0.8433\n",
      "Epoch 7/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.75 - ETA: 0s - loss: 0.5079 - accuracy: 0.78 - ETA: 0s - loss: 0.5222 - accuracy: 0.75 - ETA: 1s - loss: 0.4756 - accuracy: 0.78 - ETA: 1s - loss: 0.4594 - accuracy: 0.80 - ETA: 0s - loss: 0.4777 - accuracy: 0.79 - ETA: 0s - loss: 0.4621 - accuracy: 0.79 - ETA: 0s - loss: 0.4779 - accuracy: 0.79 - ETA: 0s - loss: 0.5265 - accuracy: 0.79 - ETA: 0s - loss: 0.5121 - accuracy: 0.80 - ETA: 0s - loss: 0.4991 - accuracy: 0.81 - ETA: 0s - loss: 0.4990 - accuracy: 0.81 - ETA: 0s - loss: 0.4942 - accuracy: 0.81 - ETA: 0s - loss: 0.4876 - accuracy: 0.82 - ETA: 0s - loss: 0.4842 - accuracy: 0.82 - ETA: 0s - loss: 0.4844 - accuracy: 0.82 - ETA: 0s - loss: 0.4791 - accuracy: 0.82 - 1s 2ms/step - loss: 0.4781 - accuracy: 0.8290 - val_loss: 0.4802 - val_accuracy: 0.8456\n",
      "Epoch 8/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.4091 - accuracy: 0.84 - ETA: 1s - loss: 0.4928 - accuracy: 0.79 - ETA: 0s - loss: 0.4339 - accuracy: 0.82 - ETA: 0s - loss: 0.4299 - accuracy: 0.83 - ETA: 0s - loss: 0.4755 - accuracy: 0.82 - ETA: 0s - loss: 0.4744 - accuracy: 0.83 - ETA: 0s - loss: 0.5089 - accuracy: 0.81 - ETA: 0s - loss: 0.4922 - accuracy: 0.82 - ETA: 0s - loss: 0.4962 - accuracy: 0.82 - ETA: 0s - loss: 0.4873 - accuracy: 0.82 - ETA: 0s - loss: 0.4597 - accuracy: 0.83 - ETA: 0s - loss: 0.4597 - accuracy: 0.83 - ETA: 0s - loss: 0.4504 - accuracy: 0.84 - ETA: 0s - loss: 0.4561 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4585 - accuracy: 0.8444 - val_loss: 0.5894 - val_accuracy: 0.8341\n",
      "Epoch 9/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.84 - ETA: 0s - loss: 0.2925 - accuracy: 0.90 - ETA: 0s - loss: 0.3366 - accuracy: 0.88 - ETA: 1s - loss: 0.3470 - accuracy: 0.88 - ETA: 0s - loss: 0.3389 - accuracy: 0.88 - ETA: 0s - loss: 0.3328 - accuracy: 0.88 - ETA: 0s - loss: 0.3836 - accuracy: 0.87 - ETA: 0s - loss: 0.3757 - accuracy: 0.86 - ETA: 0s - loss: 0.3875 - accuracy: 0.86 - ETA: 0s - loss: 0.4103 - accuracy: 0.86 - ETA: 0s - loss: 0.4211 - accuracy: 0.85 - ETA: 0s - loss: 0.4203 - accuracy: 0.86 - ETA: 0s - loss: 0.4237 - accuracy: 0.85 - ETA: 0s - loss: 0.4200 - accuracy: 0.85 - ETA: 0s - loss: 0.4386 - accuracy: 0.84 - ETA: 0s - loss: 0.4468 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4483 - accuracy: 0.8413 - val_loss: 0.4525 - val_accuracy: 0.8456\n",
      "Epoch 10/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.81 - ETA: 1s - loss: 0.4179 - accuracy: 0.84 - ETA: 1s - loss: 0.4159 - accuracy: 0.84 - ETA: 1s - loss: 0.3844 - accuracy: 0.86 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4473 - accuracy: 0.83 - ETA: 0s - loss: 0.4378 - accuracy: 0.84 - ETA: 0s - loss: 0.4244 - accuracy: 0.84 - ETA: 0s - loss: 0.4230 - accuracy: 0.85 - ETA: 0s - loss: 0.4302 - accuracy: 0.85 - ETA: 0s - loss: 0.4255 - accuracy: 0.85 - ETA: 0s - loss: 0.4184 - accuracy: 0.85 - ETA: 0s - loss: 0.4164 - accuracy: 0.85 - ETA: 0s - loss: 0.4170 - accuracy: 0.85 - ETA: 0s - loss: 0.4172 - accuracy: 0.85 - ETA: 0s - loss: 0.4021 - accuracy: 0.86 - 1s 2ms/step - loss: 0.4048 - accuracy: 0.8598 - val_loss: 0.4856 - val_accuracy: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.68 - ETA: 0s - loss: 0.5325 - accuracy: 0.78 - ETA: 0s - loss: 0.5262 - accuracy: 0.78 - ETA: 0s - loss: 0.5029 - accuracy: 0.80 - ETA: 0s - loss: 0.4684 - accuracy: 0.81 - ETA: 0s - loss: 0.4503 - accuracy: 0.82 - ETA: 0s - loss: 0.4202 - accuracy: 0.84 - ETA: 0s - loss: 0.4071 - accuracy: 0.84 - ETA: 0s - loss: 0.4123 - accuracy: 0.84 - ETA: 0s - loss: 0.4001 - accuracy: 0.84 - ETA: 0s - loss: 0.3979 - accuracy: 0.85 - ETA: 0s - loss: 0.4200 - accuracy: 0.84 - ETA: 0s - loss: 0.4283 - accuracy: 0.84 - ETA: 0s - loss: 0.4254 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4225 - accuracy: 0.8444 - val_loss: 0.4427 - val_accuracy: 0.8525\n",
      "Epoch 12/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 1s - loss: 0.2022 - accuracy: 0.96 - ETA: 1s - loss: 0.3082 - accuracy: 0.93 - ETA: 1s - loss: 0.2834 - accuracy: 0.92 - ETA: 0s - loss: 0.3418 - accuracy: 0.89 - ETA: 0s - loss: 0.3851 - accuracy: 0.86 - ETA: 0s - loss: 0.4023 - accuracy: 0.85 - ETA: 0s - loss: 0.3900 - accuracy: 0.86 - ETA: 0s - loss: 0.4120 - accuracy: 0.85 - ETA: 0s - loss: 0.4349 - accuracy: 0.85 - ETA: 0s - loss: 0.4175 - accuracy: 0.85 - ETA: 0s - loss: 0.4192 - accuracy: 0.85 - ETA: 0s - loss: 0.4113 - accuracy: 0.85 - 1s 2ms/step - loss: 0.4072 - accuracy: 0.8598 - val_loss: 0.5036 - val_accuracy: 0.8479\n",
      "Epoch 13/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.2807 - accuracy: 0.90 - ETA: 1s - loss: 0.3285 - accuracy: 0.87 - ETA: 0s - loss: 0.3767 - accuracy: 0.86 - ETA: 0s - loss: 0.3934 - accuracy: 0.85 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - ETA: 0s - loss: 0.3790 - accuracy: 0.87 - ETA: 0s - loss: 0.3939 - accuracy: 0.86 - ETA: 0s - loss: 0.3750 - accuracy: 0.87 - ETA: 0s - loss: 0.3696 - accuracy: 0.87 - ETA: 0s - loss: 0.3675 - accuracy: 0.87 - ETA: 0s - loss: 0.3717 - accuracy: 0.87 - ETA: 0s - loss: 0.3726 - accuracy: 0.87 - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - ETA: 0s - loss: 0.3792 - accuracy: 0.87 - ETA: 0s - loss: 0.3786 - accuracy: 0.87 - ETA: 0s - loss: 0.3806 - accuracy: 0.87 - 1s 2ms/step - loss: 0.3781 - accuracy: 0.8737 - val_loss: 0.4465 - val_accuracy: 0.8664\n",
      "Epoch 14/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.4288 - accuracy: 0.81 - ETA: 0s - loss: 0.3394 - accuracy: 0.84 - ETA: 0s - loss: 0.3669 - accuracy: 0.81 - ETA: 0s - loss: 0.3509 - accuracy: 0.83 - ETA: 0s - loss: 0.3477 - accuracy: 0.85 - ETA: 0s - loss: 0.3419 - accuracy: 0.85 - ETA: 0s - loss: 0.3530 - accuracy: 0.85 - ETA: 0s - loss: 0.3622 - accuracy: 0.84 - ETA: 0s - loss: 0.3400 - accuracy: 0.86 - ETA: 0s - loss: 0.3394 - accuracy: 0.86 - ETA: 0s - loss: 0.3650 - accuracy: 0.86 - ETA: 0s - loss: 0.3556 - accuracy: 0.87 - 1s 1ms/step - loss: 0.3632 - accuracy: 0.8690 - val_loss: 0.4390 - val_accuracy: 0.8641\n",
      "Epoch 15/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.81 - ETA: 0s - loss: 0.3398 - accuracy: 0.85 - ETA: 0s - loss: 0.3541 - accuracy: 0.85 - ETA: 0s - loss: 0.3178 - accuracy: 0.86 - ETA: 0s - loss: 0.3324 - accuracy: 0.86 - ETA: 0s - loss: 0.3403 - accuracy: 0.87 - ETA: 0s - loss: 0.3766 - accuracy: 0.86 - ETA: 0s - loss: 0.4194 - accuracy: 0.85 - ETA: 0s - loss: 0.4123 - accuracy: 0.85 - ETA: 0s - loss: 0.4182 - accuracy: 0.84 - ETA: 0s - loss: 0.4099 - accuracy: 0.84 - ETA: 0s - loss: 0.4362 - accuracy: 0.83 - ETA: 0s - loss: 0.4465 - accuracy: 0.83 - ETA: 0s - loss: 0.4419 - accuracy: 0.83 - ETA: 0s - loss: 0.4352 - accuracy: 0.84 - ETA: 0s - loss: 0.4226 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4214 - accuracy: 0.8459 - val_loss: 0.5638 - val_accuracy: 0.8548\n",
      "Epoch 16/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.90 - ETA: 0s - loss: 0.4549 - accuracy: 0.85 - ETA: 0s - loss: 0.4302 - accuracy: 0.86 - ETA: 0s - loss: 0.3999 - accuracy: 0.88 - ETA: 0s - loss: 0.3849 - accuracy: 0.87 - ETA: 0s - loss: 0.3758 - accuracy: 0.87 - ETA: 0s - loss: 0.3771 - accuracy: 0.86 - ETA: 0s - loss: 0.3626 - accuracy: 0.87 - ETA: 0s - loss: 0.3539 - accuracy: 0.88 - ETA: 0s - loss: 0.3533 - accuracy: 0.87 - ETA: 0s - loss: 0.3434 - accuracy: 0.87 - ETA: 0s - loss: 0.3444 - accuracy: 0.87 - ETA: 0s - loss: 0.3416 - accuracy: 0.87 - ETA: 0s - loss: 0.3446 - accuracy: 0.86 - ETA: 0s - loss: 0.3419 - accuracy: 0.87 - ETA: 0s - loss: 0.3336 - accuracy: 0.87 - 1s 2ms/step - loss: 0.3373 - accuracy: 0.8721 - val_loss: 0.5021 - val_accuracy: 0.8502\n",
      "Epoch 17/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.84 - ETA: 0s - loss: 0.4209 - accuracy: 0.85 - ETA: 0s - loss: 0.3868 - accuracy: 0.86 - ETA: 0s - loss: 0.3899 - accuracy: 0.86 - ETA: 0s - loss: 0.3427 - accuracy: 0.88 - ETA: 0s - loss: 0.3341 - accuracy: 0.88 - ETA: 0s - loss: 0.3191 - accuracy: 0.90 - ETA: 0s - loss: 0.3082 - accuracy: 0.90 - ETA: 0s - loss: 0.3161 - accuracy: 0.89 - ETA: 0s - loss: 0.3159 - accuracy: 0.90 - ETA: 0s - loss: 0.3085 - accuracy: 0.90 - ETA: 0s - loss: 0.3141 - accuracy: 0.89 - ETA: 0s - loss: 0.3144 - accuracy: 0.89 - ETA: 0s - loss: 0.3221 - accuracy: 0.89 - ETA: 0s - loss: 0.3107 - accuracy: 0.89 - 1s 2ms/step - loss: 0.3135 - accuracy: 0.8906 - val_loss: 0.4686 - val_accuracy: 0.8641\n",
      "Epoch 18/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.90 - ETA: 0s - loss: 0.2874 - accuracy: 0.92 - ETA: 0s - loss: 0.2852 - accuracy: 0.90 - ETA: 0s - loss: 0.2846 - accuracy: 0.89 - ETA: 0s - loss: 0.3291 - accuracy: 0.87 - ETA: 0s - loss: 0.3192 - accuracy: 0.87 - ETA: 0s - loss: 0.3440 - accuracy: 0.86 - ETA: 0s - loss: 0.3232 - accuracy: 0.87 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3141 - accuracy: 0.87 - ETA: 0s - loss: 0.3156 - accuracy: 0.87 - ETA: 0s - loss: 0.3037 - accuracy: 0.87 - ETA: 0s - loss: 0.2881 - accuracy: 0.88 - ETA: 0s - loss: 0.2854 - accuracy: 0.88 - ETA: 0s - loss: 0.2830 - accuracy: 0.88 - 1s 2ms/step - loss: 0.2988 - accuracy: 0.8783 - val_loss: 0.4621 - val_accuracy: 0.8502\n",
      "Epoch 19/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.93 - ETA: 0s - loss: 0.2138 - accuracy: 0.93 - ETA: 1s - loss: 0.2213 - accuracy: 0.93 - ETA: 1s - loss: 0.2246 - accuracy: 0.93 - ETA: 1s - loss: 0.2742 - accuracy: 0.90 - ETA: 1s - loss: 0.2610 - accuracy: 0.91 - ETA: 1s - loss: 0.2959 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2859 - accuracy: 0.88 - ETA: 1s - loss: 0.2825 - accuracy: 0.89 - ETA: 1s - loss: 0.2780 - accuracy: 0.89 - ETA: 0s - loss: 0.2761 - accuracy: 0.89 - ETA: 0s - loss: 0.2798 - accuracy: 0.88 - ETA: 0s - loss: 0.2962 - accuracy: 0.88 - ETA: 0s - loss: 0.2956 - accuracy: 0.87 - ETA: 0s - loss: 0.3031 - accuracy: 0.88 - ETA: 0s - loss: 0.3116 - accuracy: 0.87 - 2s 3ms/step - loss: 0.3118 - accuracy: 0.8752 - val_loss: 0.4765 - val_accuracy: 0.8733\n",
      "Epoch 20/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.87 - ETA: 1s - loss: 0.2458 - accuracy: 0.85 - ETA: 1s - loss: 0.2645 - accuracy: 0.85 - ETA: 1s - loss: 0.3058 - accuracy: 0.85 - ETA: 1s - loss: 0.3428 - accuracy: 0.85 - ETA: 1s - loss: 0.3322 - accuracy: 0.85 - ETA: 1s - loss: 0.3257 - accuracy: 0.84 - ETA: 1s - loss: 0.3133 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.84 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3231 - accuracy: 0.86 - ETA: 0s - loss: 0.3056 - accuracy: 0.86 - ETA: 0s - loss: 0.2988 - accuracy: 0.87 - ETA: 0s - loss: 0.2987 - accuracy: 0.88 - ETA: 0s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2959 - accuracy: 0.88 - ETA: 0s - loss: 0.2960 - accuracy: 0.88 - ETA: 0s - loss: 0.3035 - accuracy: 0.88 - 2s 3ms/step - loss: 0.3056 - accuracy: 0.8798 - val_loss: 0.4502 - val_accuracy: 0.8733\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649/649 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.87 - ETA: 0s - loss: 0.3177 - accuracy: 0.84 - ETA: 0s - loss: 0.2751 - accuracy: 0.86 - ETA: 0s - loss: 0.2815 - accuracy: 0.86 - ETA: 0s - loss: 0.2594 - accuracy: 0.87 - ETA: 0s - loss: 0.2449 - accuracy: 0.88 - ETA: 0s - loss: 0.2290 - accuracy: 0.89 - ETA: 0s - loss: 0.2240 - accuracy: 0.90 - ETA: 0s - loss: 0.2231 - accuracy: 0.90 - ETA: 0s - loss: 0.2160 - accuracy: 0.90 - ETA: 0s - loss: 0.2141 - accuracy: 0.90 - ETA: 0s - loss: 0.2290 - accuracy: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2472 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.89 - ETA: 0s - loss: 0.2447 - accuracy: 0.89 - ETA: 0s - loss: 0.2490 - accuracy: 0.89 - 1s 2ms/step - loss: 0.2482 - accuracy: 0.8983 - val_loss: 0.5055 - val_accuracy: 0.8825\n",
      "Epoch 22/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.1187 - accuracy: 0.96 - ETA: 1s - loss: 0.2384 - accuracy: 0.90 - ETA: 1s - loss: 0.2301 - accuracy: 0.90 - ETA: 1s - loss: 0.1995 - accuracy: 0.92 - ETA: 1s - loss: 0.1980 - accuracy: 0.93 - ETA: 0s - loss: 0.2283 - accuracy: 0.91 - ETA: 0s - loss: 0.2145 - accuracy: 0.92 - ETA: 0s - loss: 0.2158 - accuracy: 0.91 - ETA: 0s - loss: 0.2176 - accuracy: 0.91 - ETA: 0s - loss: 0.2171 - accuracy: 0.91 - ETA: 0s - loss: 0.2294 - accuracy: 0.91 - ETA: 0s - loss: 0.2316 - accuracy: 0.91 - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.91 - ETA: 0s - loss: 0.2269 - accuracy: 0.91 - ETA: 0s - loss: 0.2259 - accuracy: 0.91 - ETA: 0s - loss: 0.2290 - accuracy: 0.91 - 2s 3ms/step - loss: 0.2353 - accuracy: 0.9137 - val_loss: 0.4667 - val_accuracy: 0.8710\n",
      "Epoch 23/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.93 - ETA: 0s - loss: 0.1946 - accuracy: 0.95 - ETA: 0s - loss: 0.1731 - accuracy: 0.95 - ETA: 0s - loss: 0.1680 - accuracy: 0.95 - ETA: 0s - loss: 0.1765 - accuracy: 0.93 - ETA: 0s - loss: 0.2049 - accuracy: 0.93 - ETA: 0s - loss: 0.2022 - accuracy: 0.93 - ETA: 0s - loss: 0.1906 - accuracy: 0.93 - ETA: 0s - loss: 0.1827 - accuracy: 0.93 - ETA: 0s - loss: 0.2022 - accuracy: 0.92 - ETA: 0s - loss: 0.2142 - accuracy: 0.91 - ETA: 0s - loss: 0.2058 - accuracy: 0.92 - ETA: 0s - loss: 0.2074 - accuracy: 0.91 - ETA: 0s - loss: 0.2110 - accuracy: 0.91 - ETA: 0s - loss: 0.2212 - accuracy: 0.91 - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9183 - val_loss: 0.5114 - val_accuracy: 0.8756\n",
      "Epoch 24/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.1794 - accuracy: 0.90 - ETA: 0s - loss: 0.2208 - accuracy: 0.91 - ETA: 0s - loss: 0.2081 - accuracy: 0.91 - ETA: 0s - loss: 0.2249 - accuracy: 0.92 - ETA: 0s - loss: 0.2209 - accuracy: 0.92 - ETA: 0s - loss: 0.2036 - accuracy: 0.92 - ETA: 0s - loss: 0.2077 - accuracy: 0.92 - ETA: 0s - loss: 0.2069 - accuracy: 0.91 - ETA: 0s - loss: 0.1888 - accuracy: 0.92 - ETA: 0s - loss: 0.1895 - accuracy: 0.92 - ETA: 0s - loss: 0.1833 - accuracy: 0.93 - ETA: 0s - loss: 0.1969 - accuracy: 0.92 - ETA: 0s - loss: 0.1958 - accuracy: 0.92 - 1s 2ms/step - loss: 0.2019 - accuracy: 0.9199 - val_loss: 0.4775 - val_accuracy: 0.8710\n",
      "Epoch 25/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.87 - ETA: 0s - loss: 0.1811 - accuracy: 0.94 - ETA: 0s - loss: 0.2136 - accuracy: 0.92 - ETA: 0s - loss: 0.2182 - accuracy: 0.93 - ETA: 0s - loss: 0.2101 - accuracy: 0.93 - ETA: 0s - loss: 0.2034 - accuracy: 0.93 - ETA: 0s - loss: 0.2081 - accuracy: 0.93 - ETA: 0s - loss: 0.2104 - accuracy: 0.92 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2271 - accuracy: 0.91 - ETA: 0s - loss: 0.2202 - accuracy: 0.91 - ETA: 0s - loss: 0.2129 - accuracy: 0.92 - ETA: 0s - loss: 0.2006 - accuracy: 0.92 - ETA: 0s - loss: 0.2081 - accuracy: 0.91 - ETA: 0s - loss: 0.2078 - accuracy: 0.92 - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9214 - val_loss: 0.5941 - val_accuracy: 0.8664\n",
      "Epoch 26/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.90 - ETA: 1s - loss: 0.2454 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.88 - ETA: 0s - loss: 0.2656 - accuracy: 0.88 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2663 - accuracy: 0.88 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2548 - accuracy: 0.88 - ETA: 0s - loss: 0.2570 - accuracy: 0.88 - ETA: 0s - loss: 0.2483 - accuracy: 0.88 - ETA: 0s - loss: 0.2462 - accuracy: 0.89 - ETA: 0s - loss: 0.2451 - accuracy: 0.89 - ETA: 0s - loss: 0.2420 - accuracy: 0.89 - ETA: 0s - loss: 0.2368 - accuracy: 0.89 - ETA: 0s - loss: 0.2320 - accuracy: 0.89 - 2s 3ms/step - loss: 0.2291 - accuracy: 0.8983 - val_loss: 0.6538 - val_accuracy: 0.8641\n",
      "Epoch 27/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.93 - ETA: 1s - loss: 0.1587 - accuracy: 0.95 - ETA: 1s - loss: 0.1635 - accuracy: 0.95 - ETA: 1s - loss: 0.1572 - accuracy: 0.95 - ETA: 1s - loss: 0.1509 - accuracy: 0.95 - ETA: 0s - loss: 0.1442 - accuracy: 0.95 - ETA: 0s - loss: 0.1536 - accuracy: 0.94 - ETA: 0s - loss: 0.1850 - accuracy: 0.93 - ETA: 0s - loss: 0.1793 - accuracy: 0.93 - ETA: 0s - loss: 0.1777 - accuracy: 0.93 - ETA: 0s - loss: 0.1771 - accuracy: 0.93 - ETA: 0s - loss: 0.1753 - accuracy: 0.93 - ETA: 0s - loss: 0.1784 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.94 - ETA: 0s - loss: 0.1708 - accuracy: 0.94 - ETA: 0s - loss: 0.1661 - accuracy: 0.94 - ETA: 0s - loss: 0.1764 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1724 - accuracy: 0.9414 - val_loss: 0.5725 - val_accuracy: 0.8710\n",
      "Epoch 28/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.90 - ETA: 1s - loss: 0.2402 - accuracy: 0.93 - ETA: 0s - loss: 0.2111 - accuracy: 0.93 - ETA: 1s - loss: 0.2131 - accuracy: 0.93 - ETA: 0s - loss: 0.2001 - accuracy: 0.93 - ETA: 0s - loss: 0.1892 - accuracy: 0.94 - ETA: 0s - loss: 0.1740 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1608 - accuracy: 0.95 - ETA: 0s - loss: 0.1909 - accuracy: 0.93 - ETA: 0s - loss: 0.1819 - accuracy: 0.93 - ETA: 0s - loss: 0.1729 - accuracy: 0.93 - ETA: 0s - loss: 0.1714 - accuracy: 0.93 - ETA: 0s - loss: 0.1765 - accuracy: 0.93 - ETA: 0s - loss: 0.1703 - accuracy: 0.93 - ETA: 0s - loss: 0.1685 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1739 - accuracy: 0.9322 - val_loss: 0.5412 - val_accuracy: 0.8733\n",
      "Epoch 29/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.2795 - accuracy: 0.81 - ETA: 1s - loss: 0.2447 - accuracy: 0.87 - ETA: 1s - loss: 0.2021 - accuracy: 0.90 - ETA: 0s - loss: 0.1551 - accuracy: 0.93 - ETA: 0s - loss: 0.1509 - accuracy: 0.93 - ETA: 0s - loss: 0.1419 - accuracy: 0.93 - ETA: 0s - loss: 0.1399 - accuracy: 0.94 - ETA: 0s - loss: 0.1448 - accuracy: 0.94 - ETA: 0s - loss: 0.1456 - accuracy: 0.94 - ETA: 0s - loss: 0.1547 - accuracy: 0.93 - ETA: 0s - loss: 0.1478 - accuracy: 0.94 - ETA: 0s - loss: 0.1496 - accuracy: 0.94 - ETA: 0s - loss: 0.1581 - accuracy: 0.93 - ETA: 0s - loss: 0.1555 - accuracy: 0.93 - ETA: 0s - loss: 0.1544 - accuracy: 0.93 - 1s 2ms/step - loss: 0.1537 - accuracy: 0.9353 - val_loss: 0.7098 - val_accuracy: 0.8594\n",
      "Epoch 30/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.90 - ETA: 1s - loss: 0.2429 - accuracy: 0.90 - ETA: 1s - loss: 0.1924 - accuracy: 0.91 - ETA: 1s - loss: 0.2077 - accuracy: 0.91 - ETA: 1s - loss: 0.2070 - accuracy: 0.91 - ETA: 1s - loss: 0.2037 - accuracy: 0.91 - ETA: 0s - loss: 0.1707 - accuracy: 0.92 - ETA: 0s - loss: 0.1633 - accuracy: 0.93 - ETA: 0s - loss: 0.1549 - accuracy: 0.93 - ETA: 0s - loss: 0.1565 - accuracy: 0.93 - ETA: 0s - loss: 0.1480 - accuracy: 0.94 - ETA: 0s - loss: 0.1458 - accuracy: 0.94 - ETA: 0s - loss: 0.1409 - accuracy: 0.94 - ETA: 0s - loss: 0.1378 - accuracy: 0.94 - ETA: 0s - loss: 0.1450 - accuracy: 0.94 - ETA: 0s - loss: 0.1760 - accuracy: 0.92 - 1s 2ms/step - loss: 0.1745 - accuracy: 0.9291 - val_loss: 0.5088 - val_accuracy: 0.8571\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649/649 [==============================] - ETA: 1s - loss: 0.2651 - accuracy: 0.90 - ETA: 0s - loss: 0.2476 - accuracy: 0.90 - ETA: 1s - loss: 0.2378 - accuracy: 0.90 - ETA: 0s - loss: 0.2800 - accuracy: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.89 - ETA: 0s - loss: 0.2510 - accuracy: 0.90 - ETA: 0s - loss: 0.2515 - accuracy: 0.89 - ETA: 0s - loss: 0.2257 - accuracy: 0.90 - ETA: 0s - loss: 0.2191 - accuracy: 0.90 - ETA: 0s - loss: 0.2355 - accuracy: 0.90 - ETA: 0s - loss: 0.2290 - accuracy: 0.90 - ETA: 0s - loss: 0.2122 - accuracy: 0.90 - ETA: 0s - loss: 0.2070 - accuracy: 0.91 - ETA: 0s - loss: 0.2013 - accuracy: 0.91 - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9153 - val_loss: 0.5899 - val_accuracy: 0.8641\n",
      "Epoch 32/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.93 - ETA: 0s - loss: 0.1653 - accuracy: 0.93 - ETA: 0s - loss: 0.1273 - accuracy: 0.95 - ETA: 0s - loss: 0.0969 - accuracy: 0.97 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0861 - accuracy: 0.97 - ETA: 0s - loss: 0.1017 - accuracy: 0.96 - ETA: 0s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.95 - ETA: 0s - loss: 0.1241 - accuracy: 0.95 - ETA: 0s - loss: 0.1191 - accuracy: 0.95 - ETA: 0s - loss: 0.1211 - accuracy: 0.95 - ETA: 0s - loss: 0.1235 - accuracy: 0.95 - ETA: 0s - loss: 0.1327 - accuracy: 0.95 - ETA: 0s - loss: 0.1307 - accuracy: 0.95 - ETA: 0s - loss: 0.1303 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1292 - accuracy: 0.9553 - val_loss: 0.5471 - val_accuracy: 0.8825\n",
      "Epoch 33/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.93 - ETA: 0s - loss: 0.1146 - accuracy: 0.95 - ETA: 0s - loss: 0.1137 - accuracy: 0.95 - ETA: 1s - loss: 0.1269 - accuracy: 0.94 - ETA: 1s - loss: 0.1139 - accuracy: 0.95 - ETA: 0s - loss: 0.1047 - accuracy: 0.95 - ETA: 0s - loss: 0.1050 - accuracy: 0.95 - ETA: 0s - loss: 0.1007 - accuracy: 0.95 - ETA: 0s - loss: 0.1225 - accuracy: 0.95 - ETA: 0s - loss: 0.1227 - accuracy: 0.94 - ETA: 0s - loss: 0.1227 - accuracy: 0.95 - ETA: 0s - loss: 0.1279 - accuracy: 0.94 - ETA: 0s - loss: 0.1378 - accuracy: 0.94 - ETA: 0s - loss: 0.1407 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9492 - val_loss: 0.6552 - val_accuracy: 0.8594\n",
      "Epoch 34/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.2541 - accuracy: 0.90 - ETA: 0s - loss: 0.2018 - accuracy: 0.92 - ETA: 0s - loss: 0.1884 - accuracy: 0.92 - ETA: 0s - loss: 0.1770 - accuracy: 0.93 - ETA: 0s - loss: 0.1635 - accuracy: 0.93 - ETA: 0s - loss: 0.1337 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1418 - accuracy: 0.95 - ETA: 0s - loss: 0.1392 - accuracy: 0.95 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1308 - accuracy: 0.95 - ETA: 0s - loss: 0.1276 - accuracy: 0.96 - ETA: 0s - loss: 0.1240 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9615 - val_loss: 0.6495 - val_accuracy: 0.8641\n",
      "Epoch 35/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 1.00 - ETA: 0s - loss: 0.1027 - accuracy: 0.96 - ETA: 1s - loss: 0.0851 - accuracy: 0.97 - ETA: 0s - loss: 0.0722 - accuracy: 0.98 - ETA: 1s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.0884 - accuracy: 0.97 - ETA: 0s - loss: 0.1050 - accuracy: 0.96 - ETA: 0s - loss: 0.1028 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.97 - ETA: 0s - loss: 0.0931 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0930 - accuracy: 0.96 - ETA: 0s - loss: 0.1013 - accuracy: 0.96 - ETA: 0s - loss: 0.1127 - accuracy: 0.95 - ETA: 0s - loss: 0.1090 - accuracy: 0.96 - ETA: 0s - loss: 0.1060 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1083 - accuracy: 0.9599 - val_loss: 0.6482 - val_accuracy: 0.8687\n",
      "Epoch 36/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.93 - ETA: 0s - loss: 0.0801 - accuracy: 0.95 - ETA: 0s - loss: 0.1354 - accuracy: 0.92 - ETA: 0s - loss: 0.1218 - accuracy: 0.93 - ETA: 0s - loss: 0.1529 - accuracy: 0.91 - ETA: 0s - loss: 0.1651 - accuracy: 0.91 - ETA: 0s - loss: 0.1513 - accuracy: 0.91 - ETA: 0s - loss: 0.1482 - accuracy: 0.92 - ETA: 0s - loss: 0.1346 - accuracy: 0.93 - ETA: 0s - loss: 0.1381 - accuracy: 0.93 - ETA: 0s - loss: 0.1464 - accuracy: 0.93 - ETA: 0s - loss: 0.1472 - accuracy: 0.93 - ETA: 0s - loss: 0.1416 - accuracy: 0.93 - ETA: 0s - loss: 0.1455 - accuracy: 0.93 - ETA: 0s - loss: 0.1355 - accuracy: 0.94 - ETA: 0s - loss: 0.1301 - accuracy: 0.94 - ETA: 0s - loss: 0.1285 - accuracy: 0.94 - ETA: 0s - loss: 0.1264 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9445 - val_loss: 0.6935 - val_accuracy: 0.8594\n",
      "Epoch 37/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 1.00 - ETA: 0s - loss: 0.0606 - accuracy: 0.98 - ETA: 1s - loss: 0.0552 - accuracy: 0.97 - ETA: 1s - loss: 0.0596 - accuracy: 0.98 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0955 - accuracy: 0.96 - ETA: 0s - loss: 0.1082 - accuracy: 0.96 - ETA: 0s - loss: 0.1084 - accuracy: 0.96 - ETA: 0s - loss: 0.1095 - accuracy: 0.96 - ETA: 0s - loss: 0.1184 - accuracy: 0.95 - ETA: 0s - loss: 0.1130 - accuracy: 0.95 - ETA: 0s - loss: 0.1098 - accuracy: 0.95 - ETA: 0s - loss: 0.1110 - accuracy: 0.95 - ETA: 0s - loss: 0.1226 - accuracy: 0.95 - ETA: 0s - loss: 0.1172 - accuracy: 0.96 - ETA: 0s - loss: 0.1142 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1148 - accuracy: 0.9615 - val_loss: 0.6198 - val_accuracy: 0.8710\n",
      "Epoch 38/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 1.00 - ETA: 1s - loss: 0.0558 - accuracy: 1.00 - ETA: 1s - loss: 0.0856 - accuracy: 0.98 - ETA: 1s - loss: 0.0729 - accuracy: 0.99 - ETA: 1s - loss: 0.0734 - accuracy: 0.98 - ETA: 0s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.1166 - accuracy: 0.96 - ETA: 0s - loss: 0.1110 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.96 - ETA: 0s - loss: 0.1019 - accuracy: 0.96 - ETA: 0s - loss: 0.1095 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1089 - accuracy: 0.95 - ETA: 0s - loss: 0.1185 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1188 - accuracy: 0.9507 - val_loss: 0.6679 - val_accuracy: 0.8710\n",
      "Epoch 39/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.96 - ETA: 1s - loss: 0.1262 - accuracy: 0.95 - ETA: 1s - loss: 0.1166 - accuracy: 0.94 - ETA: 0s - loss: 0.1601 - accuracy: 0.95 - ETA: 0s - loss: 0.1658 - accuracy: 0.94 - ETA: 0s - loss: 0.1460 - accuracy: 0.94 - ETA: 0s - loss: 0.1388 - accuracy: 0.94 - ETA: 0s - loss: 0.1313 - accuracy: 0.95 - ETA: 0s - loss: 0.1267 - accuracy: 0.95 - ETA: 0s - loss: 0.1265 - accuracy: 0.95 - ETA: 0s - loss: 0.1216 - accuracy: 0.95 - ETA: 0s - loss: 0.1205 - accuracy: 0.95 - ETA: 0s - loss: 0.1221 - accuracy: 0.95 - ETA: 0s - loss: 0.1208 - accuracy: 0.95 - ETA: 0s - loss: 0.1219 - accuracy: 0.95 - ETA: 0s - loss: 0.1170 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9522 - val_loss: 0.7204 - val_accuracy: 0.8641\n",
      "Epoch 40/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 1s - loss: 0.2143 - accuracy: 0.90 - ETA: 0s - loss: 0.1647 - accuracy: 0.92 - ETA: 0s - loss: 0.1387 - accuracy: 0.93 - ETA: 0s - loss: 0.1354 - accuracy: 0.94 - ETA: 0s - loss: 0.1365 - accuracy: 0.94 - ETA: 0s - loss: 0.1317 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - ETA: 0s - loss: 0.1820 - accuracy: 0.94 - ETA: 0s - loss: 0.1772 - accuracy: 0.94 - ETA: 0s - loss: 0.1781 - accuracy: 0.94 - ETA: 0s - loss: 0.1872 - accuracy: 0.93 - ETA: 0s - loss: 0.1770 - accuracy: 0.94 - ETA: 0s - loss: 0.1763 - accuracy: 0.94 - ETA: 0s - loss: 0.1776 - accuracy: 0.94 - ETA: 0s - loss: 0.1804 - accuracy: 0.93 - ETA: 0s - loss: 0.1764 - accuracy: 0.94 - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9399 - val_loss: 0.5938 - val_accuracy: 0.8687\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649/649 [==============================] - ETA: 1s - loss: 0.0726 - accuracy: 1.00 - ETA: 0s - loss: 0.1056 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.96 - ETA: 1s - loss: 0.1043 - accuracy: 0.95 - ETA: 0s - loss: 0.1051 - accuracy: 0.95 - ETA: 0s - loss: 0.1105 - accuracy: 0.95 - ETA: 0s - loss: 0.1095 - accuracy: 0.95 - ETA: 0s - loss: 0.1195 - accuracy: 0.94 - ETA: 0s - loss: 0.1348 - accuracy: 0.94 - ETA: 0s - loss: 0.1450 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1422 - accuracy: 0.94 - ETA: 0s - loss: 0.1377 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1318 - accuracy: 0.9492 - val_loss: 0.6164 - val_accuracy: 0.8779\n",
      "Epoch 42/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.96 - ETA: 0s - loss: 0.0564 - accuracy: 0.98 - ETA: 0s - loss: 0.0657 - accuracy: 0.98 - ETA: 0s - loss: 0.0611 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0992 - accuracy: 0.96 - ETA: 0s - loss: 0.0943 - accuracy: 0.96 - ETA: 0s - loss: 0.1057 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.96 - ETA: 0s - loss: 0.1083 - accuracy: 0.96 - ETA: 0s - loss: 0.1067 - accuracy: 0.96 - ETA: 0s - loss: 0.1035 - accuracy: 0.96 - ETA: 0s - loss: 0.0966 - accuracy: 0.96 - 1s 2ms/step - loss: 0.0955 - accuracy: 0.9676 - val_loss: 0.7090 - val_accuracy: 0.8802\n",
      "Epoch 43/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.96 - ETA: 0s - loss: 0.0581 - accuracy: 0.96 - ETA: 0s - loss: 0.0814 - accuracy: 0.96 - ETA: 1s - loss: 0.0767 - accuracy: 0.96 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0630 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0641 - accuracy: 0.97 - ETA: 0s - loss: 0.0798 - accuracy: 0.97 - ETA: 0s - loss: 0.0779 - accuracy: 0.97 - ETA: 0s - loss: 0.0774 - accuracy: 0.97 - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9707 - val_loss: 0.6879 - val_accuracy: 0.8756\n",
      "Epoch 44/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.93 - ETA: 0s - loss: 0.0565 - accuracy: 0.96 - ETA: 0s - loss: 0.0654 - accuracy: 0.96 - ETA: 0s - loss: 0.0680 - accuracy: 0.96 - ETA: 0s - loss: 0.0625 - accuracy: 0.96 - ETA: 0s - loss: 0.0614 - accuracy: 0.96 - ETA: 0s - loss: 0.0605 - accuracy: 0.96 - ETA: 0s - loss: 0.0612 - accuracy: 0.96 - ETA: 0s - loss: 0.0773 - accuracy: 0.96 - ETA: 0s - loss: 0.0787 - accuracy: 0.96 - ETA: 0s - loss: 0.0766 - accuracy: 0.96 - ETA: 0s - loss: 0.0758 - accuracy: 0.96 - ETA: 0s - loss: 0.0750 - accuracy: 0.96 - 1s 2ms/step - loss: 0.0724 - accuracy: 0.9676 - val_loss: 0.6215 - val_accuracy: 0.8733\n",
      "Epoch 45/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 1.00 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.99 - ETA: 0s - loss: 0.0567 - accuracy: 0.98 - ETA: 0s - loss: 0.0543 - accuracy: 0.98 - ETA: 0s - loss: 0.0573 - accuracy: 0.98 - ETA: 0s - loss: 0.0557 - accuracy: 0.98 - ETA: 0s - loss: 0.0675 - accuracy: 0.98 - ETA: 0s - loss: 0.0701 - accuracy: 0.98 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0632 - accuracy: 0.9846 - val_loss: 0.8091 - val_accuracy: 0.8802\n",
      "Epoch 46/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.96 - ETA: 1s - loss: 0.0486 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0711 - accuracy: 0.97 - ETA: 1s - loss: 0.0681 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0855 - accuracy: 0.96 - ETA: 0s - loss: 0.0829 - accuracy: 0.96 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0695 - accuracy: 0.97 - ETA: 0s - loss: 0.0693 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0825 - accuracy: 0.97 - ETA: 0s - loss: 0.0753 - accuracy: 0.98 - ETA: 0s - loss: 0.0741 - accuracy: 0.98 - ETA: 0s - loss: 0.0718 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0709 - accuracy: 0.9815 - val_loss: 0.7007 - val_accuracy: 0.8802\n",
      "Epoch 47/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.0128 - accuracy: 1.00 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.99 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0494 - accuracy: 0.98 - ETA: 0s - loss: 0.0518 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0557 - accuracy: 0.98 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.8195 - val_accuracy: 0.8802\n",
      "Epoch 48/50\n",
      "649/649 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.96 - ETA: 1s - loss: 0.1126 - accuracy: 0.95 - ETA: 0s - loss: 0.0792 - accuracy: 0.96 - ETA: 0s - loss: 0.0835 - accuracy: 0.96 - ETA: 0s - loss: 0.0729 - accuracy: 0.97 - ETA: 0s - loss: 0.0645 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.98 - ETA: 0s - loss: 0.0632 - accuracy: 0.97 - ETA: 0s - loss: 0.0763 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0679 - accuracy: 0.97 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - 1s 1ms/step - loss: 0.0641 - accuracy: 0.9769 - val_loss: 0.6818 - val_accuracy: 0.8710\n",
      "Epoch 49/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.3168 - accuracy: 0.93 - ETA: 1s - loss: 0.1705 - accuracy: 0.96 - ETA: 0s - loss: 0.1010 - accuracy: 0.97 - ETA: 0s - loss: 0.0860 - accuracy: 0.98 - ETA: 0s - loss: 0.0812 - accuracy: 0.98 - ETA: 0s - loss: 0.0770 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.98 - ETA: 0s - loss: 0.0654 - accuracy: 0.98 - ETA: 0s - loss: 0.0636 - accuracy: 0.98 - ETA: 0s - loss: 0.0616 - accuracy: 0.98 - ETA: 0s - loss: 0.0633 - accuracy: 0.98 - ETA: 0s - loss: 0.0608 - accuracy: 0.98 - ETA: 0s - loss: 0.0611 - accuracy: 0.98 - ETA: 0s - loss: 0.0614 - accuracy: 0.98 - ETA: 0s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0576 - accuracy: 0.98 - ETA: 0s - loss: 0.0555 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.8624 - val_accuracy: 0.8802\n",
      "Epoch 50/50\n",
      "649/649 [==============================] - ETA: 1s - loss: 0.0273 - accuracy: 1.00 - ETA: 0s - loss: 0.0761 - accuracy: 0.96 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0525 - accuracy: 0.97 - ETA: 0s - loss: 0.0529 - accuracy: 0.97 - ETA: 0s - loss: 0.0592 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0592 - accuracy: 0.97 - ETA: 0s - loss: 0.0534 - accuracy: 0.97 - ETA: 0s - loss: 0.0531 - accuracy: 0.97 - ETA: 0s - loss: 0.0514 - accuracy: 0.97 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0570 - accuracy: 0.97 - ETA: 0s - loss: 0.0610 - accuracy: 0.97 - 1s 2ms/step - loss: 0.0602 - accuracy: 0.9769 - val_loss: 0.7308 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c6d6201688>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "print(y_train_hot.shape)\n",
    "print(labels.shape)\n",
    "print(X_train.shape)\n",
    "# Train the CNN model\n",
    "#    X_train: Input data\n",
    "#    y_train_hot: Target data\n",
    "model.fit(X_train, y_train_hot, epochs=config.epochs, validation_data=(X_val, y_val_hot), callbacks=[WandbCallback(data_type=\"image\", labels=labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the keras model\n",
    "model.save(\"bio_cnn_model.h5\")\n",
    "print(\"Model has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the IntelliChirp Biophony CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model('ant_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 48, 19, 24)        240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 9, 24)         0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 9, 24)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 22, 7, 48)         10416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 11, 3, 48)         0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 11, 3, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 3, 48)          6960      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 3, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1296)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1296)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                83008     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 100,949\n",
      "Trainable params: 100,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 28   4   0   0   0]\n",
      " [  0 202   5   0   0]\n",
      " [  0  10  13   0   0]\n",
      " [  0   0   1   0   0]\n",
      " [  0   7   1   0   0]]\n",
      "Accuracy for class BRA : [0.875]\n",
      "Accuracy for class BAM : [0.97584541]\n",
      "Accuracy for class BBI : [0.56521739]\n",
      "Accuracy for class BMA : [0.]\n",
      "Accuracy for class BIN : [0.]\n",
      "Overall Accuracy : 0.8966789667896679\n"
     ]
    }
   ],
   "source": [
    "y_pred_ohe = loaded_model.predict(X_test)  # shape=(n_samples, 12)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_labels)  # shape\n",
    "print(confusion_matrix)\n",
    "\n",
    "for class_i in range(len(labels)) :\n",
    "    indices = np.argwhere(y_test == class_i)\n",
    "    sum = 0\n",
    "    for index in indices:\n",
    "        sum += (y_test[index] == y_pred_labels[index])\n",
    "    if(len(indices) > 0) : mean = sum/len(indices)\n",
    "    else : mean = \"N/A\"\n",
    "    print(\"Accuracy for class\", labels[class_i], \":\", mean)\n",
    "\n",
    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[ 0.0000000e+00  1.5258789e-05  0.0000000e+00 ...  3.3020020e-02\n",
      "  1.2680054e-02 -8.7432861e-03]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000002\n",
      "\n",
      " BAM :  0.99999845\n",
      "\n",
      " BBI :  0.00000156\n",
      "\n",
      " BMA :  0.00000001\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[1. 2.]\n",
      "[-0.03717041 -0.05769348 -0.06455994 ...  0.01766968  0.01895142\n",
      "  0.01779175]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000011\n",
      "\n",
      " BAM :  0.65600646\n",
      "\n",
      " BBI :  0.34398925\n",
      "\n",
      " BMA :  0.00000400\n",
      "\n",
      " BIN :  0.00000018\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[2. 3.]\n",
      "[ 0.02345276  0.02101135  0.01712036 ... -0.01161194 -0.0141449\n",
      " -0.01431274]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000021\n",
      "\n",
      " BAM :  0.99949670\n",
      "\n",
      " BBI :  0.00050283\n",
      "\n",
      " BMA :  0.00000016\n",
      "\n",
      " BIN :  0.00000001\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[3. 4.]\n",
      "[-0.01583862 -0.01066589 -0.00762939 ... -0.0377655  -0.03556824\n",
      " -0.02685547]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00034440\n",
      "\n",
      " BAM :  0.99944335\n",
      "\n",
      " BBI :  0.00012089\n",
      "\n",
      " BMA :  0.00007492\n",
      "\n",
      " BIN :  0.00001626\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[4. 5.]\n",
      "[-0.02836609 -0.02510071 -0.02012634 ...  0.0138855  -0.00386047\n",
      " -0.00904846]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[5. 6.]\n",
      "[-0.00526428  0.00822449  0.01951599 ...  0.02729797  0.02156067\n",
      "  0.01234436]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00185306\n",
      "\n",
      " BAM :  0.97584927\n",
      "\n",
      " BBI :  0.00809248\n",
      "\n",
      " BMA :  0.01294072\n",
      "\n",
      " BIN :  0.00126453\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[6. 7.]\n",
      "[ 0.00544739  0.00053406  0.00970459 ... -0.02848816 -0.01611328\n",
      " -0.01091003]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000024\n",
      "\n",
      " BAM :  0.99988234\n",
      "\n",
      " BBI :  0.00011659\n",
      "\n",
      " BMA :  0.00000065\n",
      "\n",
      " BIN :  0.00000008\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[7. 8.]\n",
      "[-0.0177002  -0.02372742 -0.02700806 ... -0.04304504 -0.04063416\n",
      " -0.03363037]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000038\n",
      "\n",
      " BAM :  0.99999917\n",
      "\n",
      " BBI :  0.00000042\n",
      "\n",
      " BMA :  0.00000002\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[8. 9.]\n",
      "[-0.01539612 -0.00108337  0.00718689 ...  0.01161194  0.01818848\n",
      "  0.02700806]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00001630\n",
      "\n",
      " BAM :  0.99997389\n",
      "\n",
      " BBI :  0.00000005\n",
      "\n",
      " BMA :  0.00000962\n",
      "\n",
      " BIN :  0.00000009\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[ 9. 10.]\n",
      "[ 0.03549194  0.04856873  0.05519104 ... -0.02171326 -0.03634644\n",
      " -0.03912354]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000001\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000001\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[10. 11.]\n",
      "[-0.02934265 -0.0115509   0.00445557 ... -0.03616333 -0.03759766\n",
      " -0.0304718 ]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000013\n",
      "\n",
      " BAM :  0.99996102\n",
      "\n",
      " BBI :  0.00003862\n",
      "\n",
      " BMA :  0.00000027\n",
      "\n",
      " BIN :  0.00000001\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[11. 12.]\n",
      "[-0.03358459 -0.03901672 -0.03933716 ... -0.02337646 -0.02124023\n",
      " -0.02107239]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000306\n",
      "\n",
      " BAM :  0.99999297\n",
      "\n",
      " BBI :  0.00000327\n",
      "\n",
      " BMA :  0.00000053\n",
      "\n",
      " BIN :  0.00000006\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[12. 13.]\n",
      "[-0.00846863  0.00444031  0.00852966 ... -0.00604248 -0.00845337\n",
      " -0.00497437]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00555177\n",
      "\n",
      " BAM :  0.99345279\n",
      "\n",
      " BBI :  0.00021171\n",
      "\n",
      " BMA :  0.00052347\n",
      "\n",
      " BIN :  0.00026016\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[13. 14.]\n",
      "[-0.00427246 -0.00718689 -0.00811768 ... -0.01966858 -0.01296997\n",
      " -0.01628113]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00228493\n",
      "\n",
      " BAM :  0.62795508\n",
      "\n",
      " BBI :  0.34584466\n",
      "\n",
      " BMA :  0.02155693\n",
      "\n",
      " BIN :  0.00235841\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[14. 15.]\n",
      "[-0.02262878 -0.01573181 -0.00117493 ... -0.08956909 -0.0695343\n",
      " -0.04067993]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00019938\n",
      "\n",
      " BAM :  0.97783411\n",
      "\n",
      " BBI :  0.02189673\n",
      "\n",
      " BMA :  0.00006556\n",
      "\n",
      " BIN :  0.00000426\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[15. 16.]\n",
      "[-0.02532959 -0.01031494 -0.00280762 ... -0.07128906 -0.07106018\n",
      " -0.05839539]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000299\n",
      "\n",
      " BAM :  0.99999702\n",
      "\n",
      " BBI :  0.00000001\n",
      "\n",
      " BMA :  0.00000001\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[16. 17.]\n",
      "[-0.04600525 -0.02149963  0.00523376 ... -0.02526855 -0.02735901\n",
      " -0.03106689]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000888\n",
      "\n",
      " BAM :  0.99997985\n",
      "\n",
      " BBI :  0.00001121\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[17. 18.]\n",
      "[-0.02043152 -0.01174927 -0.02088928 ...  0.10055542  0.08653259\n",
      "  0.06604004]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000032\n",
      "\n",
      " BAM :  0.99999928\n",
      "\n",
      " BBI :  0.00000033\n",
      "\n",
      " BMA :  0.00000004\n",
      "\n",
      " BIN :  0.00000002\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[18. 19.]\n",
      "[ 0.04153442  0.01223755 -0.00654602 ...  0.03269958  0.02374268\n",
      "  0.02774048]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000001\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[19. 20.]\n",
      "[0.02185059 0.02069092 0.01451111 ... 0.03469849 0.03985596 0.04600525]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00007026\n",
      "\n",
      " BAM :  0.99992979\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000006\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[20. 21.]\n",
      "[ 0.0353241   0.01567078 -0.00102234 ...  0.1058197   0.10365295\n",
      "  0.09759521]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000927\n",
      "\n",
      " BAM :  0.99992955\n",
      "\n",
      " BBI :  0.00005708\n",
      "\n",
      " BMA :  0.00000321\n",
      "\n",
      " BIN :  0.00000085\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[21. 22.]\n",
      "[ 0.09413147  0.07905579  0.05625916 ... -0.01145935 -0.00245667\n",
      "  0.00479126]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00001921\n",
      "\n",
      " BAM :  0.99991000\n",
      "\n",
      " BBI :  0.00007006\n",
      "\n",
      " BMA :  0.00000078\n",
      "\n",
      " BIN :  0.00000001\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[22. 23.]\n",
      "[ 0.0037384   0.01168823  0.01628113 ... -0.03440857 -0.05511475\n",
      " -0.08209229]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[23. 24.]\n",
      "[-0.1026001  -0.12590027 -0.14944458 ...  0.03462219  0.02537537\n",
      "  0.02354431]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[24. 25.]\n",
      "[ 0.0196991   0.02836609  0.03103638 ... -0.03009033 -0.03392029\n",
      " -0.03681946]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[25. 26.]\n",
      "[-0.04151917 -0.03933716 -0.03703308 ...  0.05451965  0.0519104\n",
      "  0.05206299]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000080\n",
      "\n",
      " BAM :  0.99998569\n",
      "\n",
      " BBI :  0.00001264\n",
      "\n",
      " BMA :  0.00000066\n",
      "\n",
      " BIN :  0.00000009\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[26. 27.]\n",
      "[ 0.05670166  0.06253052  0.07643127 ... -0.00396729  0.00715637\n",
      "  0.00585938]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.01487411\n",
      "\n",
      " BAM :  0.51351196\n",
      "\n",
      " BBI :  0.44204229\n",
      "\n",
      " BMA :  0.01676478\n",
      "\n",
      " BIN :  0.01280690\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[27. 28.]\n",
      "[-0.00222778 -0.01303101 -0.02310181 ...  0.01165771  0.01649475\n",
      "  0.0194397 ]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000007\n",
      "\n",
      " BAM :  0.99999762\n",
      "\n",
      " BBI :  0.00000168\n",
      "\n",
      " BMA :  0.00000055\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[28. 29.]\n",
      "[0.01657104 0.01519775 0.00924683 ... 0.03746033 0.03282166 0.02775574]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[29. 30.]\n",
      "[ 0.01919556  0.0135498   0.01724243 ... -0.00575256 -0.01502991\n",
      " -0.02742004]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000002\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000004\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[30. 31.]\n",
      "[-0.0322876  -0.0365448  -0.03544617 ... -0.0218811  -0.02978516\n",
      " -0.04052734]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00162166\n",
      "\n",
      " BAM :  0.99825948\n",
      "\n",
      " BBI :  0.00002736\n",
      "\n",
      " BMA :  0.00007135\n",
      "\n",
      " BIN :  0.00002006\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[31. 32.]\n",
      "[-0.04328918 -0.03413391 -0.03421021 ...  0.05908203  0.06370544\n",
      "  0.05949402]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00003307\n",
      "\n",
      " BAM :  0.99972826\n",
      "\n",
      " BBI :  0.00022972\n",
      "\n",
      " BMA :  0.00000743\n",
      "\n",
      " BIN :  0.00000154\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[32. 33.]\n",
      "[ 0.06063843  0.06056213  0.06610107 ... -0.12741089 -0.13371277\n",
      " -0.12313843]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000000\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000000\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[33. 34.]\n",
      "[-0.09968567 -0.06376648 -0.03105164 ... -0.0138092  -0.01574707\n",
      " -0.01896667]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000002\n",
      "\n",
      " BAM :  1.00000000\n",
      "\n",
      " BBI :  0.00000001\n",
      "\n",
      " BMA :  0.00000000\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[34. 35.]\n",
      "[-0.00811768  0.00149536  0.00953674 ... -0.004776   -0.0010376\n",
      "  0.00231934]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00000147\n",
      "\n",
      " BAM :  0.99999702\n",
      "\n",
      " BBI :  0.00000150\n",
      "\n",
      " BMA :  0.00000001\n",
      "\n",
      " BIN :  0.00000000\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[35. 36.]\n",
      "[ 0.00238037  0.00236511  0.00231934 ... -0.00193787  0.0068512\n",
      "  0.00695801]\n",
      "(50, 21)\n",
      "PREDICTED VALUES\n",
      "\n",
      " BRA :  0.00021959\n",
      "\n",
      " BAM :  0.99587798\n",
      "\n",
      " BBI :  0.00373552\n",
      "\n",
      " BMA :  0.00012462\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BIN :  0.00004231\n",
      "\n",
      "\n",
      "GUESS:  BAM\n",
      "[{'class': 'BAM', 'timestamp': 0}, {'class': 'BAM', 'timestamp': 1}, {'class': 'BAM', 'timestamp': 2}, {'class': 'BAM', 'timestamp': 3}, {'class': 'BAM', 'timestamp': 4}, {'class': 'BAM', 'timestamp': 5}, {'class': 'BAM', 'timestamp': 6}, {'class': 'BAM', 'timestamp': 7}, {'class': 'BAM', 'timestamp': 8}, {'class': 'BAM', 'timestamp': 9}, {'class': 'BAM', 'timestamp': 10}, {'class': 'BAM', 'timestamp': 11}, {'class': 'BAM', 'timestamp': 12}, {'class': 'BAM', 'timestamp': 13}, {'class': 'BAM', 'timestamp': 14}, {'class': 'BAM', 'timestamp': 15}, {'class': 'BAM', 'timestamp': 16}, {'class': 'BAM', 'timestamp': 17}, {'class': 'BAM', 'timestamp': 18}, {'class': 'BAM', 'timestamp': 19}, {'class': 'BAM', 'timestamp': 20}, {'class': 'BAM', 'timestamp': 21}, {'class': 'BAM', 'timestamp': 22}, {'class': 'BAM', 'timestamp': 23}, {'class': 'BAM', 'timestamp': 24}, {'class': 'BAM', 'timestamp': 25}, {'class': 'BAM', 'timestamp': 26}, {'class': 'BAM', 'timestamp': 27}, {'class': 'BAM', 'timestamp': 28}, {'class': 'BAM', 'timestamp': 29}, {'class': 'BAM', 'timestamp': 30}, {'class': 'BAM', 'timestamp': 31}, {'class': 'BAM', 'timestamp': 32}, {'class': 'BAM', 'timestamp': 33}, {'class': 'BAM', 'timestamp': 34}, {'class': 'BAM', 'timestamp': 35}]\n"
     ]
    }
   ],
   "source": [
    "## Running the model\n",
    "\n",
    "n_mfcc = config.buckets\n",
    "max_len = config.max_len\n",
    "# convert file to wav2mfcc\n",
    "# Mel-frequency cepstral coefficients\n",
    "file_path = \"./prediction/nature_sc.wav\"\n",
    "big_wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "#print(wave.shape, sr)\n",
    "\n",
    "classification = []\n",
    "\n",
    "for sec_index in range( int(big_wave.shape[0] / sr) ) :\n",
    "    start_sec = sec_index\n",
    "    end_sec = sec_index + 1\n",
    "    \n",
    "    sec_to_trim = np.array( [ float(start_sec), float(end_sec) ] )\n",
    "    print(sec_to_trim)\n",
    "    sec_to_trim = np.ceil( sec_to_trim * sr )\n",
    "\n",
    "    wave = big_wave[int(sec_to_trim[0]) : int(sec_to_trim[1])]\n",
    "    print(wave)\n",
    "\n",
    "    wave = np.asfortranarray(wave[::3])\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=n_mfcc)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "\n",
    "    # Convert wav to MFCC\n",
    "    prediction_data = wav2mfcc('./prediction/nature_sc.wav')\n",
    "    prediction_data = mfcc\n",
    "    print(prediction_data.shape)\n",
    "    #print(wav2mfcc())\n",
    "    # Reshape to 4 dimensions\n",
    "    prediction_data = prediction_data.reshape(1, config.buckets, config.max_len, channels)\n",
    "    #prediction_data = prediction_data.reshape(1, 20, config.max_len, channels)\n",
    "\n",
    "    # Run the model on the inputted file\n",
    "    predicted = loaded_model.predict(prediction_data)\n",
    "\n",
    "    # Output the prediction values for each class\n",
    "    print ('PREDICTED VALUES')\n",
    "    labels_indices = range(len(labels))\n",
    "    max_value = 0\n",
    "    max_value_index = 0\n",
    "    for index in labels_indices:\n",
    "        print('\\n', labels[index], \": \", '%.08f' % predicted[0,index])\n",
    "        if predicted[0,index] > max_value:\n",
    "            max_value_index = index\n",
    "            max_value = predicted[0,index]\n",
    "\n",
    "    # Output the prediction\n",
    "    if max_value < 0.5:\n",
    "        print(\"GUESS: Nothing\")\n",
    "        classification.append( { \"class\" : \"Nothing\", \"timestamp\" : start_sec } )\n",
    "    else:\n",
    "        print('\\n\\nGUESS: ', labels[max_value_index])\n",
    "        classification.append( { \"class\" : labels[max_value_index], \"timestamp\" : start_sec } )\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
