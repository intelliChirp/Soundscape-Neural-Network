{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM, Activation\n",
    "from keras.utils import to_categorical\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/15ugcco4\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/15ugcco4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - 'GOC': 100%|█████████████████████████████████████████████████| 91/91 [00:01<00:00, 68.76it/s]\n",
      "Saving vectors of label - 'GRA': 100%|███████████████████████████████████████████████| 623/623 [00:06<00:00, 93.69it/s]\n",
      "Saving vectors of label - 'GST': 100%|█████████████████████████████████████████████████| 85/85 [00:01<00:00, 80.33it/s]\n",
      "Saving vectors of label - 'GWC': 100%|███████████████████████████████████████████████| 464/464 [00:05<00:00, 78.34it/s]\n",
      "Saving vectors of label - 'GWG': 100%|██████████████████████████████████████████████| 203/203 [00:02<00:00, 101.32it/s]\n"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.max_len = 32\n",
    "config.buckets = 128\n",
    "\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
    "\n",
    "#labels=np.array([\"chirping_birds\", \"crickets\", \"crow\", \n",
    "#                 \"frog\", \"insects\"])\n",
    "labels=np.array([\"GOC\", \"GRA\", \"GST\", \n",
    "                 \"GWG\", \"GWC\", \"OPI\", \"OQU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train/test set\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 128, 32)\n"
     ]
    }
   ],
   "source": [
    "# Setting channels to 1 to generalize stereo sound to 1 channel\n",
    "channels = 1\n",
    "config.epochs = 20\n",
    "config.batch_size = 100\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 5\n",
    "print(X_train.shape)\n",
    "# Reshape X_train and X_test to include a 4th dimension (channels)\n",
    "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len, channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len, channels)\n",
    "X_val = X_val.reshape(X_val.shape[0], config.buckets, config.max_len, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#111111', '#222222', '#333333', '#444444', '#555555', '#666666', '#777777', '#888888']\n",
    "    markers = ['o', 's', 'o', 's','o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "def run_sampler( X, y, sampler ) :\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_samples, _, _, _ = X.shape\n",
    "\n",
    "    d2_X = X.reshape((X_samples,config.buckets*config.max_len*channels))\n",
    "\n",
    "    X_s, y_s = sampler.fit_sample(d2_X, y)\n",
    "\n",
    "    #plot_2d_space(X_rus, y_rus, 'Random under-sampling')\n",
    "\n",
    "    X_s = X_s.reshape((X_s.shape[0], config.buckets, config.max_len, channels))\n",
    "    print(\"X_s\", X_s.shape)\n",
    "    print(\"Y_s\", y_s.shape)\n",
    "    \n",
    "    return X_s, y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smt = RandomUnderSampler()\n",
    "#sampler = RandomOverSampler()\n",
    "smt = SMOTETomek()\n",
    "\n",
    "X_train, y_train = run_sampler( X_train, y_train, smt )\n",
    "X_test, y_test = run_sampler( X_test, y_test, smt )\n",
    "X_val, y_val = run_sampler( X_val, y_val, smt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 128, 32, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dd77bd2b48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spectrogram visualized of 0th element\n",
    "print(X_train.shape)\n",
    "plt.imshow(X_train[10, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vector number where each number corresponds to a label\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_val_hot = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                516160    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 534,101\n",
      "Trainable params: 534,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"model.add(Conv2D(32, (3, 3),\\n    input_shape=(config.buckets, config.max_len, channels),\\n    activation='relu'))\\n\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "\n",
    "input_shape= (config.buckets, config.max_len, channels)\n",
    "\n",
    "model.add(Conv2D(24, (3, 3), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (3, 3), padding=\"valid\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (3, 1), padding=\"valid\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(len(labels)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# Conv2D: \n",
    "#    Filters: 32\n",
    "#    Kernel_size: (3,3) (height/width of the 2D convolution window)     \n",
    "'''model.add(Conv2D(32, (3, 3),\n",
    "    input_shape=(config.buckets, config.max_len, channels),\n",
    "    activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CNN for training\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2fhx4pil\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2fhx4pil</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 5)\n",
      "(5,)\n",
      "(703, 128, 32, 1)\n",
      "Train on 703 samples, validate on 469 samples\n",
      "Epoch 1/20\n",
      "703/703 [==============================] - ETA: 31s - loss: 2.6549 - accuracy: 0.156 - ETA: 15s - loss: 2.9842 - accuracy: 0.296 - ETA: 10s - loss: 3.0744 - accuracy: 0.270 - ETA: 8s - loss: 2.7303 - accuracy: 0.273 - ETA: 6s - loss: 2.4843 - accuracy: 0.30 - ETA: 5s - loss: 2.3492 - accuracy: 0.29 - ETA: 4s - loss: 2.2642 - accuracy: 0.29 - ETA: 4s - loss: 2.1689 - accuracy: 0.30 - ETA: 3s - loss: 2.1129 - accuracy: 0.29 - ETA: 3s - loss: 2.0378 - accuracy: 0.31 - ETA: 2s - loss: 1.9852 - accuracy: 0.31 - ETA: 2s - loss: 1.9340 - accuracy: 0.33 - ETA: 2s - loss: 1.9016 - accuracy: 0.33 - ETA: 1s - loss: 1.8562 - accuracy: 0.34 - ETA: 1s - loss: 1.8084 - accuracy: 0.36 - ETA: 1s - loss: 1.7684 - accuracy: 0.36 - ETA: 1s - loss: 1.7591 - accuracy: 0.37 - ETA: 0s - loss: 1.7334 - accuracy: 0.37 - ETA: 0s - loss: 1.7094 - accuracy: 0.38 - ETA: 0s - loss: 1.6844 - accuracy: 0.39 - ETA: 0s - loss: 1.6595 - accuracy: 0.40 - 5s 7ms/step - loss: 1.6474 - accuracy: 0.4040 - val_loss: 1.1659 - val_accuracy: 0.5714\n",
      "Epoch 2/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 1.1689 - accuracy: 0.53 - ETA: 1s - loss: 1.1853 - accuracy: 0.53 - ETA: 1s - loss: 1.2146 - accuracy: 0.53 - ETA: 1s - loss: 1.1864 - accuracy: 0.54 - ETA: 1s - loss: 1.1878 - accuracy: 0.53 - ETA: 1s - loss: 1.1766 - accuracy: 0.54 - ETA: 1s - loss: 1.1397 - accuracy: 0.54 - ETA: 1s - loss: 1.0926 - accuracy: 0.57 - ETA: 1s - loss: 1.1273 - accuracy: 0.55 - ETA: 1s - loss: 1.1230 - accuracy: 0.56 - ETA: 1s - loss: 1.1209 - accuracy: 0.55 - ETA: 1s - loss: 1.1409 - accuracy: 0.54 - ETA: 1s - loss: 1.1405 - accuracy: 0.55 - ETA: 0s - loss: 1.1359 - accuracy: 0.54 - ETA: 0s - loss: 1.1361 - accuracy: 0.54 - ETA: 0s - loss: 1.1366 - accuracy: 0.53 - ETA: 0s - loss: 1.1285 - accuracy: 0.54 - ETA: 0s - loss: 1.1155 - accuracy: 0.54 - ETA: 0s - loss: 1.1064 - accuracy: 0.54 - ETA: 0s - loss: 1.1103 - accuracy: 0.54 - ETA: 0s - loss: 1.1088 - accuracy: 0.54 - 3s 4ms/step - loss: 1.1050 - accuracy: 0.5505 - val_loss: 0.8259 - val_accuracy: 0.6951\n",
      "Epoch 3/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 1.1657 - accuracy: 0.62 - ETA: 1s - loss: 1.0297 - accuracy: 0.65 - ETA: 2s - loss: 0.9726 - accuracy: 0.63 - ETA: 2s - loss: 0.9639 - accuracy: 0.64 - ETA: 2s - loss: 0.8999 - accuracy: 0.67 - ETA: 1s - loss: 0.8855 - accuracy: 0.69 - ETA: 1s - loss: 0.8684 - accuracy: 0.69 - ETA: 1s - loss: 0.8930 - accuracy: 0.68 - ETA: 1s - loss: 0.8792 - accuracy: 0.69 - ETA: 1s - loss: 0.8761 - accuracy: 0.68 - ETA: 1s - loss: 0.9006 - accuracy: 0.66 - ETA: 1s - loss: 0.9117 - accuracy: 0.65 - ETA: 1s - loss: 0.9005 - accuracy: 0.65 - ETA: 0s - loss: 0.9088 - accuracy: 0.65 - ETA: 0s - loss: 0.9208 - accuracy: 0.64 - ETA: 0s - loss: 0.9158 - accuracy: 0.63 - ETA: 0s - loss: 0.9094 - accuracy: 0.64 - ETA: 0s - loss: 0.9108 - accuracy: 0.63 - ETA: 0s - loss: 0.9111 - accuracy: 0.63 - ETA: 0s - loss: 0.8972 - accuracy: 0.64 - ETA: 0s - loss: 0.8848 - accuracy: 0.65 - 3s 4ms/step - loss: 0.8768 - accuracy: 0.6615 - val_loss: 0.6931 - val_accuracy: 0.7207\n",
      "Epoch 4/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.7302 - accuracy: 0.71 - ETA: 2s - loss: 0.8262 - accuracy: 0.62 - ETA: 2s - loss: 0.7754 - accuracy: 0.66 - ETA: 2s - loss: 0.8438 - accuracy: 0.63 - ETA: 2s - loss: 0.8224 - accuracy: 0.64 - ETA: 2s - loss: 0.8558 - accuracy: 0.63 - ETA: 1s - loss: 0.8644 - accuracy: 0.63 - ETA: 1s - loss: 0.8531 - accuracy: 0.63 - ETA: 1s - loss: 0.8553 - accuracy: 0.64 - ETA: 1s - loss: 0.8430 - accuracy: 0.65 - ETA: 1s - loss: 0.8547 - accuracy: 0.65 - ETA: 1s - loss: 0.8600 - accuracy: 0.66 - ETA: 1s - loss: 0.8458 - accuracy: 0.66 - ETA: 1s - loss: 0.8280 - accuracy: 0.66 - ETA: 0s - loss: 0.8251 - accuracy: 0.67 - ETA: 0s - loss: 0.8243 - accuracy: 0.66 - ETA: 0s - loss: 0.8126 - accuracy: 0.67 - ETA: 0s - loss: 0.8111 - accuracy: 0.67 - ETA: 0s - loss: 0.8034 - accuracy: 0.67 - ETA: 0s - loss: 0.8006 - accuracy: 0.67 - ETA: 0s - loss: 0.7956 - accuracy: 0.68 - 3s 4ms/step - loss: 0.7886 - accuracy: 0.6828 - val_loss: 0.6377 - val_accuracy: 0.7527\n",
      "Epoch 5/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.7483 - accuracy: 0.65 - ETA: 2s - loss: 0.7772 - accuracy: 0.67 - ETA: 2s - loss: 0.7717 - accuracy: 0.67 - ETA: 2s - loss: 0.7714 - accuracy: 0.65 - ETA: 2s - loss: 0.7747 - accuracy: 0.66 - ETA: 2s - loss: 0.7828 - accuracy: 0.66 - ETA: 1s - loss: 0.7560 - accuracy: 0.67 - ETA: 1s - loss: 0.7632 - accuracy: 0.67 - ETA: 1s - loss: 0.7435 - accuracy: 0.67 - ETA: 1s - loss: 0.7254 - accuracy: 0.68 - ETA: 1s - loss: 0.7383 - accuracy: 0.68 - ETA: 1s - loss: 0.7453 - accuracy: 0.68 - ETA: 1s - loss: 0.7375 - accuracy: 0.68 - ETA: 0s - loss: 0.7438 - accuracy: 0.68 - ETA: 0s - loss: 0.7291 - accuracy: 0.68 - ETA: 0s - loss: 0.7334 - accuracy: 0.68 - ETA: 0s - loss: 0.7219 - accuracy: 0.69 - ETA: 0s - loss: 0.7123 - accuracy: 0.69 - ETA: 0s - loss: 0.7190 - accuracy: 0.68 - ETA: 0s - loss: 0.7165 - accuracy: 0.69 - ETA: 0s - loss: 0.7038 - accuracy: 0.70 - 3s 4ms/step - loss: 0.6996 - accuracy: 0.7070 - val_loss: 0.5041 - val_accuracy: 0.8081\n",
      "Epoch 6/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.4186 - accuracy: 0.84 - ETA: 1s - loss: 0.4938 - accuracy: 0.82 - ETA: 1s - loss: 0.5886 - accuracy: 0.79 - ETA: 1s - loss: 0.5768 - accuracy: 0.78 - ETA: 1s - loss: 0.5932 - accuracy: 0.78 - ETA: 1s - loss: 0.6154 - accuracy: 0.76 - ETA: 1s - loss: 0.5818 - accuracy: 0.78 - ETA: 1s - loss: 0.5586 - accuracy: 0.79 - ETA: 1s - loss: 0.5578 - accuracy: 0.79 - ETA: 1s - loss: 0.5580 - accuracy: 0.78 - ETA: 1s - loss: 0.5580 - accuracy: 0.79 - ETA: 1s - loss: 0.5687 - accuracy: 0.78 - ETA: 0s - loss: 0.5542 - accuracy: 0.78 - ETA: 0s - loss: 0.5590 - accuracy: 0.78 - ETA: 0s - loss: 0.5549 - accuracy: 0.78 - ETA: 0s - loss: 0.5587 - accuracy: 0.78 - ETA: 0s - loss: 0.5531 - accuracy: 0.79 - ETA: 0s - loss: 0.5718 - accuracy: 0.78 - ETA: 0s - loss: 0.5740 - accuracy: 0.78 - ETA: 0s - loss: 0.5730 - accuracy: 0.78 - ETA: 0s - loss: 0.5701 - accuracy: 0.78 - 3s 4ms/step - loss: 0.5677 - accuracy: 0.7852 - val_loss: 0.4942 - val_accuracy: 0.7953\n",
      "Epoch 7/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.5380 - accuracy: 0.78 - ETA: 2s - loss: 0.5388 - accuracy: 0.76 - ETA: 2s - loss: 0.5438 - accuracy: 0.77 - ETA: 2s - loss: 0.5308 - accuracy: 0.78 - ETA: 2s - loss: 0.5148 - accuracy: 0.76 - ETA: 1s - loss: 0.5688 - accuracy: 0.75 - ETA: 1s - loss: 0.5506 - accuracy: 0.75 - ETA: 1s - loss: 0.5409 - accuracy: 0.76 - ETA: 1s - loss: 0.5528 - accuracy: 0.76 - ETA: 1s - loss: 0.5465 - accuracy: 0.76 - ETA: 1s - loss: 0.5389 - accuracy: 0.77 - ETA: 1s - loss: 0.5316 - accuracy: 0.77 - ETA: 1s - loss: 0.5243 - accuracy: 0.78 - ETA: 0s - loss: 0.5203 - accuracy: 0.78 - ETA: 0s - loss: 0.5279 - accuracy: 0.78 - ETA: 0s - loss: 0.5226 - accuracy: 0.78 - ETA: 0s - loss: 0.5156 - accuracy: 0.78 - ETA: 0s - loss: 0.5114 - accuracy: 0.78 - ETA: 0s - loss: 0.5075 - accuracy: 0.79 - ETA: 0s - loss: 0.5139 - accuracy: 0.79 - ETA: 0s - loss: 0.5146 - accuracy: 0.79 - 3s 4ms/step - loss: 0.5199 - accuracy: 0.7923 - val_loss: 0.4109 - val_accuracy: 0.8486\n",
      "Epoch 8/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.6302 - accuracy: 0.68 - ETA: 1s - loss: 0.5093 - accuracy: 0.76 - ETA: 1s - loss: 0.4595 - accuracy: 0.79 - ETA: 1s - loss: 0.4537 - accuracy: 0.80 - ETA: 1s - loss: 0.5101 - accuracy: 0.78 - ETA: 1s - loss: 0.5269 - accuracy: 0.77 - ETA: 1s - loss: 0.5585 - accuracy: 0.76 - ETA: 1s - loss: 0.5607 - accuracy: 0.75 - ETA: 1s - loss: 0.5315 - accuracy: 0.76 - ETA: 1s - loss: 0.5156 - accuracy: 0.77 - ETA: 1s - loss: 0.5094 - accuracy: 0.77 - ETA: 1s - loss: 0.5043 - accuracy: 0.77 - ETA: 0s - loss: 0.5121 - accuracy: 0.78 - ETA: 0s - loss: 0.5207 - accuracy: 0.77 - ETA: 0s - loss: 0.5084 - accuracy: 0.78 - ETA: 0s - loss: 0.5173 - accuracy: 0.78 - ETA: 0s - loss: 0.5047 - accuracy: 0.78 - ETA: 0s - loss: 0.4943 - accuracy: 0.79 - ETA: 0s - loss: 0.5012 - accuracy: 0.79 - ETA: 0s - loss: 0.5003 - accuracy: 0.79 - ETA: 0s - loss: 0.4982 - accuracy: 0.79 - 3s 4ms/step - loss: 0.4998 - accuracy: 0.7937 - val_loss: 0.4046 - val_accuracy: 0.8230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.4845 - accuracy: 0.75 - ETA: 2s - loss: 0.4677 - accuracy: 0.78 - ETA: 2s - loss: 0.4696 - accuracy: 0.80 - ETA: 2s - loss: 0.4628 - accuracy: 0.81 - ETA: 1s - loss: 0.4336 - accuracy: 0.81 - ETA: 1s - loss: 0.4494 - accuracy: 0.82 - ETA: 1s - loss: 0.4543 - accuracy: 0.81 - ETA: 1s - loss: 0.4611 - accuracy: 0.80 - ETA: 1s - loss: 0.4484 - accuracy: 0.81 - ETA: 1s - loss: 0.4424 - accuracy: 0.82 - ETA: 1s - loss: 0.4411 - accuracy: 0.82 - ETA: 1s - loss: 0.4719 - accuracy: 0.81 - ETA: 1s - loss: 0.4809 - accuracy: 0.80 - ETA: 0s - loss: 0.4852 - accuracy: 0.80 - ETA: 0s - loss: 0.4700 - accuracy: 0.81 - ETA: 0s - loss: 0.4808 - accuracy: 0.80 - ETA: 0s - loss: 0.4666 - accuracy: 0.81 - ETA: 0s - loss: 0.4851 - accuracy: 0.80 - ETA: 0s - loss: 0.4920 - accuracy: 0.80 - ETA: 0s - loss: 0.4855 - accuracy: 0.80 - ETA: 0s - loss: 0.4795 - accuracy: 0.81 - 3s 4ms/step - loss: 0.4820 - accuracy: 0.8094 - val_loss: 0.4047 - val_accuracy: 0.8124\n",
      "Epoch 10/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.3916 - accuracy: 0.87 - ETA: 2s - loss: 0.3660 - accuracy: 0.87 - ETA: 2s - loss: 0.3773 - accuracy: 0.86 - ETA: 2s - loss: 0.4001 - accuracy: 0.85 - ETA: 2s - loss: 0.3903 - accuracy: 0.85 - ETA: 2s - loss: 0.3829 - accuracy: 0.86 - ETA: 1s - loss: 0.3896 - accuracy: 0.85 - ETA: 1s - loss: 0.3891 - accuracy: 0.85 - ETA: 1s - loss: 0.3916 - accuracy: 0.85 - ETA: 1s - loss: 0.3775 - accuracy: 0.85 - ETA: 1s - loss: 0.3794 - accuracy: 0.86 - ETA: 1s - loss: 0.3743 - accuracy: 0.86 - ETA: 1s - loss: 0.3642 - accuracy: 0.87 - ETA: 1s - loss: 0.3599 - accuracy: 0.87 - ETA: 0s - loss: 0.3582 - accuracy: 0.87 - ETA: 0s - loss: 0.3602 - accuracy: 0.87 - ETA: 0s - loss: 0.3793 - accuracy: 0.86 - ETA: 0s - loss: 0.4010 - accuracy: 0.85 - ETA: 0s - loss: 0.4015 - accuracy: 0.85 - ETA: 0s - loss: 0.3929 - accuracy: 0.85 - ETA: 0s - loss: 0.3934 - accuracy: 0.85 - 3s 5ms/step - loss: 0.3922 - accuracy: 0.8592 - val_loss: 0.3622 - val_accuracy: 0.8699\n",
      "Epoch 11/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.4113 - accuracy: 0.87 - ETA: 1s - loss: 0.3553 - accuracy: 0.87 - ETA: 1s - loss: 0.3829 - accuracy: 0.86 - ETA: 2s - loss: 0.3465 - accuracy: 0.88 - ETA: 2s - loss: 0.3565 - accuracy: 0.86 - ETA: 2s - loss: 0.3519 - accuracy: 0.85 - ETA: 2s - loss: 0.3343 - accuracy: 0.87 - ETA: 2s - loss: 0.4087 - accuracy: 0.85 - ETA: 1s - loss: 0.3958 - accuracy: 0.85 - ETA: 1s - loss: 0.4004 - accuracy: 0.84 - ETA: 1s - loss: 0.3891 - accuracy: 0.84 - ETA: 1s - loss: 0.3870 - accuracy: 0.85 - ETA: 1s - loss: 0.3787 - accuracy: 0.85 - ETA: 1s - loss: 0.3815 - accuracy: 0.85 - ETA: 1s - loss: 0.3927 - accuracy: 0.85 - ETA: 0s - loss: 0.3890 - accuracy: 0.85 - ETA: 0s - loss: 0.3885 - accuracy: 0.85 - ETA: 0s - loss: 0.3870 - accuracy: 0.85 - ETA: 0s - loss: 0.3833 - accuracy: 0.85 - ETA: 0s - loss: 0.3781 - accuracy: 0.85 - ETA: 0s - loss: 0.3750 - accuracy: 0.85 - 4s 6ms/step - loss: 0.3777 - accuracy: 0.8549 - val_loss: 0.3595 - val_accuracy: 0.8742\n",
      "Epoch 12/20\n",
      "703/703 [==============================] - ETA: 3s - loss: 0.3841 - accuracy: 0.81 - ETA: 2s - loss: 0.4127 - accuracy: 0.84 - ETA: 2s - loss: 0.4122 - accuracy: 0.84 - ETA: 2s - loss: 0.3829 - accuracy: 0.84 - ETA: 2s - loss: 0.3869 - accuracy: 0.84 - ETA: 2s - loss: 0.3867 - accuracy: 0.86 - ETA: 2s - loss: 0.3801 - accuracy: 0.86 - ETA: 2s - loss: 0.3770 - accuracy: 0.85 - ETA: 1s - loss: 0.3931 - accuracy: 0.84 - ETA: 1s - loss: 0.3823 - accuracy: 0.85 - ETA: 1s - loss: 0.3746 - accuracy: 0.85 - ETA: 1s - loss: 0.3640 - accuracy: 0.85 - ETA: 1s - loss: 0.3766 - accuracy: 0.85 - ETA: 0s - loss: 0.3792 - accuracy: 0.84 - ETA: 0s - loss: 0.3734 - accuracy: 0.84 - ETA: 0s - loss: 0.3676 - accuracy: 0.84 - ETA: 0s - loss: 0.3708 - accuracy: 0.84 - ETA: 0s - loss: 0.3666 - accuracy: 0.84 - ETA: 0s - loss: 0.3632 - accuracy: 0.85 - ETA: 0s - loss: 0.3540 - accuracy: 0.85 - ETA: 0s - loss: 0.3436 - accuracy: 0.86 - 3s 5ms/step - loss: 0.3428 - accuracy: 0.8620 - val_loss: 0.2976 - val_accuracy: 0.8806\n",
      "Epoch 13/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.0931 - accuracy: 0.96 - ETA: 2s - loss: 0.2314 - accuracy: 0.89 - ETA: 2s - loss: 0.2322 - accuracy: 0.88 - ETA: 2s - loss: 0.2098 - accuracy: 0.90 - ETA: 2s - loss: 0.2262 - accuracy: 0.90 - ETA: 2s - loss: 0.2380 - accuracy: 0.89 - ETA: 2s - loss: 0.2444 - accuracy: 0.89 - ETA: 2s - loss: 0.2267 - accuracy: 0.90 - ETA: 1s - loss: 0.2296 - accuracy: 0.90 - ETA: 1s - loss: 0.2357 - accuracy: 0.90 - ETA: 1s - loss: 0.2746 - accuracy: 0.88 - ETA: 1s - loss: 0.2888 - accuracy: 0.87 - ETA: 1s - loss: 0.2861 - accuracy: 0.87 - ETA: 1s - loss: 0.2749 - accuracy: 0.88 - ETA: 0s - loss: 0.2702 - accuracy: 0.88 - ETA: 0s - loss: 0.2643 - accuracy: 0.89 - ETA: 0s - loss: 0.2651 - accuracy: 0.89 - ETA: 0s - loss: 0.2652 - accuracy: 0.89 - ETA: 0s - loss: 0.2767 - accuracy: 0.88 - ETA: 0s - loss: 0.2743 - accuracy: 0.88 - ETA: 0s - loss: 0.2719 - accuracy: 0.88 - 3s 5ms/step - loss: 0.2763 - accuracy: 0.8862 - val_loss: 0.3465 - val_accuracy: 0.8571\n",
      "Epoch 14/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.3396 - accuracy: 0.87 - ETA: 1s - loss: 0.3321 - accuracy: 0.87 - ETA: 1s - loss: 0.2989 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2650 - accuracy: 0.90 - ETA: 1s - loss: 0.2664 - accuracy: 0.90 - ETA: 1s - loss: 0.2753 - accuracy: 0.89 - ETA: 1s - loss: 0.2679 - accuracy: 0.90 - ETA: 1s - loss: 0.2793 - accuracy: 0.89 - ETA: 1s - loss: 0.2694 - accuracy: 0.89 - ETA: 1s - loss: 0.2679 - accuracy: 0.89 - ETA: 1s - loss: 0.2611 - accuracy: 0.90 - ETA: 1s - loss: 0.2625 - accuracy: 0.89 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2593 - accuracy: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2538 - accuracy: 0.90 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2544 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.90 - ETA: 0s - loss: 0.2691 - accuracy: 0.89 - 3s 4ms/step - loss: 0.2748 - accuracy: 0.8905 - val_loss: 0.2620 - val_accuracy: 0.8913\n",
      "Epoch 15/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.2962 - accuracy: 0.87 - ETA: 2s - loss: 0.3433 - accuracy: 0.84 - ETA: 2s - loss: 0.3125 - accuracy: 0.86 - ETA: 2s - loss: 0.3087 - accuracy: 0.87 - ETA: 2s - loss: 0.2851 - accuracy: 0.88 - ETA: 2s - loss: 0.2723 - accuracy: 0.89 - ETA: 1s - loss: 0.2781 - accuracy: 0.89 - ETA: 1s - loss: 0.2752 - accuracy: 0.89 - ETA: 1s - loss: 0.2694 - accuracy: 0.90 - ETA: 1s - loss: 0.2705 - accuracy: 0.91 - ETA: 1s - loss: 0.2786 - accuracy: 0.91 - ETA: 1s - loss: 0.2819 - accuracy: 0.91 - ETA: 1s - loss: 0.2761 - accuracy: 0.91 - ETA: 1s - loss: 0.2753 - accuracy: 0.91 - ETA: 0s - loss: 0.2641 - accuracy: 0.92 - ETA: 0s - loss: 0.2561 - accuracy: 0.92 - ETA: 0s - loss: 0.2649 - accuracy: 0.92 - ETA: 0s - loss: 0.2697 - accuracy: 0.91 - ETA: 0s - loss: 0.2690 - accuracy: 0.91 - ETA: 0s - loss: 0.2702 - accuracy: 0.91 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - 3s 4ms/step - loss: 0.2660 - accuracy: 0.9118 - val_loss: 0.2809 - val_accuracy: 0.8763\n",
      "Epoch 16/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.2073 - accuracy: 0.87 - ETA: 2s - loss: 0.2524 - accuracy: 0.85 - ETA: 1s - loss: 0.2867 - accuracy: 0.85 - ETA: 1s - loss: 0.2501 - accuracy: 0.87 - ETA: 1s - loss: 0.2704 - accuracy: 0.88 - ETA: 1s - loss: 0.2697 - accuracy: 0.87 - ETA: 1s - loss: 0.2504 - accuracy: 0.88 - ETA: 1s - loss: 0.2425 - accuracy: 0.89 - ETA: 1s - loss: 0.2301 - accuracy: 0.90 - ETA: 1s - loss: 0.2360 - accuracy: 0.90 - ETA: 1s - loss: 0.2324 - accuracy: 0.90 - ETA: 1s - loss: 0.2269 - accuracy: 0.90 - ETA: 0s - loss: 0.2152 - accuracy: 0.91 - ETA: 0s - loss: 0.2151 - accuracy: 0.91 - ETA: 0s - loss: 0.2126 - accuracy: 0.91 - ETA: 0s - loss: 0.2134 - accuracy: 0.91 - ETA: 0s - loss: 0.2123 - accuracy: 0.91 - ETA: 0s - loss: 0.2169 - accuracy: 0.91 - ETA: 0s - loss: 0.2122 - accuracy: 0.91 - ETA: 0s - loss: 0.2132 - accuracy: 0.91 - ETA: 0s - loss: 0.2165 - accuracy: 0.91 - 3s 4ms/step - loss: 0.2229 - accuracy: 0.9147 - val_loss: 0.2815 - val_accuracy: 0.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "703/703 [==============================] - ETA: 2s - loss: 0.0761 - accuracy: 1.00 - ETA: 2s - loss: 0.0978 - accuracy: 0.98 - ETA: 2s - loss: 0.1218 - accuracy: 0.95 - ETA: 2s - loss: 0.1368 - accuracy: 0.94 - ETA: 2s - loss: 0.1530 - accuracy: 0.93 - ETA: 2s - loss: 0.1898 - accuracy: 0.92 - ETA: 2s - loss: 0.1810 - accuracy: 0.93 - ETA: 2s - loss: 0.1851 - accuracy: 0.93 - ETA: 1s - loss: 0.2138 - accuracy: 0.93 - ETA: 1s - loss: 0.2094 - accuracy: 0.93 - ETA: 1s - loss: 0.2079 - accuracy: 0.93 - ETA: 1s - loss: 0.1979 - accuracy: 0.93 - ETA: 1s - loss: 0.2033 - accuracy: 0.93 - ETA: 1s - loss: 0.2072 - accuracy: 0.92 - ETA: 0s - loss: 0.2051 - accuracy: 0.92 - ETA: 0s - loss: 0.2008 - accuracy: 0.93 - ETA: 0s - loss: 0.1977 - accuracy: 0.93 - ETA: 0s - loss: 0.2074 - accuracy: 0.92 - ETA: 0s - loss: 0.2194 - accuracy: 0.92 - ETA: 0s - loss: 0.2159 - accuracy: 0.92 - ETA: 0s - loss: 0.2109 - accuracy: 0.93 - 3s 5ms/step - loss: 0.2068 - accuracy: 0.9317 - val_loss: 0.2568 - val_accuracy: 0.8870\n",
      "Epoch 18/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.2396 - accuracy: 0.81 - ETA: 2s - loss: 0.2274 - accuracy: 0.89 - ETA: 2s - loss: 0.2077 - accuracy: 0.89 - ETA: 2s - loss: 0.1903 - accuracy: 0.91 - ETA: 2s - loss: 0.1785 - accuracy: 0.91 - ETA: 2s - loss: 0.1704 - accuracy: 0.92 - ETA: 1s - loss: 0.1823 - accuracy: 0.91 - ETA: 1s - loss: 0.1723 - accuracy: 0.92 - ETA: 1s - loss: 0.1623 - accuracy: 0.93 - ETA: 1s - loss: 0.1524 - accuracy: 0.93 - ETA: 1s - loss: 0.1414 - accuracy: 0.94 - ETA: 1s - loss: 0.1466 - accuracy: 0.94 - ETA: 1s - loss: 0.1635 - accuracy: 0.92 - ETA: 0s - loss: 0.1613 - accuracy: 0.92 - ETA: 0s - loss: 0.1602 - accuracy: 0.92 - ETA: 0s - loss: 0.1611 - accuracy: 0.93 - ETA: 0s - loss: 0.1583 - accuracy: 0.93 - ETA: 0s - loss: 0.1558 - accuracy: 0.93 - ETA: 0s - loss: 0.1629 - accuracy: 0.93 - ETA: 0s - loss: 0.1574 - accuracy: 0.93 - ETA: 0s - loss: 0.1612 - accuracy: 0.93 - 3s 4ms/step - loss: 0.1574 - accuracy: 0.9388 - val_loss: 0.2297 - val_accuracy: 0.9041\n",
      "Epoch 19/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.2134 - accuracy: 0.90 - ETA: 2s - loss: 0.1912 - accuracy: 0.93 - ETA: 2s - loss: 0.1658 - accuracy: 0.94 - ETA: 2s - loss: 0.1487 - accuracy: 0.95 - ETA: 2s - loss: 0.1577 - accuracy: 0.94 - ETA: 1s - loss: 0.1700 - accuracy: 0.94 - ETA: 1s - loss: 0.1669 - accuracy: 0.94 - ETA: 1s - loss: 0.1695 - accuracy: 0.94 - ETA: 1s - loss: 0.1748 - accuracy: 0.93 - ETA: 1s - loss: 0.1761 - accuracy: 0.93 - ETA: 1s - loss: 0.1749 - accuracy: 0.93 - ETA: 1s - loss: 0.1706 - accuracy: 0.93 - ETA: 1s - loss: 0.1651 - accuracy: 0.93 - ETA: 1s - loss: 0.1596 - accuracy: 0.94 - ETA: 0s - loss: 0.1659 - accuracy: 0.94 - ETA: 0s - loss: 0.1604 - accuracy: 0.94 - ETA: 0s - loss: 0.1601 - accuracy: 0.94 - ETA: 0s - loss: 0.1546 - accuracy: 0.94 - ETA: 0s - loss: 0.1610 - accuracy: 0.94 - ETA: 0s - loss: 0.1590 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - 3s 4ms/step - loss: 0.1609 - accuracy: 0.9445 - val_loss: 0.2845 - val_accuracy: 0.8998\n",
      "Epoch 20/20\n",
      "703/703 [==============================] - ETA: 1s - loss: 0.2891 - accuracy: 0.87 - ETA: 2s - loss: 0.2990 - accuracy: 0.87 - ETA: 2s - loss: 0.2143 - accuracy: 0.91 - ETA: 1s - loss: 0.1812 - accuracy: 0.92 - ETA: 1s - loss: 0.1767 - accuracy: 0.93 - ETA: 1s - loss: 0.1552 - accuracy: 0.94 - ETA: 1s - loss: 0.1531 - accuracy: 0.94 - ETA: 1s - loss: 0.1448 - accuracy: 0.94 - ETA: 1s - loss: 0.1562 - accuracy: 0.94 - ETA: 1s - loss: 0.1540 - accuracy: 0.94 - ETA: 1s - loss: 0.1491 - accuracy: 0.94 - ETA: 1s - loss: 0.1528 - accuracy: 0.94 - ETA: 1s - loss: 0.1484 - accuracy: 0.94 - ETA: 0s - loss: 0.1449 - accuracy: 0.95 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - ETA: 0s - loss: 0.1554 - accuracy: 0.94 - ETA: 0s - loss: 0.1582 - accuracy: 0.93 - ETA: 0s - loss: 0.1533 - accuracy: 0.94 - ETA: 0s - loss: 0.1489 - accuracy: 0.94 - ETA: 0s - loss: 0.1470 - accuracy: 0.94 - ETA: 0s - loss: 0.1441 - accuracy: 0.94 - 3s 4ms/step - loss: 0.1417 - accuracy: 0.9502 - val_loss: 0.2835 - val_accuracy: 0.8742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dd64399688>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "print(y_train_hot.shape)\n",
    "print(labels.shape)\n",
    "print(X_train.shape)\n",
    "# Train the CNN model\n",
    "#    X_train: Input data\n",
    "#    y_train_hot: Target data\n",
    "model.fit(X_train, y_train_hot, epochs=config.epochs, validation_data=(X_val, y_val_hot), callbacks=[WandbCallback(data_type=\"image\", labels=labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the keras model\n",
    "model.save(\"geo_cnn_model.h5\")\n",
    "print(\"Model has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the IntelliChirp Biophony CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model('geo_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                516160    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 534,101\n",
      "Trainable params: 534,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19   0   0   0   0]\n",
      " [  1 112   0  10   0]\n",
      " [  0   0  14   0   0]\n",
      " [  1  12   1  67  14]\n",
      " [  0   1   0   3  39]]\n",
      "Accuracy for class GOC : [1.]\n",
      "Accuracy for class GRA : [0.91056911]\n",
      "Accuracy for class GST : [1.]\n",
      "Accuracy for class GWG : [0.70526316]\n",
      "Accuracy for class GWC : [0.90697674]\n",
      "Overall Accuracy : 0.8537414965986394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[ 19   0   0   0   0]\\n [  0 119   0   4   0]\\n [  0   0  14   0   0]\\n [  2  19   0  67   7]\\n [  0   4   0   3  36]]\\nAccuracy for class GOC : [1.]\\nAccuracy for class GRA : [0.96747967]\\nAccuracy for class GST : [1.]\\nAccuracy for class GWG : [0.70526316]\\nAccuracy for class GWC : [0.8372093]\\nOverall Accuracy : 0.8673469387755102'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ohe = loaded_model.predict(X_test)  # shape=(n_samples, 12)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_labels)  # shape\n",
    "print(confusion_matrix)\n",
    "\n",
    "for class_i in range(len(labels)) :\n",
    "    indices = np.argwhere(y_test == class_i)\n",
    "    sum = 0\n",
    "    for index in indices:\n",
    "        sum += (y_test[index] == y_pred_labels[index])\n",
    "    if(len(indices) > 0) : mean = sum/len(indices)\n",
    "    else : mean = \"N/A\"\n",
    "    print(\"Accuracy for class\", labels[class_i], \":\", mean)\n",
    "\n",
    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))\n",
    "\n",
    "'''[[ 19   0   0   0   0]\n",
    " [  1 112   0  10   0]\n",
    " [  0   0  14   0   0]\n",
    " [  1  12   1  67  14]\n",
    " [  0   1   0   3  39]]\n",
    "Accuracy for class GOC : [1.]\n",
    "Accuracy for class GRA : [0.91056911]\n",
    "Accuracy for class GST : [1.]\n",
    "Accuracy for class GWG : [0.70526316]\n",
    "Accuracy for class GWC : [0.90697674]\n",
    "Overall Accuracy : 0.8537414965986394'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[ 0.0000000e+00  1.5258789e-05  0.0000000e+00 ...  3.3020020e-02\n",
      "  1.2680054e-02 -8.7432861e-03]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000176\n",
      "\n",
      " GRA :  0.38465884\n",
      "\n",
      " GST :  0.00000003\n",
      "\n",
      " GWG :  0.00648697\n",
      "\n",
      " GWC :  0.60885239\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[1. 2.]\n",
      "[-0.03717041 -0.05769348 -0.06455994 ...  0.01766968  0.01895142\n",
      "  0.01779175]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000044\n",
      "\n",
      " GRA :  0.00247770\n",
      "\n",
      " GST :  0.00000003\n",
      "\n",
      " GWG :  0.01955874\n",
      "\n",
      " GWC :  0.97796303\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[2. 3.]\n",
      "[ 0.02345276  0.02101135  0.01712036 ... -0.01161194 -0.0141449\n",
      " -0.01431274]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00002324\n",
      "\n",
      " GRA :  0.00770301\n",
      "\n",
      " GST :  0.00000020\n",
      "\n",
      " GWG :  0.01101210\n",
      "\n",
      " GWC :  0.98126149\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[3. 4.]\n",
      "[-0.01583862 -0.01066589 -0.00762939 ... -0.0377655  -0.03556824\n",
      " -0.02685547]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00001256\n",
      "\n",
      " GRA :  0.94493687\n",
      "\n",
      " GST :  0.00001222\n",
      "\n",
      " GWG :  0.03799357\n",
      "\n",
      " GWC :  0.01704480\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[4. 5.]\n",
      "[-0.02836609 -0.02510071 -0.02012634 ...  0.0138855  -0.00386047\n",
      " -0.00904846]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00009305\n",
      "\n",
      " GRA :  0.07313888\n",
      "\n",
      " GST :  0.00000047\n",
      "\n",
      " GWG :  0.15516256\n",
      "\n",
      " GWC :  0.77160501\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[5. 6.]\n",
      "[-0.00526428  0.00822449  0.01951599 ...  0.02729797  0.02156067\n",
      "  0.01234436]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00002247\n",
      "\n",
      " GRA :  0.26627952\n",
      "\n",
      " GST :  0.00000522\n",
      "\n",
      " GWG :  0.01112610\n",
      "\n",
      " GWC :  0.72256666\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[6. 7.]\n",
      "[ 0.00544739  0.00053406  0.00970459 ... -0.02848816 -0.01611328\n",
      " -0.01091003]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000011\n",
      "\n",
      " GRA :  0.00877954\n",
      "\n",
      " GST :  0.00000002\n",
      "\n",
      " GWG :  0.00733439\n",
      "\n",
      " GWC :  0.98388600\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[7. 8.]\n",
      "[-0.0177002  -0.02372742 -0.02700806 ... -0.04304504 -0.04063416\n",
      " -0.03363037]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000136\n",
      "\n",
      " GRA :  0.99270928\n",
      "\n",
      " GST :  0.00002075\n",
      "\n",
      " GWG :  0.00483087\n",
      "\n",
      " GWC :  0.00243783\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[8. 9.]\n",
      "[-0.01539612 -0.00108337  0.00718689 ...  0.01161194  0.01818848\n",
      "  0.02700806]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000364\n",
      "\n",
      " GRA :  0.89666098\n",
      "\n",
      " GST :  0.00000704\n",
      "\n",
      " GWG :  0.00706323\n",
      "\n",
      " GWC :  0.09626509\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[ 9. 10.]\n",
      "[ 0.03549194  0.04856873  0.05519104 ... -0.02171326 -0.03634644\n",
      " -0.03912354]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00001260\n",
      "\n",
      " GRA :  0.15989006\n",
      "\n",
      " GST :  0.00000509\n",
      "\n",
      " GWG :  0.05977093\n",
      "\n",
      " GWC :  0.78032130\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[10. 11.]\n",
      "[-0.02934265 -0.0115509   0.00445557 ... -0.03616333 -0.03759766\n",
      " -0.0304718 ]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000484\n",
      "\n",
      " GRA :  0.62774366\n",
      "\n",
      " GST :  0.00000141\n",
      "\n",
      " GWG :  0.05731521\n",
      "\n",
      " GWC :  0.31493491\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[11. 12.]\n",
      "[-0.03358459 -0.03901672 -0.03933716 ... -0.02337646 -0.02124023\n",
      " -0.02107239]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00001070\n",
      "\n",
      " GRA :  0.02227188\n",
      "\n",
      " GST :  0.00000205\n",
      "\n",
      " GWG :  0.06705441\n",
      "\n",
      " GWC :  0.91066092\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[12. 13.]\n",
      "[-0.00846863  0.00444031  0.00852966 ... -0.00604248 -0.00845337\n",
      " -0.00497437]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000028\n",
      "\n",
      " GRA :  0.21407339\n",
      "\n",
      " GST :  0.00000426\n",
      "\n",
      " GWG :  0.48406833\n",
      "\n",
      " GWC :  0.30185366\n",
      "GUESS: Nothing\n",
      "[13. 14.]\n",
      "[-0.00427246 -0.00718689 -0.00811768 ... -0.01966858 -0.01296997\n",
      " -0.01628113]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000002\n",
      "\n",
      " GRA :  0.04747614\n",
      "\n",
      " GST :  0.00000026\n",
      "\n",
      " GWG :  0.28232297\n",
      "\n",
      " GWC :  0.67020059\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[14. 15.]\n",
      "[-0.02262878 -0.01573181 -0.00117493 ... -0.08956909 -0.0695343\n",
      " -0.04067993]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000068\n",
      "\n",
      " GRA :  0.03221902\n",
      "\n",
      " GST :  0.00000030\n",
      "\n",
      " GWG :  0.06798044\n",
      "\n",
      " GWC :  0.89979953\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[15. 16.]\n",
      "[-0.02532959 -0.01031494 -0.00280762 ... -0.07128906 -0.07106018\n",
      " -0.05839539]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000002\n",
      "\n",
      " GRA :  0.86727339\n",
      "\n",
      " GST :  0.00000005\n",
      "\n",
      " GWG :  0.00011261\n",
      "\n",
      " GWC :  0.13261394\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[16. 17.]\n",
      "[-0.04600525 -0.02149963  0.00523376 ... -0.02526855 -0.02735901\n",
      " -0.03106689]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000016\n",
      "\n",
      " GRA :  0.10364556\n",
      "\n",
      " GST :  0.00000006\n",
      "\n",
      " GWG :  0.04509518\n",
      "\n",
      " GWC :  0.85125911\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[17. 18.]\n",
      "[-0.02043152 -0.01174927 -0.02088928 ...  0.10055542  0.08653259\n",
      "  0.06604004]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00009542\n",
      "\n",
      " GRA :  0.34512478\n",
      "\n",
      " GST :  0.00000620\n",
      "\n",
      " GWG :  0.19678749\n",
      "\n",
      " GWC :  0.45798609\n",
      "GUESS: Nothing\n",
      "[18. 19.]\n",
      "[ 0.04153442  0.01223755 -0.00654602 ...  0.03269958  0.02374268\n",
      "  0.02774048]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000035\n",
      "\n",
      " GRA :  0.76191986\n",
      "\n",
      " GST :  0.00000023\n",
      "\n",
      " GWG :  0.08037928\n",
      "\n",
      " GWC :  0.15770023\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[19. 20.]\n",
      "[0.02185059 0.02069092 0.01451111 ... 0.03469849 0.03985596 0.04600525]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00047749\n",
      "\n",
      " GRA :  0.76327378\n",
      "\n",
      " GST :  0.00000129\n",
      "\n",
      " GWG :  0.03341111\n",
      "\n",
      " GWC :  0.20283636\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[20. 21.]\n",
      "[ 0.0353241   0.01567078 -0.00102234 ...  0.1058197   0.10365295\n",
      "  0.09759521]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00002144\n",
      "\n",
      " GRA :  0.63609004\n",
      "\n",
      " GST :  0.00001051\n",
      "\n",
      " GWG :  0.30752742\n",
      "\n",
      " GWC :  0.05635052\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[21. 22.]\n",
      "[ 0.09413147  0.07905579  0.05625916 ... -0.01145935 -0.00245667\n",
      "  0.00479126]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000066\n",
      "\n",
      " GRA :  0.99761701\n",
      "\n",
      " GST :  0.00000020\n",
      "\n",
      " GWG :  0.00054236\n",
      "\n",
      " GWC :  0.00183976\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[22. 23.]\n",
      "[ 0.0037384   0.01168823  0.01628113 ... -0.03440857 -0.05511475\n",
      " -0.08209229]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00001949\n",
      "\n",
      " GRA :  0.87093616\n",
      "\n",
      " GST :  0.00000191\n",
      "\n",
      " GWG :  0.04734631\n",
      "\n",
      " GWC :  0.08169603\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[23. 24.]\n",
      "[-0.1026001  -0.12590027 -0.14944458 ...  0.03462219  0.02537537\n",
      "  0.02354431]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000000\n",
      "\n",
      " GRA :  0.79179174\n",
      "\n",
      " GST :  0.00000004\n",
      "\n",
      " GWG :  0.19508553\n",
      "\n",
      " GWC :  0.01312272\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[24. 25.]\n",
      "[ 0.0196991   0.02836609  0.03103638 ... -0.03009033 -0.03392029\n",
      " -0.03681946]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000019\n",
      "\n",
      " GRA :  0.99032038\n",
      "\n",
      " GST :  0.00000008\n",
      "\n",
      " GWG :  0.00241051\n",
      "\n",
      " GWC :  0.00726884\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[25. 26.]\n",
      "[-0.04151917 -0.03933716 -0.03703308 ...  0.05451965  0.0519104\n",
      "  0.05206299]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000147\n",
      "\n",
      " GRA :  0.94179332\n",
      "\n",
      " GST :  0.00000139\n",
      "\n",
      " GWG :  0.05180373\n",
      "\n",
      " GWC :  0.00640011\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[26. 27.]\n",
      "[ 0.05670166  0.06253052  0.07643127 ... -0.00396729  0.00715637\n",
      "  0.00585938]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000093\n",
      "\n",
      " GRA :  0.06941112\n",
      "\n",
      " GST :  0.00000640\n",
      "\n",
      " GWG :  0.14259055\n",
      "\n",
      " GWC :  0.78799099\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[27. 28.]\n",
      "[-0.00222778 -0.01303101 -0.02310181 ...  0.01165771  0.01649475\n",
      "  0.0194397 ]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000000\n",
      "\n",
      " GRA :  0.01543347\n",
      "\n",
      " GST :  0.00000000\n",
      "\n",
      " GWG :  0.04449097\n",
      "\n",
      " GWC :  0.94007552\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[28. 29.]\n",
      "[0.01657104 0.01519775 0.00924683 ... 0.03746033 0.03282166 0.02775574]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000003\n",
      "\n",
      " GRA :  0.33569953\n",
      "\n",
      " GST :  0.00000165\n",
      "\n",
      " GWG :  0.14353959\n",
      "\n",
      " GWC :  0.52075917\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[29. 30.]\n",
      "[ 0.01919556  0.0135498   0.01724243 ... -0.00575256 -0.01502991\n",
      " -0.02742004]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000223\n",
      "\n",
      " GRA :  0.93829304\n",
      "\n",
      " GST :  0.00000170\n",
      "\n",
      " GWG :  0.00578377\n",
      "\n",
      " GWC :  0.05591923\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[30. 31.]\n",
      "[-0.0322876  -0.0365448  -0.03544617 ... -0.0218811  -0.02978516\n",
      " -0.04052734]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000810\n",
      "\n",
      " GRA :  0.60701579\n",
      "\n",
      " GST :  0.00000535\n",
      "\n",
      " GWG :  0.21953745\n",
      "\n",
      " GWC :  0.17343327\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[31. 32.]\n",
      "[-0.04328918 -0.03413391 -0.03421021 ...  0.05908203  0.06370544\n",
      "  0.05949402]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000032\n",
      "\n",
      " GRA :  0.07600122\n",
      "\n",
      " GST :  0.00000019\n",
      "\n",
      " GWG :  0.49096382\n",
      "\n",
      " GWC :  0.43303445\n",
      "GUESS: Nothing\n",
      "[32. 33.]\n",
      "[ 0.06063843  0.06056213  0.06610107 ... -0.12741089 -0.13371277\n",
      " -0.12313843]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000140\n",
      "\n",
      " GRA :  0.12951134\n",
      "\n",
      " GST :  0.00000031\n",
      "\n",
      " GWG :  0.58525521\n",
      "\n",
      " GWC :  0.28523183\n",
      "\n",
      "\n",
      "GUESS:  GWG\n",
      "[33. 34.]\n",
      "[-0.09968567 -0.06376648 -0.03105164 ... -0.0138092  -0.01574707\n",
      " -0.01896667]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000170\n",
      "\n",
      " GRA :  0.86591440\n",
      "\n",
      " GST :  0.00000117\n",
      "\n",
      " GWG :  0.13115266\n",
      "\n",
      " GWC :  0.00293008\n",
      "\n",
      "\n",
      "GUESS:  GRA\n",
      "[34. 35.]\n",
      "[-0.00811768  0.00149536  0.00953674 ... -0.004776   -0.0010376\n",
      "  0.00231934]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000166\n",
      "\n",
      " GRA :  0.07164631\n",
      "\n",
      " GST :  0.00000392\n",
      "\n",
      " GWG :  0.40353966\n",
      "\n",
      " GWC :  0.52480853\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[35. 36.]\n",
      "[ 0.00238037  0.00236511  0.00231934 ... -0.00193787  0.0068512\n",
      "  0.00695801]\n",
      "(128, 32)\n",
      "PREDICTED VALUES\n",
      "\n",
      " GOC :  0.00000096\n",
      "\n",
      " GRA :  0.02603642\n",
      "\n",
      " GST :  0.00000121\n",
      "\n",
      " GWG :  0.05616241\n",
      "\n",
      " GWC :  0.91779894\n",
      "\n",
      "\n",
      "GUESS:  GWC\n",
      "[{'class': 'GWC', 'timestamp': 0}, {'class': 'GWC', 'timestamp': 1}, {'class': 'GWC', 'timestamp': 2}, {'class': 'GRA', 'timestamp': 3}, {'class': 'GWC', 'timestamp': 4}, {'class': 'GWC', 'timestamp': 5}, {'class': 'GWC', 'timestamp': 6}, {'class': 'GRA', 'timestamp': 7}, {'class': 'GRA', 'timestamp': 8}, {'class': 'GWC', 'timestamp': 9}, {'class': 'GRA', 'timestamp': 10}, {'class': 'GWC', 'timestamp': 11}, {'class': 'Nothing', 'timestamp': 12}, {'class': 'GWC', 'timestamp': 13}, {'class': 'GWC', 'timestamp': 14}, {'class': 'GRA', 'timestamp': 15}, {'class': 'GWC', 'timestamp': 16}, {'class': 'Nothing', 'timestamp': 17}, {'class': 'GRA', 'timestamp': 18}, {'class': 'GRA', 'timestamp': 19}, {'class': 'GRA', 'timestamp': 20}, {'class': 'GRA', 'timestamp': 21}, {'class': 'GRA', 'timestamp': 22}, {'class': 'GRA', 'timestamp': 23}, {'class': 'GRA', 'timestamp': 24}, {'class': 'GRA', 'timestamp': 25}, {'class': 'GWC', 'timestamp': 26}, {'class': 'GWC', 'timestamp': 27}, {'class': 'GWC', 'timestamp': 28}, {'class': 'GRA', 'timestamp': 29}, {'class': 'GRA', 'timestamp': 30}, {'class': 'Nothing', 'timestamp': 31}, {'class': 'GWG', 'timestamp': 32}, {'class': 'GRA', 'timestamp': 33}, {'class': 'GWC', 'timestamp': 34}, {'class': 'GWC', 'timestamp': 35}]\n"
     ]
    }
   ],
   "source": [
    "## Running the model\n",
    "\n",
    "n_mfcc = config.buckets\n",
    "max_len = config.max_len\n",
    "# convert file to wav2mfcc\n",
    "# Mel-frequency cepstral coefficients\n",
    "file_path = \"./prediction/nature_sc.wav\"\n",
    "big_wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "#print(wave.shape, sr)\n",
    "\n",
    "classification = []\n",
    "\n",
    "for sec_index in range( int(big_wave.shape[0] / sr) ) :\n",
    "    start_sec = sec_index\n",
    "    end_sec = sec_index + 1\n",
    "    \n",
    "    sec_to_trim = np.array( [ float(start_sec), float(end_sec) ] )\n",
    "    print(sec_to_trim)\n",
    "    sec_to_trim = np.ceil( sec_to_trim * sr )\n",
    "\n",
    "    wave = big_wave[int(sec_to_trim[0]) : int(sec_to_trim[1])]\n",
    "    print(wave)\n",
    "\n",
    "    wave = np.asfortranarray(wave[::3])\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=n_mfcc)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "\n",
    "    # Convert wav to MFCC\n",
    "    prediction_data = wav2mfcc('./prediction/nature_sc.wav')\n",
    "    prediction_data = mfcc\n",
    "    print(prediction_data.shape)\n",
    "    #print(wav2mfcc())\n",
    "    # Reshape to 4 dimensions\n",
    "    prediction_data = prediction_data.reshape(1, config.buckets, config.max_len, channels)\n",
    "    #prediction_data = prediction_data.reshape(1, 20, config.max_len, channels)\n",
    "\n",
    "    # Run the model on the inputted file\n",
    "    predicted = loaded_model.predict(prediction_data)\n",
    "\n",
    "    # Output the prediction values for each class\n",
    "    print ('PREDICTED VALUES')\n",
    "    labels_indices = range(len(labels))\n",
    "    max_value = 0\n",
    "    max_value_index = 0\n",
    "    for index in labels_indices:\n",
    "        print('\\n', labels[index], \": \", '%.08f' % predicted[0,index])\n",
    "        if predicted[0,index] > max_value:\n",
    "            max_value_index = index\n",
    "            max_value = predicted[0,index]\n",
    "\n",
    "    # Output the prediction\n",
    "    if max_value < 0.5:\n",
    "        print(\"GUESS: Nothing\")\n",
    "        classification.append( { \"class\" : \"Nothing\", \"timestamp\" : start_sec } )\n",
    "    else:\n",
    "        print('\\n\\nGUESS: ', labels[max_value_index])\n",
    "        classification.append( { \"class\" : labels[max_value_index], \"timestamp\" : start_sec } )\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
