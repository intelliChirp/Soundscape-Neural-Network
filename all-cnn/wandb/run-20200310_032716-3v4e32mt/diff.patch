diff --git a/.idea/workspace.xml b/.idea/workspace.xml
index 2a9807c..38223a3 100644
--- a/.idea/workspace.xml
+++ b/.idea/workspace.xml
@@ -1,5 +1,16 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
+  <component name="ChangeListManager">
+    <list default="true" id="b5902b74-b91d-4838-aff6-605119fa9e4b" name="Default Changelist" comment="">
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/geo-cnn/geo-cnn.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/geo-cnn/geo-cnn.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/geo-cnn/geo_cnn_model.h5" beforeDir="false" afterPath="$PROJECT_DIR$/geo-cnn/geo_cnn_model.h5" afterDir="false" />
+    </list>
+    <option name="SHOW_DIALOG" value="false" />
+    <option name="HIGHLIGHT_CONFLICTS" value="true" />
+    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
+    <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
   <component name="Git.Settings">
     <option name="RECENT_BRANCH_BY_REPOSITORY">
       <map>
@@ -9,13 +20,40 @@
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
   <component name="ProjectId" id="1YS87hcwtpKSQTYA3v8PdlK9DdT" />
+  <component name="ProjectViewState">
+    <option name="hideEmptyMiddlePackages" value="true" />
+    <option name="showExcludedFiles" value="true" />
+    <option name="showLibraryContents" value="true" />
+  </component>
   <component name="PropertiesComponent">
     <property name="RunOnceActivity.ShowReadmeOnStart" value="true" />
     <property name="WebServerToolWindowFactoryState" value="false" />
-    <property name="last_opened_file_path" value="$PROJECT_DIR$" />
+    <property name="last_opened_file_path" value="$PROJECT_DIR$/../dir/SNAW" />
     <property name="settings.editor.selected.configurable" value="reference.settingsdialog.IDE.editor.colors" />
   </component>
+  <component name="SvnConfiguration">
+    <configuration />
+  </component>
+  <component name="TaskManager">
+    <task active="true" id="Default" summary="Default task">
+      <changelist id="b5902b74-b91d-4838-aff6-605119fa9e4b" name="Default Changelist" comment="" />
+      <created>1583654581585</created>
+      <option name="number" value="Default" />
+      <option name="presentableId" value="Default" />
+      <updated>1583654581585</updated>
+      <workItem from="1583654586575" duration="666000" />
+      <workItem from="1583786421901" duration="244000" />
+    </task>
+    <servers />
+  </component>
+  <component name="TypeScriptGeneratedFilesManager">
+    <option name="version" value="1" />
+  </component>
   <component name="WindowStateProjectService">
+    <state x="740" y="276" key="FileChooserDialogImpl" timestamp="1583786665877">
+      <screen x="0" y="0" width="1920" height="1040" />
+    </state>
+    <state x="740" y="276" key="FileChooserDialogImpl/0.0.1920.1040@0.0.1920.1040" timestamp="1583786665877" />
     <state x="461" y="163" key="SettingsEditor" timestamp="1582924517268">
       <screen x="0" y="0" width="1920" height="1040" />
     </state>
diff --git a/ant-cnn/ant-cnn.ipynb b/ant-cnn/ant-cnn.ipynb
index e1e1d80..ee2f5b5 100644
--- a/ant-cnn/ant-cnn.ipynb
+++ b/ant-cnn/ant-cnn.ipynb
@@ -2,17 +2,9 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 43,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "Using TensorFlow backend.\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "from preprocess import *\n",
     "import keras\n",
@@ -27,8 +19,10 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
+   "execution_count": 44,
+   "metadata": {
+    "scrolled": true
+   },
    "outputs": [
     {
      "data": {
@@ -36,7 +30,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/5iidzo5p\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/5iidzo5p</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/ftl7qqh6\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/ftl7qqh6</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -50,22 +44,32 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "Saving vectors of label - 'AAT': 100%|█████████████████████████████████████████████████| 58/58 [00:01<00:00, 54.05it/s]\n",
-      "Saving vectors of label - 'AHV': 100%|█████████████████████████████████████████████████| 16/16 [00:00<00:00, 65.22it/s]\n",
-      "Saving vectors of label - 'AMA': 100%|█████████████████████████████████████████████████| 30/30 [00:00<00:00, 61.26it/s]\n",
-      "Saving vectors of label - 'ART': 100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00, 53.90it/s]\n",
-      "Saving vectors of label - 'ASI': 100%|███████████████████████████████████████████████████| 4/4 [00:00<00:00, 35.81it/s]\n",
-      "Saving vectors of label - 'AVH': 100%|█████████████████████████████████████████████████| 18/18 [00:00<00:00, 96.00it/s]\n",
-      "Saving vectors of label - 'AVT': 100%|███████████████████████████████████████████████| 222/222 [00:04<00:00, 50.72it/s]\n"
+      "Saving vectors of label - 'AAT': 100%|███████████████████████████████████████████████| 205/205 [00:03<00:00, 59.44it/s]\n",
+      "Saving vectors of label - 'AHV': 100%|█████████████████████████████████████████████████| 33/33 [00:00<00:00, 41.21it/s]\n",
+      "Saving vectors of label - 'AMA': 100%|█████████████████████████████████████████████████| 94/94 [00:02<00:00, 34.51it/s]\n",
+      "Saving vectors of label - 'ART': 100%|████████████████████████████████████████████████| 21/21 [00:00<00:00, 151.48it/s]\n",
+      "Saving vectors of label - 'ASI': 100%|████████████████████████████████████████████████| 17/17 [00:00<00:00, 146.96it/s]\n",
+      "Saving vectors of label - 'AVH': 100%|████████████████████████████████████████████████| 35/35 [00:00<00:00, 137.62it/s]\n",
+      "Saving vectors of label - 'AVT': 100%|██████████████████████████████████████████████| 965/965 [00:07<00:00, 129.08it/s]\n"
      ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "\"Saving vectors of label - 'AAT': 100%|█████████████████████████████████████████████████| 58/58 [00:01<00:00, 54.05it/s]\\nSaving vectors of label - 'AHV': 100%|█████████████████████████████████████████████████| 16/16 [00:00<00:00, 65.22it/s]\\nSaving vectors of label - 'AMA': 100%|█████████████████████████████████████████████████| 30/30 [00:00<00:00, 61.26it/s]\\nSaving vectors of label - 'ART': 100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00, 53.90it/s]\\nSaving vectors of label - 'ASI': 100%|███████████████████████████████████████████████████| 4/4 [00:00<00:00, 35.81it/s]\\nSaving vectors of label - 'AVH': 100%|█████████████████████████████████████████████████| 18/18 [00:00<00:00, 96.00it/s]\\nSaving vectors of label - 'AVT': 100%|███████████████████████████████████████████████| 222/222 [00:04<00:00, 50.72it/s]\""
+      ]
+     },
+     "execution_count": 44,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
     "wandb.init()\n",
     "config = wandb.config\n",
     "\n",
-    "config.max_len = 21\n",
-    "config.buckets = 50\n",
+    "config.max_len = 32\n",
+    "config.buckets = 128\n",
     "\n",
     "# Save data to array file first\n",
     "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
@@ -74,12 +78,20 @@
     "#                 \"frog\", \"insects\"])\n",
     "labels=np.array([\"AAT\", \"AHV\", \"AMA\", \n",
     "                 \"ART\", \"ASI\", \"AVH\",\n",
-    "                \"AVT\"])"
+    "                \"AVT\"])\n",
+    "\n",
+    "'''Saving vectors of label - 'AAT': 100%|███████████████████████████████████████████████| 205/205 [00:03<00:00, 59.44it/s]\n",
+    "Saving vectors of label - 'AHV': 100%|█████████████████████████████████████████████████| 33/33 [00:00<00:00, 41.21it/s]\n",
+    "Saving vectors of label - 'AMA': 100%|█████████████████████████████████████████████████| 94/94 [00:02<00:00, 34.51it/s]\n",
+    "Saving vectors of label - 'ART': 100%|████████████████████████████████████████████████| 21/21 [00:00<00:00, 151.48it/s]\n",
+    "Saving vectors of label - 'ASI': 100%|████████████████████████████████████████████████| 17/17 [00:00<00:00, 146.96it/s]\n",
+    "Saving vectors of label - 'AVH': 100%|████████████████████████████████████████████████| 35/35 [00:00<00:00, 137.62it/s]\n",
+    "Saving vectors of label - 'AVT': 100%|██████████████████████████████████████████████| 965/965 [00:07<00:00, 129.08it/s]'''"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 45,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -89,18 +101,26 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 46,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(657, 128, 32)\n"
+     ]
+    }
+   ],
    "source": [
     "# Setting channels to 1 to generalize stereo sound to 1 channel\n",
     "channels = 1\n",
-    "config.epochs = 50\n",
+    "config.epochs = 17\n",
     "config.batch_size = 100\n",
     "\n",
     "# Number of classes\n",
     "num_classes = 7\n",
-    "\n",
+    "print(X_train.shape)\n",
     "# Reshape X_train and X_test to include a 4th dimension (channels)\n",
     "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len, channels)\n",
     "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len, channels)\n",
@@ -109,26 +129,48 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 47,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(169, 50, 21, 1)\n"
+      "(657, 128, 32, 1)\n"
      ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<matplotlib.image.AxesImage at 0x25a47642788>"
+      ]
+     },
+     "execution_count": 47,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAF4AAAD7CAYAAADjAyMzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO1dbaw1V1V+1p5z3nvfttZSPmtppJpGLUQ+bGojiRKRCEioJkogBquQoAkoGIxQ/IE/JMGIKP6QpGIRkkr5NDaKIBLQ+IPaAo1QarEBhGL5igVa7vvenjN7+WNm7Vl7zdpzzr33vZ1p33mSm3vOfOzZs8/aa629vjYxM2Y8+Ahjd+BsxTzwI2Ee+JEwD/xImAd+JMwDPxKObeCJ6NlEdCcR3UVErz2u5zxUQcehxxNRBeDzAJ4F4G4AtwB4ETN/7ow/7CGKxTG1eyWAu5j5CwBARDcCuBqAO/AnaJdPhvPyg7Tlk4Ru5PrI6TBRe9AhrnTEIzz77KYx59kFoqXuAffh3m8x86PtJcc18BcD+Ir6fjeAn8z6RvQyAC8DgF06F1ed9/zmRF035xfbdY3l+qpqvj/wQHs8gpZtGzE69zXHeL1qD3AaXGkrXRu5a0sQubs3vVRo/xG4JYB/qd/9P16/j2vgPXrNyIOZrwNwHQD8xJN3+B8+9K+oKGDFzUAuqeq30KLmZtAq6kSU3Cf/m/N5NwJCdh4AVmi+73NMAq9qu79SXd5pn7VEldrei6vsOn3NHjfnHvt4/x2Oa+DvBnCJ+v54AP9bvHh1Ll79tSuxjhVWnMv7irqXX1AzSGtuXr5mSucD+tN+Pzavtwx1un4dm3sXIf8BTtVLLKn5QUPbZuTmBzhZrRDbH0OeEyjiVH0ia+Nk9UDq36l62R59u/vOx6XV3ALgMiK6lIhOAHghgJuO6VkPSRwLxTPzmoheAeDDACoA1zPz7cVOUMRjlvdlx/a5pdaWygMYVUuRdTsrVlylY+dVp7P7Iwecjg3VVYmSI/bqney63bBK5/S9+tiS6nRsxR0LlH4sw7p5TjsbahBWcXhoj4vVgJk/COCDx9X+Qx3HNvAHQaCIc6r97NhuK5xWiZ+HjAIBYBer3rEKzfcVFthZ5FpHhYhzwgO9Y81zFqkteeYuNffXCJCWhHMvqU7XSxunuTkbAOxW6+F3Hjw749gwCYqPHLBX72CfF4lvCpJmQjXQ8uegNJ1E6Yb/RxB2aJ0+A42cEH4sEF68F08kfi9t7vFOul+3K20J5D59jadlaUxj4EG4v97Biqs0qMISdhfdS1WGFegfSQSpqIDLUGeCEAAC1dhvr5NzMsjyX7crAn4VK5zTqoq7VKf7bT+kf1B9LWFmNSNhEhTPTInaPQoEGmpNn9Mipu5TVrtY1fdrdVJQ1yFr65zqdKe6ttftxWaBtKJFr43AoeurUSd3wiqpnyXMFD8SJkHx1FJ6oG6RJPxVKHLFVca/EwaEnhyLwovjMqmt8l8ocy+ewE4Qg1m/j0kmtN8risnEUCkzglwrsqSEaQw8GMtQI4DTYK2iFYyMnXZK76tVoRxbKqEn/+3aQK9atTYDNJpJiT1UFCG9CWawgWalCiARQQXurRcsZlYzEiZB8YIIcildUBnrodwDIFGbB203kc/Cyqz+D3SzR6uJwQhxbY+RPiRWtQVmih8Jk6B4VitDoeZE3S0vjaCetRHoZIHwdrl/SXXi6XJ9BPUWTvI9ghIftwujFVe91anMGN1HPQPsCtxiEgMfWmHUaAPCAppzMo2XVKcXzHT39v12sM7O6ZWuDELNAStPM0Kz8g0hV2dE8IoRTLe/pLq31hDCaDS0/Fz/nWeMgmlQPDF2wwq7WOGcanN4wW5r/Fpx1VGZUSu1Hi1Cbz8uO+eFpcjQF7RaJdw1gnPFVc9Go21JvfbtO298yxnHgklQPMAIFN3Vnlbj5Px+u37UfDy55tprPeEWwBA6jE4gREkgaurNHTO52pkEb1xgDycwhJniR8IkKL7mgHvX52YObUFQVKhVS8C3oQtFno7LZNvxFmHegkvutX1YcYX7W9VUtK6dsEYM+azRfdjE4ycx8BFNRIBW0eTlTyv2IwJO9O2aQ49laFuNqIGubcfo5ZptCbqBJMepwr0fex3bthCyH9vDzGpGwiQoPiAmaramXK3i3VfvZvdVFHsCUdhLZMI+n8iOnaw6ldBbnVrWpClZjsn//bhI1y9CzP4H5mzR5b/zjFEwCYrXtpq9Nh6xc6t1Kpvls/cr+3rntlOCFGKjUTy4JbVko1EzRM6FVukUnr3iKjlHZNZUiFgh74/GJh5/6IEnoksAvBPA4wBEANcx81uI6EIA7wbwBABfAvACZr53qC1WxivpcN2OwqruXs6+zI4K1Ug2Hs6nPQCc34b3VRR70Qi6TWF3nmna+oItEdg25XMJR2E1awCvZuYfA3AVgJcT0eUAXgvgo8x8GYCPtt9nGBya4pn5HgD3tJ/vI6I70CQkXA3gGe1l7wDwcQCvGWorMuFUvcTJapUoSsKctRnXCrPoqHlp5jC7013aFyEuMyty54SRY0mgonYFrsAKXt1GCWdEuBLREwA8FcDNAB7b/ijy4zymcM/LiOhWIrr11L2nvUse1jiycCWi8wC8H8CrmPm75OUKOdAZIY+5/JEcOTRU3hJwcmig46lyTGzva16kmSEe/50Udt3xWU8IJgqO1LXPOeVqAZwcNK3ArhF6q9MUBUF1z95vcaSBJ6IlmkG/gZk/0B7+OhFdxMz3ENFFAL6xqR0GmkwQDklgSraFsISFYjUiSANxpmUAik04rEavYL0lfabhIBeWveu5E97y4+sfbMnHZBamhrT/GsAdzPxmdeomANe0n68B8PeHfcbDGUeh+KcDeDGAzxDRbe2x1wF4I4D3ENFLAXwZwK9s01hlpncwxiwd87Lffl6EurdClBynZVX3ZoGmeCE5CUbVz06XKP+tPaZZmRx7oG3/ZOvGHMJRtJp/Rzkb9ZmHbfdswSRWroTG5KspeN1mV9jsPKCjag+SeReIM1URQAoT1NCuQEvBWqDKbLFRCkC3kNP3L2jOCJkkJkLxDbXniySxl2iTQTMbNFXLQsaqgOCOEr0FjkBnnNjYTOnLWtnXhZIjU3GGSHtDmMTAM5qXCIF7tpAq5AIS8O0snm3EXqdZhkCes49FerasjMV2s1ef6NYEjtCUdUVaFTNlBjwPM6sZCZOgeAInt5+mMgCoqVsQeVRtp7msPkuswLKbkwOBptpKKTNOWJ8OULUmZgCzI2SqmATF66SDUmxLIE6LnSykOuTWRu0cT8KxFcCRfJuLXJvCs82CK5By5bWPXoTYM2HoRd6mkO1JDDyDmpfliEiGPaATWDL1Pe1Efgxtrk3nxH6DTtOQwcrMwpTr6B6LElZWIXaBtSTGuPY7h42pODOrGQmToHgRrkCeSgl0LETHsmpWY9U7YVn6uJ41JZectkDaejXaLJxZIM31YsIG1Q+OI2TGwTEJihdUFF3VDGh48ZCKZu0yK64S9XcVPbpZYNuPTMnkZ6MTIlNv4aVjdOwM0W2UMJmBb9hA1RvA7BrjSdLXeCbcdJ5C71z6EcShQZ1ZwPpcvedEptSfnuENYU63nComQfFJnVSwEQVLqnvGK6BPlULJOiQvsRxHrup4nhSbAxOSR5yEvjYBey7C5oViyp8qYab4kTAJitew1CMLonUT3dK/gY31UjmhrWlWC2hpX4SkjtHRDnOvHQsdtSZtzbaaiWIaFM8dzyw5nPUiZh07h3in8rUZeEomCDQvXhdorUYohmnrJAdBtkDjvlbzkHCERBBO1SeyRDNrzFqGziy8NoPdXJ+zEO2dsm0CuQEMyANgJU5GKrouqE6CuROg3Q9qa2QCvsMk68vg2RnHhklQPEMq1/WhfagCW0kD6FsUNWuSc9odZ2eUt2A7EfqRAnK9tk4KRChXiDPFTxWToHhCw0e1w8FTHW3NX30MUmnbscenMltOgkHn0gu9GaItnckpPlAzuCvdxanmZQlnIlq4AnArgK8y8/OI6FIANwK4EMCnALyYmQcNFwQ/ll1jSXVyNORCshk4YSKiU69jp/d3Juey8NO6uNVIssGmjp0IkoBWg/1gpFu+EsAd6vsfA/izNiPkXgAvPQPPeNjhSANPRI8H8AsA3tZ+JwA/C+B97SXvAPCLG9tBnlskToYl1ThZrTITrMVOWGMnrFG3K9t1bP5E964QM9eddYbY6AbdB8HJapWeI9fL952wztYgYiPaDatexQ+No1L8nwP4fSDNu0cC+DYzy5y7G016Tg86I2Tv2/veJQ9rHCXr73kAvsHMnySiZ8hh51KX2emMkIufeAF//+IU9uoTeRFnjYD08+qVpRcwCjT8Vo51WSN14uVWSNYx9KyTUrFpSate+rx+lkCeV1EXF1TCUePjn09EzwWwC+B8NDPgAiJatFQ/uDfI2YyjxMdfC+BaAGgp/veY+VeJ6L0AfhmNZrNVRogu9Gb5eZZXVOWq4H5cJApMcTnKg5UskGGVtaU/i/1+iTozG2TX8LK30NrnEz1L53JAa7I4Dj3+NQBuJKI/AvBpNOk6g5B0y0WIWJIVSM1A7oZV72WWoc5eFmhYRgle4q8evCFDmHU76sjmFGAL7Th5EIxkzPxxNPmsaHc7u/JMtPtwxiRWrkDDIhaIvSTfTmDFrH4MgFSPGOiXutIU7DmoLUsbclyUFkOl9PpNJmFgttWMhklQfKCu+rR2GAOdg9qr3rFEfzsKL8lXZtFe7AtErQLq/jR9aP7tx0XvvkBcpPRNghWYyMDLjglLqnuCSqAL8whOY9mb1hJRvIpV0qW9qISeMU01baMHKsSk/2c/AOXXd4G2x1u9Y8YRMAmKB/UdE940lpWkDvNbofsM+I4QwYLWvXA+uU/7XL0wbc9iWapl4D3bYqb4kTAJipdicLpWZM9mQ36ZwZ4lqP3uVW/yQrE92NjJdQwpesG7bsh5U8IkBp7VTgl2unvQL6hTHDU8v6cW2F60cOnH0Mc9AWpXtd4quNfm4NkZx4bJULw4MWxROIEXBOrVJkjnQu2G4A2Fgdss8a6MVrehgDYrl9YEkSnlRZUwU/xImAbFMxK1x1RNoytxJZCqTUHt+dHVIm6ukZlx/3qnt+jZj4tiXYPI1As+9ZzYunpHyREiGYxDmCl+JEyC4gmtqoca9YBmIUm7wqcXoZwzBfTde5Ep2e9tXI2HTct+y+NTU8PEDmAiA98UgytHCcOcT8WEVOyMDq2zSLp76AdMZe1zfr0OVPV+4FQNNvZN0rOtZqKYBMULPGq1ZuLSPVYwAuhNeYmxydprI8N0VIJAKLpmZfJFf+bZ7L+lt8+sfa/BszOODZOheBvxZQWXFpRabUv82LGX9PJUqe+Y9iyK1h5/slolCpZQ78iU5U9prNSCq4TJDHyNgIUOQorNC9q6YkDu6SnZdDzBK6tjACklR2eEJGHpVNoTW1IS4uQMuPyoPMfHTxaToPhAjPNkK2fxp4XuHJBbG0XNG6oPrNnQkHPEY2VekX5rx9EqY0rFb5vexjw8U/xImATFS72a2NZcBTr7SNq4UBVm0xZMa4/XDmpBV721e11d/N+2m0qnK9XRqqsVYlJF7TMDdXuPl3DUMuYXoImNfxKadd9LANyJQ+4R4iWMCbSZV6Bj3Ycqbuhztk6ZJ7S9TG2LLJe1PZ1pTMPjfmRW8xYAH2LmHwXwZDSZIfMeIVvgKPHx5wP4aQC/DgBtntMDRHTgPUIEmiIlYcyqhEBXeS8LQpLZ0B7TcTjZLCqkSGr0dseh0Fsn6NWpRDqnWJ2wOYzvKBT/QwC+CeDtRPRpInobEZ2LQ+wRsnfv2ZcRcpSBXwB4GoC3MvNTAXwPB2ArzHwdM1/BzFec+4gTqSzWflxgPy5STpPAOwY0oX26OFxFERVF7IR1Wgl7dRKWoQnxkzypTCa0bcr9dlUNdOUYV7HfZymlNbR6PcrA3w3gbma+uf3+PjQ/xNfbvUGw7R4hZyOOkhHyNSL6ChH9CDPfiWaXhM+1f9eg2bJiy4yQkPikLV+oqUx46Ol2B+MKsScD4oaluq1F49l23O3oWujw7lRsQlUTARr5dNzVO34bwA1EdALAFwD8BppZdKA9QsTn2gSC+pNQl5TVJWu9YkD6GrlX0MXC5Lp3JCpW+ygFR3XnV737NjlCjjTwzHwbgCucU/MeIRswiZVrRRHnL05nyWQCHcrXcwMyejadbfd6qs3MWlKdjnmUbylY1z6wSWvLUGfFpT3MtpqRMAmKZ1BSx9wKqMgFlqYwzxkux629ZEn1YIRXL8FgYBbValFlM09WsTpeHn+moEvIWluL/p7COtQGKd6WQ0BTusS2pe0r7orYpNLoqGTLToDuB5EgKm2wm+PjJ4pJULxYJwNxciZ4FCyw3n3Aj3f3BKKQmg2x1iF5sY2T0VSedHtVydVGRWi2F/wSDl1/B8/OODZMguIFuae/oQnZUPFk9YC7srTVO4bykGqEXui2dnCLA8QKVS/6QWcoBnMf4oO0g/GMg2MSFB/APRuNbDmn3WlDBTy9LMHeLmqIxZ1vNI8Xt+Om2JiihhSAxUOhcD9tkRRWUtE8f6dgqOCbQD/XxtPoge2CorrVrS02J46ZLFSwgJnVjIRJULwsoHbCuu92a6FrCwv0BupDFOatIu1zdInzUn2EErydeeYw7YliEhQPNBS34iqpe/3qGl0tX70DWinzzpsNkalLNjALHL0Yslti6CJyKXYyxJ65Qqu7DwlbDVF/+2X7UnXs12TXg+VllFjWoWsfrOr+9nLyLK+y31Cmia2LsA1mVjMSJkHxescEa0spRZYBedi1QIfmeanudkZpXd8+07NICnQUm25fzs3q5EQxDYpvsQgxC/zX8GIh5R6gnz9VkU/VArGv6Gskek1mUdpvJNRJoHuVQHoCHv29TnrvOnj2QYKsXGuEnnail+NeHYIUmz4webVwHWIPAstiVrHbCs+r5GG9YJsqdwAzqxkNk6B4KQanq154Jl0rSGuEntPCyyDxMGT3WTlC2X6uEHv6fjpHxxu0OuMImAbFww+XA3wVLbNIFiLPhsqNe8e0jPAc7T2o527Tfu/84NkNIKLfJaLbieizRPQuItolokuJ6GYi+m8iencb3jfD4NADT0QXA/gdAFcw85MAVABeiEPsESLVOxYh9rZ/8DQRCYeWpbzWImSrCgnLlow9OVaCd05cfuJUEWeJLMT0ca8K1BCOyuMXAE4S0QLAOQDuwSH2CBFWI8WCPKGo49x1TLuNW5fvTUn0fED0jyHx6/KjNGVZmh99EWLmNJF7kgJgVqZe3L7E6Zdw6IFn5q8CeBOaiOB7AHwHwCdxiD1CTp2FGSFHyYF6BICrAVwK4NsA3gvgOc6lrpTRe4Q87vILuSjsuFsV2hRJTXneoiWZkZX1sFTtQwfFCvT3IduRxALpdzjOoNWfA/BFZv4mM68AfADAT6HdI6S9Zt4jpICjqJNfBnAVEZ0D4BSamPhbAXwMB9wjRCo06arVEuNikxE0tLXR1ozRDmeJGtAWSG9bCi9t3numwM6Qg1RcPUoqzs1E9D4028qt0ewHch2Af8QB9wgJ1NSP1ymS1lYD9Adrk/nVsgcNOzj6h/XYl2e069mOqNxXi6NmhLwewOvN4XmPkC0wjZWrSoXsUacqieXVHyhRtaZoKdbmsYltohS8VbBeP0iNBGnTLdNlMNtqRsIkKF7s8StUrhAVeG4+136D0n6w3UzpFd1HVVRNS/b/0ko4MhVtSKnvg2dHhFemxKa8ZOF6Km4dyEvWeoY2m5aZJZiZPVs9NqS3jrZJa3bjdw8zqxkJk6J4bXexBYB0bLte1Vr2YKlQQxvcrFD1opGjWvFaluZlo8gMqzdsNwfMFD8aJkHxBE4VmOwCSqtywsczAWoqsCYKRt1bhHmZHRridrS1DKQSiIa+38s8nINWJ4pJUHzNAfevmwKfngUSyDWLjMJMQbahbDtPcxFoCvVie+wM0WHdNgtRa1QlTGLgAzFOVqt8elI/jkWQBoFCceWqBWJWMbWwYdcmuHuLGLVTv8+8K85EMQmKBzprn52+3p4cmyII5Lgt3JxRoqNultL5Ad8snajfVBARF+MQZoofCZOgeMmBArBx/yQgX1wNUpah6oC6uA+UlgklAV+C50AZsjkBExl4oq70VSmYqKaQfJuH2VtPt6WhBbDN6PbMvJ5J2mvL2xswu/ZAPZ9xxjAJitewwtINo9tgcrXtdAls6B3T7EHiYKwdJ9ucUfl2bd90XP2cETJRTILimf0dgQFftduGz3rntEyQsG5dvWNoG1KBm2Zv2t9UAwGYyMAT+mZg68TQ5wResZ5tE8CGsktsrTG9we824R3bYGY1I2EaFK9yoIYgsyJT94y9RK9MPbWzVK/Amx0ShuepoRViUhlt1Y/MGFfATPEjYRIUz84K1KOY3tZu6MpTpQBV3Y41GSuq9lIwSxsw6nv1NdrxYft+5NJYRHQ9EX2DiD6rjl1IRB9psz4+0kYOgxr8BRHdRUT/SURP29T+2YptWM3fAHi2OVbaB+Q5AC5r/14G4K1b9YK6BYlNNBB42SE6mUBnatjZIgkKkgmo/zzo5IOh/lhIgsI26uTGgWfmfwPwf+bw1WiyPYA86+NqAO/kBp9AE7J90cZetHD9oGoQbKaG1edlipfSdOS4/rO1y/SPoVN4bHaJ/pHtfYsQ070lHFa4lvYBuRjAV9R1W2WEnI17hJxp4erpUBszQi564iO4tPjYFKveO6+aGaph49mASitRL2Jhk7q4CYel+NI+IHcDuERdN2eEFHDYgb8JTbYHkGd93ATg11rt5ioA3xGWtLEjigKtYNP8VPNbT5h6wlBfo3eusVQrMkTa0DLCZvZ5glfa9HbRsdjIaojoXQCeAeBRRHQ3mkSEN8LfB+SDAJ4L4C4Ae2j2DNkKIhiHnAvCOjZFAgvslnC6fV2cObtWQQS5zpHdJk1nk78V2GLgmflFhVO9fUCYmQG8fONTZ0xj5Qo01FJzn5I2xbtbStSlcu1mAFlY9xa2mpTx0VYIBLqkOHm+vvcgAne21YyEyVC8CCXJJ/JC4NyQ7AKRVYhuxIJXN1ie37PpqGg2W+J8aDU750BNGJOgeImr8RzI1sKojwGODFBh20OahxcUOxSVNtRW6gP1Z08Jkxh4wYqrXjlzL0BVD5YVuptUORtjr9XV0mBtalPYo2ZDc2msiWISFE9op7OTO1RK7pXvvZKDKgPPCkJN0dbmYgvA6XNevLvnTPdi+EuYKX4kTIPiFU8slQbXBT/1YqaUDOyFYWgn9EESE3QFWI+SPbfgxjY3XvEgoeSZ17YVyzqGBkHfmw2MyTSRwRKtCtA2nq59SxAeqzlIEO3MakbCJCienSLI2zgqhqjbSyYLxL30zLRxY+uuAzpB66mEttyW92yvrxYzxY+ESVA8gVMEgKUeTXXbeO8FWb5TCx3v4lGpnS2eiun1K6Xxm8IUQ5jEwAOix5eX7aXNVlwB6rQNICu9ZQd5k8fIQjtV7H16Y/cSZlYzEiZB8Vq4lmwcpYztkglWq3ZeXUiP8odMvl5439As0/vPepgpfiRMguIFkakn0IZWnW5U2Yb8KL3Zub7Pm2nZFhe0ubaC7su8g/FEMSmK11Rt+a1e7nuLqpKTXEPHxtjZs88L1wJp2y7FbmronNkSJjXwpcwLwKw61TwtbVWn6yKkQeDO0DYU9OQJ2SGP1ZCNqYSZ1YyESVA8c+64APrCsi7MXFvGyhN6cq/nC91E1VlfNkD6ol2YJRw2I+RPiOi/2qyPvyOiC9S5a9uMkDuJ6Oe36vFZiMNmhHwEwJOY+ccBfB7AtQBARJej2Sfkie09f0lEGwMJA3HK6vC2l4hMKX5R4m9s0oG93vuzz9TP08GqpT7aQFXbvrdfSPGdN13gZYQw8z+r7Sg+gSYcG2gyQm5k5n1m/iKa4NWNlbUZXcCqjebVgyaZF96eHN6gbXNMP2/oGg8Hte9k9x7qrhwvAfBP7ec5I2RLHEm4EtEfoCnaf4Mcci7bmBHy2MsvZBGuQ75QG4WgnSderPsQPHWyZKvZobWbmJxYXUu+tmDQEI6yOcs1AJ4H4JlteDYwZ4RsjUMNPBE9G8BrAPwMM++pUzcB+FsiejOAH0CTdvkfG9uDbx/X/zW8gm5Dqt+QFdFbUG1rj7H1iz1LZwmHzQi5FsAOgI9QYzz6BDP/FjPfTkTvAfA5NCzo5cy8UcR7tQyGWIHAKzPrQQZyzQtX2NrrhiKBS/0rHSvhsBkhxQ1XmPkNAN6wdQ/OUkxj5QrqTfkhCtO7Ggt6Zcx1DUsVGTCkp9tnyvedsEZEbgvy+ljqr3t+8OyMY8M0KL611XibpQ/tA7WJr9uKHpvkhDg7bDihF6atzdQ2qmAbXj9T/EiYBMULIlPRdbeOwSWTUqCoDuEW6EWVt/GKbE3n8Xore7SGZCuzbpOYMImBl0qrOiLYDmhJ7bMbd2U+1AGPVfqBuazbawyF6XlBrnNR54mCutX+iJ0g+iaA7wH41th92RKPwvZ9/UFmfrQ9OImBBwAiupWZrxi7H9vgTPR1ZjUjYR74kTClgb9u7A4cAEfu62R4/NmGKVH8WYV54EfCJAaeiJ7dxuHcRUSv3XzHgwMiuoSIPkZEdxDR7UT0yvb4HxLRV4notvbvuQdue2we38bdfB7As9D4bG8B8CJm/tyoHUOqMHgRM3+KiL4PwCfRFDd9AYD7mflNh217ChR/JYC7mPkLzPwAgBvRxOeMDma+h5k/1X6+D8AdKISrHBRTGPitY3HGBBE9AcBTAdzcHnpFG8J4vRS1PgimMPBbx+KMBSI6D8D7AbyKmb+Lplj1DwN4CoB7APzpQducwsBPOhaHiJZoBv0GZv4AADDz15m5ZuYI4K+wRZiixRQG/hYAlxHRpUR0Ak3Q600j9wlAUw8fTUTFHcz8ZnVcVwj/JQCftfduwuiOEGZeE9ErAHwYQAXgema+feRuCZ4O4MUAPkNEt7XHXgfgRUT0FDQs8UsAfvOgDY+uTp6tmAKrOSsxD/xImAd+JMwDPxLmgR8J88CPhHngR8L/AyCrtxddH+8AAAABSURBVMS0MXtBAAAAAElFTkSuQmCC\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
     }
    ],
    "source": [
     "# Spectrogram visualized of 0th element\n",
     "print(X_train.shape)\n",
-    "#plt.imshow(X_train[500, :, :, 0])"
+    "plt.imshow(X_train[200, :, :, 0])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 48,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -140,51 +182,49 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 49,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
-      "\n",
-      "Model: \"sequential_1\"\n",
+      "Model: \"sequential_4\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_1 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_10 (Conv2D)           (None, 126, 30, 21)       210       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_1 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_7 (MaxPooling2 (None, 63, 15, 21)        0         \n",
       "_________________________________________________________________\n",
-      "activation_1 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_16 (Activation)   (None, 63, 15, 21)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_2 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_11 (Conv2D)           (None, 61, 13, 48)        9120      \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_2 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_8 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_2 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_17 (Activation)   (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_3 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_12 (Conv2D)           (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_3 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_18 (Activation)   (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_1 (Flatten)          (None, 1296)              0         \n",
+      "flatten_4 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_1 (Dropout)          (None, 1296)              0         \n",
+      "dropout_7 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_1 (Dense)              (None, 64)                83008     \n",
+      "dense_7 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_4 (Activation)    (None, 64)                0         \n",
+      "activation_19 (Activation)   (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dropout_2 (Dropout)          (None, 64)                0         \n",
+      "dropout_8 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 7)                 455       \n",
+      "dense_8 (Dense)              (None, 7)                 455       \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 7)                 0         \n",
+      "activation_20 (Activation)   (None, 7)                 0         \n",
       "=================================================================\n",
-      "Total params: 101,079\n",
-      "Trainable params: 101,079\n",
+      "Total params: 532,905\n",
+      "Trainable params: 532,905\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -195,7 +235,7 @@
        "\"model.add(Conv2D(32, (3, 3),\\n    input_shape=(config.buckets, config.max_len, channels),\\n    activation='relu'))\\n\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\""
       ]
      },
-     "execution_count": 7,
+     "execution_count": 49,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -206,7 +246,7 @@
     "\n",
     "input_shape= (config.buckets, config.max_len, channels)\n",
     "\n",
-    "model.add(Conv2D(24, (3, 3), strides=(1, 1), input_shape=input_shape))\n",
+    "model.add(Conv2D(21, (3, 3), strides=(1, 1), input_shape=input_shape))\n",
     "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
     "model.add(Activation('relu'))\n",
     "\n",
@@ -244,7 +284,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 50,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -256,7 +296,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 51,
    "metadata": {
     "scrolled": false
    },
@@ -267,7 +307,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/1r446vev\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/1r446vev</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2rtny7n6\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2rtny7n6</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -278,139 +318,77 @@
      "output_type": "display_data"
     },
     {
-     "name": "stderr",
+     "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Failed to connect to W&B servers after 10 seconds.                    Letting user process proceed while attempting to reconnect.\n"
+      "(657, 7)\n",
+      "(7,)\n",
+      "(657, 128, 32, 1)\n",
+      "Train on 657 samples, validate on 439 samples\n",
+      "Epoch 1/17\n",
+      "657/657 [==============================] - ETA: 19s - loss: 4.0558 - accuracy: 0.156 - ETA: 10s - loss: 3.1060 - accuracy: 0.406 - ETA: 7s - loss: 3.2798 - accuracy: 0.447 - ETA: 6s - loss: 3.0464 - accuracy: 0.45 - ETA: 5s - loss: 2.8030 - accuracy: 0.46 - ETA: 4s - loss: 2.6097 - accuracy: 0.45 - ETA: 3s - loss: 2.4701 - accuracy: 0.42 - ETA: 3s - loss: 2.3784 - accuracy: 0.42 - ETA: 3s - loss: 2.2925 - accuracy: 0.43 - ETA: 2s - loss: 2.2017 - accuracy: 0.44 - ETA: 2s - loss: 2.1122 - accuracy: 0.46 - ETA: 2s - loss: 2.0394 - accuracy: 0.47 - ETA: 1s - loss: 1.9996 - accuracy: 0.48 - ETA: 1s - loss: 1.9450 - accuracy: 0.49 - ETA: 1s - loss: 1.9012 - accuracy: 0.50 - ETA: 1s - loss: 1.8715 - accuracy: 0.50 - ETA: 0s - loss: 1.8469 - accuracy: 0.52 - ETA: 0s - loss: 1.8115 - accuracy: 0.52 - ETA: 0s - loss: 1.7669 - accuracy: 0.53 - ETA: 0s - loss: 1.7500 - accuracy: 0.54 - 6s 9ms/step - loss: 1.7466 - accuracy: 0.5403 - val_loss: 0.9263 - val_accuracy: 0.7403\n",
+      "Epoch 2/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 1.1509 - accuracy: 0.65 - ETA: 4s - loss: 1.2885 - accuracy: 0.65 - ETA: 3s - loss: 1.2698 - accuracy: 0.63 - ETA: 3s - loss: 1.2546 - accuracy: 0.62 - ETA: 3s - loss: 1.2028 - accuracy: 0.65 - ETA: 2s - loss: 1.2129 - accuracy: 0.64 - ETA: 2s - loss: 1.2027 - accuracy: 0.63 - ETA: 2s - loss: 1.1956 - accuracy: 0.62 - ETA: 2s - loss: 1.1730 - accuracy: 0.63 - ETA: 2s - loss: 1.1439 - accuracy: 0.64 - ETA: 1s - loss: 1.1532 - accuracy: 0.65 - ETA: 1s - loss: 1.1592 - accuracy: 0.65 - ETA: 1s - loss: 1.1075 - accuracy: 0.66 - ETA: 1s - loss: 1.1140 - accuracy: 0.66 - ETA: 1s - loss: 1.1062 - accuracy: 0.67 - ETA: 1s - loss: 1.0849 - accuracy: 0.67 - ETA: 0s - loss: 1.0751 - accuracy: 0.68 - ETA: 0s - loss: 1.0769 - accuracy: 0.67 - ETA: 0s - loss: 1.0793 - accuracy: 0.68 - ETA: 0s - loss: 1.0833 - accuracy: 0.67 - 7s 11ms/step - loss: 1.0879 - accuracy: 0.6773 - val_loss: 0.7706 - val_accuracy: 0.7950\n",
+      "Epoch 3/17\n",
+      "657/657 [==============================] - ETA: 3s - loss: 0.9944 - accuracy: 0.78 - ETA: 4s - loss: 1.0090 - accuracy: 0.71 - ETA: 3s - loss: 1.0620 - accuracy: 0.69 - ETA: 3s - loss: 0.9736 - accuracy: 0.72 - ETA: 3s - loss: 0.9877 - accuracy: 0.72 - ETA: 3s - loss: 1.0172 - accuracy: 0.71 - ETA: 3s - loss: 0.9724 - accuracy: 0.72 - ETA: 3s - loss: 0.9700 - accuracy: 0.73 - ETA: 2s - loss: 0.9516 - accuracy: 0.74 - ETA: 2s - loss: 0.9033 - accuracy: 0.75 - ETA: 2s - loss: 0.9295 - accuracy: 0.75 - ETA: 2s - loss: 0.9624 - accuracy: 0.75 - ETA: 1s - loss: 1.0083 - accuracy: 0.73 - ETA: 1s - loss: 0.9966 - accuracy: 0.73 - ETA: 1s - loss: 0.9908 - accuracy: 0.73 - ETA: 1s - loss: 0.9704 - accuracy: 0.74 - ETA: 0s - loss: 0.9569 - accuracy: 0.74 - ETA: 0s - loss: 0.9406 - accuracy: 0.74 - ETA: 0s - loss: 0.9327 - accuracy: 0.74 - ETA: 0s - loss: 0.9367 - accuracy: 0.74 - 7s 11ms/step - loss: 0.9294 - accuracy: 0.7397 - val_loss: 0.6112 - val_accuracy: 0.8132\n",
+      "Epoch 4/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 0.9354 - accuracy: 0.75 - ETA: 5s - loss: 0.7274 - accuracy: 0.81 - ETA: 6s - loss: 0.7188 - accuracy: 0.80 - ETA: 6s - loss: 0.7575 - accuracy: 0.78 - ETA: 6s - loss: 0.7361 - accuracy: 0.78 - ETA: 5s - loss: 0.7440 - accuracy: 0.78 - ETA: 4s - loss: 0.7807 - accuracy: 0.76 - ETA: 4s - loss: 0.7925 - accuracy: 0.76 - ETA: 3s - loss: 0.7831 - accuracy: 0.76 - ETA: 3s - loss: 0.7635 - accuracy: 0.76 - ETA: 3s - loss: 0.7662 - accuracy: 0.76 - ETA: 3s - loss: 0.8079 - accuracy: 0.75 - ETA: 2s - loss: 0.8197 - accuracy: 0.75 - ETA: 2s - loss: 0.8151 - accuracy: 0.75 - ETA: 2s - loss: 0.8265 - accuracy: 0.74 - ETA: 1s - loss: 0.8232 - accuracy: 0.74 - ETA: 1s - loss: 0.8224 - accuracy: 0.74 - ETA: 0s - loss: 0.8200 - accuracy: 0.75 - ETA: 0s - loss: 0.8059 - accuracy: 0.75 - ETA: 0s - loss: 0.7877 - accuracy: 0.76 - 9s 14ms/step - loss: 0.7801 - accuracy: 0.7626 - val_loss: 0.5614 - val_accuracy: 0.8292\n",
+      "Epoch 5/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 0.8717 - accuracy: 0.78 - ETA: 3s - loss: 0.7388 - accuracy: 0.79 - ETA: 3s - loss: 0.7251 - accuracy: 0.79 - ETA: 3s - loss: 0.9180 - accuracy: 0.72 - ETA: 3s - loss: 0.8470 - accuracy: 0.74 - ETA: 3s - loss: 0.8564 - accuracy: 0.73 - ETA: 2s - loss: 0.7896 - accuracy: 0.76 - ETA: 2s - loss: 0.7865 - accuracy: 0.76 - ETA: 2s - loss: 0.7887 - accuracy: 0.74 - ETA: 2s - loss: 0.7887 - accuracy: 0.75 - ETA: 2s - loss: 0.8024 - accuracy: 0.74 - ETA: 2s - loss: 0.7989 - accuracy: 0.74 - ETA: 1s - loss: 0.7856 - accuracy: 0.75 - ETA: 1s - loss: 0.7943 - accuracy: 0.75 - ETA: 1s - loss: 0.7788 - accuracy: 0.76 - ETA: 1s - loss: 0.7626 - accuracy: 0.76 - ETA: 0s - loss: 0.7563 - accuracy: 0.77 - ETA: 0s - loss: 0.7758 - accuracy: 0.76 - ETA: 0s - loss: 0.7528 - accuracy: 0.77 - ETA: 0s - loss: 0.7622 - accuracy: 0.76 - 7s 11ms/step - loss: 0.7514 - accuracy: 0.7702 - val_loss: 0.4817 - val_accuracy: 0.8497\n",
+      "Epoch 6/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 0.8847 - accuracy: 0.78 - ETA: 4s - loss: 0.7266 - accuracy: 0.78 - ETA: 4s - loss: 0.7128 - accuracy: 0.80 - ETA: 4s - loss: 0.7204 - accuracy: 0.77 - ETA: 4s - loss: 0.7218 - accuracy: 0.77 - ETA: 4s - loss: 0.7009 - accuracy: 0.78 - ETA: 3s - loss: 0.6731 - accuracy: 0.79 - ETA: 3s - loss: 0.6392 - accuracy: 0.81 - ETA: 3s - loss: 0.6047 - accuracy: 0.82 - ETA: 2s - loss: 0.5835 - accuracy: 0.82 - ETA: 2s - loss: 0.5918 - accuracy: 0.81 - ETA: 2s - loss: 0.5608 - accuracy: 0.82 - ETA: 2s - loss: 0.6113 - accuracy: 0.81 - ETA: 1s - loss: 0.6224 - accuracy: 0.81 - ETA: 1s - loss: 0.6306 - accuracy: 0.80 - ETA: 1s - loss: 0.6313 - accuracy: 0.79 - ETA: 1s - loss: 0.6271 - accuracy: 0.79 - ETA: 0s - loss: 0.6260 - accuracy: 0.79 - ETA: 0s - loss: 0.6283 - accuracy: 0.79 - ETA: 0s - loss: 0.6135 - accuracy: 0.80 - 10s 15ms/step - loss: 0.6093 - accuracy: 0.8006 - val_loss: 0.4722 - val_accuracy: 0.8542\n",
+      "Epoch 7/17\n",
+      "657/657 [==============================] - ETA: 23s - loss: 0.7458 - accuracy: 0.750 - ETA: 17s - loss: 0.6467 - accuracy: 0.812 - ETA: 15s - loss: 0.5985 - accuracy: 0.812 - ETA: 13s - loss: 0.6466 - accuracy: 0.796 - ETA: 12s - loss: 0.6417 - accuracy: 0.793 - ETA: 13s - loss: 0.6491 - accuracy: 0.786 - ETA: 11s - loss: 0.6074 - accuracy: 0.803 - ETA: 10s - loss: 0.5828 - accuracy: 0.812 - ETA: 10s - loss: 0.5942 - accuracy: 0.809 - ETA: 9s - loss: 0.5959 - accuracy: 0.806 - ETA: 7s - loss: 0.5921 - accuracy: 0.80 - ETA: 6s - loss: 0.5746 - accuracy: 0.81 - ETA: 5s - loss: 0.5906 - accuracy: 0.79 - ETA: 4s - loss: 0.5923 - accuracy: 0.80 - ETA: 3s - loss: 0.5961 - accuracy: 0.79 - ETA: 3s - loss: 0.5974 - accuracy: 0.79 - ETA: 2s - loss: 0.6017 - accuracy: 0.79 - ETA: 1s - loss: 0.5861 - accuracy: 0.79 - ETA: 0s - loss: 0.5895 - accuracy: 0.79 - ETA: 0s - loss: 0.5829 - accuracy: 0.80 - 15s 24ms/step - loss: 0.5745 - accuracy: 0.8052 - val_loss: 0.4689 - val_accuracy: 0.8428\n",
+      "Epoch 8/17\n",
+      "657/657 [==============================] - ETA: 5s - loss: 0.3866 - accuracy: 0.90 - ETA: 5s - loss: 0.4083 - accuracy: 0.89 - ETA: 5s - loss: 0.4690 - accuracy: 0.85 - ETA: 4s - loss: 0.4655 - accuracy: 0.85 - ETA: 4s - loss: 0.4468 - accuracy: 0.86 - ETA: 3s - loss: 0.4656 - accuracy: 0.85 - ETA: 3s - loss: 0.4574 - accuracy: 0.85 - ETA: 3s - loss: 0.4881 - accuracy: 0.82 - ETA: 3s - loss: 0.4773 - accuracy: 0.84 - ETA: 2s - loss: 0.4800 - accuracy: 0.83 - ETA: 2s - loss: 0.4944 - accuracy: 0.82 - ETA: 2s - loss: 0.5078 - accuracy: 0.82 - ETA: 2s - loss: 0.5019 - accuracy: 0.82 - ETA: 1s - loss: 0.4889 - accuracy: 0.83 - ETA: 1s - loss: 0.4874 - accuracy: 0.83 - ETA: 1s - loss: 0.4903 - accuracy: 0.83 - ETA: 0s - loss: 0.5046 - accuracy: 0.83 - ETA: 0s - loss: 0.5251 - accuracy: 0.82 - ETA: 0s - loss: 0.5299 - accuracy: 0.81 - ETA: 0s - loss: 0.5244 - accuracy: 0.82 - 7s 11ms/step - loss: 0.5228 - accuracy: 0.8189 - val_loss: 0.4748 - val_accuracy: 0.8428\n",
+      "Epoch 9/17\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(169, 7)\n",
-      "(7,)\n",
-      "(169, 50, 21, 1)\n",
-      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
-      "\n",
-      "Train on 169 samples, validate on 113 samples\n",
-      "Epoch 1/50\n",
-      "169/169 [==============================] - ETA: 3s - loss: 7.2267 - accuracy: 0.12 - ETA: 1s - loss: 5.7420 - accuracy: 0.15 - ETA: 0s - loss: 4.6317 - accuracy: 0.25 - ETA: 0s - loss: 3.7351 - accuracy: 0.34 - 1s 8ms/step - loss: 3.6083 - accuracy: 0.3609 - val_loss: 1.2447 - val_accuracy: 0.6991\n",
-      "Epoch 2/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.5250 - accuracy: 0.50 - ETA: 0s - loss: 1.6566 - accuracy: 0.50 - ETA: 0s - loss: 1.7947 - accuracy: 0.47 - ETA: 0s - loss: 1.8819 - accuracy: 0.46 - 0s 2ms/step - loss: 1.9045 - accuracy: 0.4675 - val_loss: 1.2482 - val_accuracy: 0.6991\n",
-      "Epoch 3/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.7073 - accuracy: 0.43 - ETA: 0s - loss: 2.0419 - accuracy: 0.39 - ETA: 0s - loss: 1.8637 - accuracy: 0.43 - ETA: 0s - loss: 1.8119 - accuracy: 0.40 - ETA: 0s - loss: 1.7048 - accuracy: 0.43 - 0s 3ms/step - loss: 1.6837 - accuracy: 0.4438 - val_loss: 1.1829 - val_accuracy: 0.7168\n",
-      "Epoch 4/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.6658 - accuracy: 0.46 - ETA: 0s - loss: 1.4895 - accuracy: 0.55 - ETA: 0s - loss: 1.5428 - accuracy: 0.52 - 0s 1ms/step - loss: 1.5334 - accuracy: 0.5325 - val_loss: 1.1670 - val_accuracy: 0.6991\n",
-      "Epoch 5/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.1465 - accuracy: 0.75 - ETA: 0s - loss: 1.4339 - accuracy: 0.59 - ETA: 0s - loss: 1.4026 - accuracy: 0.58 - 0s 2ms/step - loss: 1.3955 - accuracy: 0.5917 - val_loss: 1.0805 - val_accuracy: 0.7080\n",
-      "Epoch 6/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.0407 - accuracy: 0.68 - ETA: 0s - loss: 1.2186 - accuracy: 0.61 - ETA: 0s - loss: 1.3785 - accuracy: 0.55 - 0s 2ms/step - loss: 1.3789 - accuracy: 0.5621 - val_loss: 0.9999 - val_accuracy: 0.7080\n",
-      "Epoch 7/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.3955 - accuracy: 0.59 - ETA: 0s - loss: 1.3306 - accuracy: 0.59 - ETA: 0s - loss: 1.3788 - accuracy: 0.54 - ETA: 0s - loss: 1.3117 - accuracy: 0.58 - 0s 2ms/step - loss: 1.3064 - accuracy: 0.5858 - val_loss: 1.0428 - val_accuracy: 0.7168\n",
-      "Epoch 8/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.3417 - accuracy: 0.59 - ETA: 0s - loss: 1.2640 - accuracy: 0.60 - ETA: 0s - loss: 1.2015 - accuracy: 0.65 - 0s 1ms/step - loss: 1.2218 - accuracy: 0.6568 - val_loss: 0.9011 - val_accuracy: 0.7257\n",
-      "Epoch 9/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.1504 - accuracy: 0.62 - ETA: 0s - loss: 1.2459 - accuracy: 0.60 - ETA: 0s - loss: 1.1888 - accuracy: 0.63 - 0s 2ms/step - loss: 1.1826 - accuracy: 0.6272 - val_loss: 0.8624 - val_accuracy: 0.7522\n",
-      "Epoch 10/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.3000 - accuracy: 0.46 - ETA: 0s - loss: 1.2544 - accuracy: 0.57 - ETA: 0s - loss: 1.1616 - accuracy: 0.60 - ETA: 0s - loss: 1.0447 - accuracy: 0.64 - ETA: 0s - loss: 1.0728 - accuracy: 0.63 - 0s 2ms/step - loss: 1.0814 - accuracy: 0.6331 - val_loss: 0.8037 - val_accuracy: 0.7522\n",
-      "Epoch 11/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.9429 - accuracy: 0.68 - ETA: 0s - loss: 0.8877 - accuracy: 0.71 - ETA: 0s - loss: 0.8898 - accuracy: 0.71 - ETA: 0s - loss: 0.9820 - accuracy: 0.68 - 0s 2ms/step - loss: 0.9709 - accuracy: 0.6864 - val_loss: 0.7861 - val_accuracy: 0.7522\n",
-      "Epoch 12/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.9820 - accuracy: 0.75 - ETA: 0s - loss: 0.9991 - accuracy: 0.71 - ETA: 0s - loss: 0.9899 - accuracy: 0.68 - 0s 1ms/step - loss: 1.0027 - accuracy: 0.6686 - val_loss: 0.8406 - val_accuracy: 0.7699\n",
-      "Epoch 13/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.0651 - accuracy: 0.65 - ETA: 0s - loss: 1.0189 - accuracy: 0.62 - ETA: 0s - loss: 1.0346 - accuracy: 0.64 - ETA: 0s - loss: 1.0537 - accuracy: 0.64 - 0s 2ms/step - loss: 1.0476 - accuracy: 0.6509 - val_loss: 0.8063 - val_accuracy: 0.7611\n",
-      "Epoch 14/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.1256 - accuracy: 0.68 - ETA: 0s - loss: 1.0219 - accuracy: 0.67 - ETA: 0s - loss: 0.9018 - accuracy: 0.71 - ETA: 0s - loss: 0.8877 - accuracy: 0.71 - 0s 2ms/step - loss: 0.8748 - accuracy: 0.7219 - val_loss: 0.7731 - val_accuracy: 0.7345\n",
-      "Epoch 15/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.9091 - accuracy: 0.62 - ETA: 0s - loss: 0.7913 - accuracy: 0.73 - ETA: 0s - loss: 0.8411 - accuracy: 0.71 - ETA: 0s - loss: 0.8311 - accuracy: 0.72 - ETA: 0s - loss: 0.8319 - accuracy: 0.71 - 0s 2ms/step - loss: 0.8334 - accuracy: 0.7160 - val_loss: 0.7657 - val_accuracy: 0.7522\n",
-      "Epoch 16/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.78 - ETA: 0s - loss: 0.9101 - accuracy: 0.68 - ETA: 0s - loss: 0.9246 - accuracy: 0.70 - 0s 1ms/step - loss: 0.9141 - accuracy: 0.7101 - val_loss: 0.7534 - val_accuracy: 0.8053\n",
-      "Epoch 17/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.8091 - accuracy: 0.78 - ETA: 0s - loss: 0.7497 - accuracy: 0.79 - ETA: 0s - loss: 0.7920 - accuracy: 0.75 - ETA: 0s - loss: 0.8153 - accuracy: 0.76 - 0s 2ms/step - loss: 0.8270 - accuracy: 0.7633 - val_loss: 0.7277 - val_accuracy: 0.8053\n",
-      "Epoch 18/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.78 - ETA: 0s - loss: 0.7829 - accuracy: 0.73 - ETA: 0s - loss: 0.7741 - accuracy: 0.75 - 0s 1ms/step - loss: 0.7750 - accuracy: 0.7515 - val_loss: 0.7245 - val_accuracy: 0.7876\n",
-      "Epoch 19/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 1.0268 - accuracy: 0.68 - ETA: 0s - loss: 0.8704 - accuracy: 0.72 - ETA: 0s - loss: 0.8279 - accuracy: 0.74 - ETA: 0s - loss: 0.7921 - accuracy: 0.74 - 0s 2ms/step - loss: 0.7880 - accuracy: 0.7396 - val_loss: 0.7006 - val_accuracy: 0.7965\n",
-      "Epoch 20/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.75 - ETA: 0s - loss: 0.7032 - accuracy: 0.70 - ETA: 0s - loss: 0.7083 - accuracy: 0.73 - 0s 1ms/step - loss: 0.7161 - accuracy: 0.7278 - val_loss: 0.7300 - val_accuracy: 0.7788\n",
-      "Epoch 21/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.84 - ETA: 0s - loss: 0.6065 - accuracy: 0.78 - ETA: 0s - loss: 0.5819 - accuracy: 0.80 - ETA: 0s - loss: 0.6834 - accuracy: 0.76 - 0s 2ms/step - loss: 0.7078 - accuracy: 0.7515 - val_loss: 0.7210 - val_accuracy: 0.8142\n",
-      "Epoch 22/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.71 - ETA: 0s - loss: 0.6816 - accuracy: 0.76 - ETA: 0s - loss: 0.6543 - accuracy: 0.80 - ETA: 0s - loss: 0.7259 - accuracy: 0.77 - 0s 2ms/step - loss: 0.7513 - accuracy: 0.7574 - val_loss: 0.7256 - val_accuracy: 0.8142\n",
-      "Epoch 23/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.7937 - accuracy: 0.62 - ETA: 0s - loss: 0.6402 - accuracy: 0.75 - ETA: 0s - loss: 0.7434 - accuracy: 0.75 - ETA: 0s - loss: 0.7842 - accuracy: 0.73 - 0s 2ms/step - loss: 0.7803 - accuracy: 0.7337 - val_loss: 0.7280 - val_accuracy: 0.8230\n",
-      "Epoch 24/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.7734 - accuracy: 0.68 - ETA: 0s - loss: 0.6359 - accuracy: 0.75 - ETA: 0s - loss: 0.6038 - accuracy: 0.76 - 0s 1ms/step - loss: 0.6388 - accuracy: 0.7811 - val_loss: 0.6599 - val_accuracy: 0.8053\n",
-      "Epoch 25/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.6819 - accuracy: 0.78 - ETA: 0s - loss: 0.7275 - accuracy: 0.73 - ETA: 0s - loss: 0.6392 - accuracy: 0.75 - ETA: 0s - loss: 0.6209 - accuracy: 0.77 - ETA: 0s - loss: 0.6329 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6385 - accuracy: 0.7751 - val_loss: 0.6739 - val_accuracy: 0.8142\n",
-      "Epoch 26/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.75 - ETA: 0s - loss: 0.5333 - accuracy: 0.83 - ETA: 0s - loss: 0.5309 - accuracy: 0.83 - ETA: 0s - loss: 0.5055 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5163 - accuracy: 0.8225 - val_loss: 0.7551 - val_accuracy: 0.7965\n",
-      "Epoch 27/50\n"
+      "657/657 [==============================] - ETA: 6s - loss: 0.5219 - accuracy: 0.81 - ETA: 6s - loss: 0.4303 - accuracy: 0.85 - ETA: 6s - loss: 0.3996 - accuracy: 0.89 - ETA: 5s - loss: 0.3617 - accuracy: 0.90 - ETA: 4s - loss: 0.3568 - accuracy: 0.90 - ETA: 4s - loss: 0.3520 - accuracy: 0.90 - ETA: 4s - loss: 0.4301 - accuracy: 0.88 - ETA: 3s - loss: 0.4188 - accuracy: 0.88 - ETA: 3s - loss: 0.4456 - accuracy: 0.87 - ETA: 3s - loss: 0.4406 - accuracy: 0.87 - ETA: 2s - loss: 0.4408 - accuracy: 0.87 - ETA: 2s - loss: 0.4326 - accuracy: 0.87 - ETA: 2s - loss: 0.4298 - accuracy: 0.87 - ETA: 1s - loss: 0.4257 - accuracy: 0.87 - ETA: 1s - loss: 0.4437 - accuracy: 0.86 - ETA: 1s - loss: 0.4434 - accuracy: 0.86 - ETA: 1s - loss: 0.4371 - accuracy: 0.86 - ETA: 0s - loss: 0.4309 - accuracy: 0.86 - ETA: 0s - loss: 0.4250 - accuracy: 0.86 - ETA: 0s - loss: 0.4262 - accuracy: 0.86 - 8s 12ms/step - loss: 0.4280 - accuracy: 0.8661 - val_loss: 0.4008 - val_accuracy: 0.8770\n",
+      "Epoch 10/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 0.3568 - accuracy: 0.84 - ETA: 4s - loss: 0.3297 - accuracy: 0.87 - ETA: 4s - loss: 0.4147 - accuracy: 0.84 - ETA: 3s - loss: 0.4121 - accuracy: 0.85 - ETA: 3s - loss: 0.4045 - accuracy: 0.85 - ETA: 3s - loss: 0.3893 - accuracy: 0.85 - ETA: 3s - loss: 0.3803 - accuracy: 0.86 - ETA: 2s - loss: 0.3816 - accuracy: 0.87 - ETA: 2s - loss: 0.3897 - accuracy: 0.87 - ETA: 2s - loss: 0.3703 - accuracy: 0.88 - ETA: 2s - loss: 0.3698 - accuracy: 0.87 - ETA: 2s - loss: 0.3768 - accuracy: 0.87 - ETA: 1s - loss: 0.3647 - accuracy: 0.87 - ETA: 1s - loss: 0.3550 - accuracy: 0.87 - ETA: 1s - loss: 0.3459 - accuracy: 0.87 - ETA: 1s - loss: 0.3603 - accuracy: 0.86 - ETA: 1s - loss: 0.3468 - accuracy: 0.86 - ETA: 0s - loss: 0.3434 - accuracy: 0.87 - ETA: 0s - loss: 0.3438 - accuracy: 0.87 - ETA: 0s - loss: 0.3606 - accuracy: 0.86 - 7s 11ms/step - loss: 0.3630 - accuracy: 0.8676 - val_loss: 0.4258 - val_accuracy: 0.8337\n",
+      "Epoch 11/17\n",
+      "657/657 [==============================] - ETA: 4s - loss: 0.2802 - accuracy: 0.90 - ETA: 4s - loss: 0.3108 - accuracy: 0.90 - ETA: 4s - loss: 0.2954 - accuracy: 0.89 - ETA: 4s - loss: 0.2840 - accuracy: 0.90 - ETA: 4s - loss: 0.2974 - accuracy: 0.90 - ETA: 3s - loss: 0.3111 - accuracy: 0.91 - ETA: 3s - loss: 0.3021 - accuracy: 0.91 - ETA: 3s - loss: 0.3102 - accuracy: 0.90 - ETA: 3s - loss: 0.3206 - accuracy: 0.90 - ETA: 3s - loss: 0.3199 - accuracy: 0.90 - ETA: 2s - loss: 0.3094 - accuracy: 0.90 - ETA: 2s - loss: 0.2993 - accuracy: 0.90 - ETA: 2s - loss: 0.3106 - accuracy: 0.90 - ETA: 2s - loss: 0.3135 - accuracy: 0.90 - ETA: 1s - loss: 0.3147 - accuracy: 0.89 - ETA: 1s - loss: 0.3170 - accuracy: 0.89 - ETA: 1s - loss: 0.3265 - accuracy: 0.89 - ETA: 0s - loss: 0.3202 - accuracy: 0.90 - ETA: 0s - loss: 0.3331 - accuracy: 0.89 - ETA: 0s - loss: 0.3293 - accuracy: 0.89 - 10s 15ms/step - loss: 0.3241 - accuracy: 0.9011 - val_loss: 0.5041 - val_accuracy: 0.8497\n",
+      "Epoch 12/17\n",
+      "657/657 [==============================] - ETA: 9s - loss: 0.2008 - accuracy: 0.87 - ETA: 8s - loss: 0.2736 - accuracy: 0.87 - ETA: 8s - loss: 0.2541 - accuracy: 0.89 - ETA: 8s - loss: 0.2461 - accuracy: 0.91 - ETA: 9s - loss: 0.2538 - accuracy: 0.91 - ETA: 7s - loss: 0.2943 - accuracy: 0.89 - ETA: 7s - loss: 0.2845 - accuracy: 0.89 - ETA: 6s - loss: 0.2893 - accuracy: 0.89 - ETA: 5s - loss: 0.2782 - accuracy: 0.90 - ETA: 5s - loss: 0.2935 - accuracy: 0.89 - ETA: 4s - loss: 0.2955 - accuracy: 0.88 - ETA: 3s - loss: 0.3051 - accuracy: 0.88 - ETA: 3s - loss: 0.2980 - accuracy: 0.88 - ETA: 2s - loss: 0.2976 - accuracy: 0.88 - ETA: 2s - loss: 0.2916 - accuracy: 0.88 - ETA: 1s - loss: 0.2814 - accuracy: 0.89 - ETA: 1s - loss: 0.2813 - accuracy: 0.89 - ETA: 1s - loss: 0.2827 - accuracy: 0.89 - ETA: 0s - loss: 0.2834 - accuracy: 0.89 - ETA: 0s - loss: 0.2817 - accuracy: 0.88 - 11s 17ms/step - loss: 0.2912 - accuracy: 0.8843 - val_loss: 0.4004 - val_accuracy: 0.8656\n",
+      "Epoch 13/17\n",
+      "657/657 [==============================] - ETA: 5s - loss: 0.2760 - accuracy: 0.87 - ETA: 6s - loss: 0.2380 - accuracy: 0.90 - ETA: 6s - loss: 0.2356 - accuracy: 0.89 - ETA: 5s - loss: 0.3320 - accuracy: 0.85 - ETA: 5s - loss: 0.3177 - accuracy: 0.87 - ETA: 5s - loss: 0.2959 - accuracy: 0.89 - ETA: 4s - loss: 0.2884 - accuracy: 0.89 - ETA: 4s - loss: 0.3046 - accuracy: 0.88 - ETA: 3s - loss: 0.3167 - accuracy: 0.87 - ETA: 3s - loss: 0.3161 - accuracy: 0.87 - ETA: 3s - loss: 0.3066 - accuracy: 0.88 - ETA: 2s - loss: 0.3012 - accuracy: 0.88 - ETA: 2s - loss: 0.2997 - accuracy: 0.88 - ETA: 2s - loss: 0.2919 - accuracy: 0.89 - ETA: 1s - loss: 0.3087 - accuracy: 0.88 - ETA: 1s - loss: 0.3053 - accuracy: 0.89 - ETA: 1s - loss: 0.3004 - accuracy: 0.88 - ETA: 0s - loss: 0.2916 - accuracy: 0.89 - ETA: 0s - loss: 0.2913 - accuracy: 0.89 - ETA: 0s - loss: 0.2936 - accuracy: 0.89 - 10s 15ms/step - loss: 0.2900 - accuracy: 0.8904 - val_loss: 0.4083 - val_accuracy: 0.8565\n",
+      "Epoch 14/17\n",
+      "657/657 [==============================] - ETA: 7s - loss: 0.1383 - accuracy: 0.93 - ETA: 6s - loss: 0.2606 - accuracy: 0.87 - ETA: 5s - loss: 0.2588 - accuracy: 0.88 - ETA: 5s - loss: 0.2843 - accuracy: 0.88 - ETA: 5s - loss: 0.2554 - accuracy: 0.89 - ETA: 5s - loss: 0.2605 - accuracy: 0.89 - ETA: 4s - loss: 0.2588 - accuracy: 0.89 - ETA: 4s - loss: 0.2557 - accuracy: 0.90 - ETA: 3s - loss: 0.2425 - accuracy: 0.90 - ETA: 3s - loss: 0.2405 - accuracy: 0.90 - ETA: 3s - loss: 0.2328 - accuracy: 0.90 - ETA: 2s - loss: 0.2315 - accuracy: 0.90 - ETA: 2s - loss: 0.2324 - accuracy: 0.91 - ETA: 2s - loss: 0.2315 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.90 - ETA: 1s - loss: 0.2354 - accuracy: 0.90 - ETA: 1s - loss: 0.2463 - accuracy: 0.90 - ETA: 0s - loss: 0.2382 - accuracy: 0.90 - ETA: 0s - loss: 0.2383 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.90 - 11s 16ms/step - loss: 0.2403 - accuracy: 0.9041 - val_loss: 0.4337 - val_accuracy: 0.8747\n",
+      "Epoch 15/17\n",
+      "657/657 [==============================] - ETA: 7s - loss: 0.0555 - accuracy: 1.00 - ETA: 9s - loss: 0.1010 - accuracy: 0.96 - ETA: 7s - loss: 0.1470 - accuracy: 0.94 - ETA: 6s - loss: 0.1366 - accuracy: 0.96 - ETA: 5s - loss: 0.1278 - accuracy: 0.96 - ETA: 5s - loss: 0.1132 - accuracy: 0.96 - ETA: 5s - loss: 0.1314 - accuracy: 0.95 - ETA: 4s - loss: 0.1421 - accuracy: 0.94 - ETA: 4s - loss: 0.1431 - accuracy: 0.94 - ETA: 4s - loss: 0.1523 - accuracy: 0.93 - ETA: 3s - loss: 0.1552 - accuracy: 0.94 - ETA: 3s - loss: 0.1483 - accuracy: 0.94 - ETA: 2s - loss: 0.1462 - accuracy: 0.94 - ETA: 2s - loss: 0.1570 - accuracy: 0.93 - ETA: 2s - loss: 0.1680 - accuracy: 0.93 - ETA: 1s - loss: 0.1650 - accuracy: 0.93 - ETA: 1s - loss: 0.1682 - accuracy: 0.93 - ETA: 1s - loss: 0.1626 - accuracy: 0.94 - ETA: 0s - loss: 0.1598 - accuracy: 0.94 - ETA: 0s - loss: 0.1681 - accuracy: 0.93 - 11s 17ms/step - loss: 0.1674 - accuracy: 0.9376 - val_loss: 0.3869 - val_accuracy: 0.8770\n",
+      "Epoch 16/17\n",
+      "657/657 [==============================] - ETA: 6s - loss: 0.1634 - accuracy: 0.93 - ETA: 6s - loss: 0.2038 - accuracy: 0.92 - ETA: 7s - loss: 0.2980 - accuracy: 0.87 - ETA: 6s - loss: 0.2515 - accuracy: 0.89 - ETA: 7s - loss: 0.2261 - accuracy: 0.91 - ETA: 6s - loss: 0.2081 - accuracy: 0.91 - ETA: 5s - loss: 0.2003 - accuracy: 0.91 - ETA: 5s - loss: 0.1930 - accuracy: 0.91 - ETA: 4s - loss: 0.1953 - accuracy: 0.91 - ETA: 4s - loss: 0.2013 - accuracy: 0.91 - ETA: 4s - loss: 0.2186 - accuracy: 0.90 - ETA: 3s - loss: 0.2140 - accuracy: 0.90 - ETA: 3s - loss: 0.2212 - accuracy: 0.90 - ETA: 2s - loss: 0.2182 - accuracy: 0.90 - ETA: 2s - loss: 0.2136 - accuracy: 0.90 - ETA: 2s - loss: 0.2137 - accuracy: 0.91 - ETA: 1s - loss: 0.2160 - accuracy: 0.91 - ETA: 1s - loss: 0.2101 - accuracy: 0.91 - ETA: 0s - loss: 0.2114 - accuracy: 0.91 - ETA: 0s - loss: 0.2079 - accuracy: 0.91 - 12s 18ms/step - loss: 0.2090 - accuracy: 0.9148 - val_loss: 0.4128 - val_accuracy: 0.8702\n",
+      "Epoch 17/17\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "169/169 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - ETA: 0s - loss: 0.5882 - accuracy: 0.72 - ETA: 0s - loss: 0.6115 - accuracy: 0.74 - ETA: 0s - loss: 0.5959 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6150 - accuracy: 0.7574 - val_loss: 0.6825 - val_accuracy: 0.8053\n",
-      "Epoch 28/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.78 - ETA: 0s - loss: 0.6044 - accuracy: 0.80 - 0s 814us/step - loss: 0.5800 - accuracy: 0.8107 - val_loss: 0.6592 - val_accuracy: 0.8142\n",
-      "Epoch 29/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.84 - ETA: 0s - loss: 0.4826 - accuracy: 0.84 - ETA: 0s - loss: 0.4482 - accuracy: 0.86 - ETA: 0s - loss: 0.5395 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5262 - accuracy: 0.8284 - val_loss: 0.7025 - val_accuracy: 0.8053\n",
-      "Epoch 30/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.90 - ETA: 0s - loss: 0.3292 - accuracy: 0.89 - ETA: 0s - loss: 0.4548 - accuracy: 0.86 - ETA: 0s - loss: 0.4912 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4797 - accuracy: 0.8462 - val_loss: 0.7280 - val_accuracy: 0.8230\n",
-      "Epoch 31/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.90 - ETA: 0s - loss: 0.3632 - accuracy: 0.92 - ETA: 0s - loss: 0.3875 - accuracy: 0.89 - 0s 2ms/step - loss: 0.4802 - accuracy: 0.8698 - val_loss: 0.7389 - val_accuracy: 0.8142\n",
-      "Epoch 32/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.84 - ETA: 0s - loss: 0.4224 - accuracy: 0.85 - ETA: 0s - loss: 0.4240 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8402 - val_loss: 0.6243 - val_accuracy: 0.8319\n",
-      "Epoch 33/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.90 - ETA: 0s - loss: 0.3529 - accuracy: 0.89 - ETA: 0s - loss: 0.4116 - accuracy: 0.86 - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8698 - val_loss: 0.6090 - val_accuracy: 0.8230\n",
-      "Epoch 34/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.81 - ETA: 0s - loss: 0.4574 - accuracy: 0.82 - ETA: 0s - loss: 0.4332 - accuracy: 0.83 - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8462 - val_loss: 0.6882 - val_accuracy: 0.8407\n",
-      "Epoch 35/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.84 - ETA: 0s - loss: 0.3725 - accuracy: 0.87 - ETA: 0s - loss: 0.3253 - accuracy: 0.90 - ETA: 0s - loss: 0.3170 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8994 - val_loss: 0.6899 - val_accuracy: 0.8407\n",
-      "Epoch 36/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.84 - ETA: 0s - loss: 0.3540 - accuracy: 0.89 - ETA: 0s - loss: 0.3619 - accuracy: 0.89 - ETA: 0s - loss: 0.3928 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8698 - val_loss: 0.6528 - val_accuracy: 0.8319\n",
-      "Epoch 37/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.90 - ETA: 0s - loss: 0.3725 - accuracy: 0.82 - ETA: 0s - loss: 0.4067 - accuracy: 0.88 - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8817 - val_loss: 0.9041 - val_accuracy: 0.7876\n",
-      "Epoch 38/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.87 - ETA: 0s - loss: 0.3818 - accuracy: 0.86 - ETA: 0s - loss: 0.3725 - accuracy: 0.86 - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8639 - val_loss: 0.7236 - val_accuracy: 0.8230\n",
-      "Epoch 39/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.84 - ETA: 0s - loss: 0.3247 - accuracy: 0.85 - ETA: 0s - loss: 0.3425 - accuracy: 0.85 - ETA: 0s - loss: 0.3705 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8462 - val_loss: 0.7642 - val_accuracy: 0.8053\n",
-      "Epoch 40/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.90 - ETA: 0s - loss: 0.2666 - accuracy: 0.93 - ETA: 0s - loss: 0.2923 - accuracy: 0.92 - ETA: 0s - loss: 0.2533 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2601 - accuracy: 0.9231 - val_loss: 0.8567 - val_accuracy: 0.7965\n",
-      "Epoch 41/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.87 - ETA: 0s - loss: 0.3698 - accuracy: 0.88 - ETA: 0s - loss: 0.3593 - accuracy: 0.89 - ETA: 0s - loss: 0.3353 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8994 - val_loss: 0.7658 - val_accuracy: 0.7965\n",
-      "Epoch 42/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.96 - ETA: 0s - loss: 0.1956 - accuracy: 0.95 - ETA: 0s - loss: 0.2661 - accuracy: 0.90 - 0s 1ms/step - loss: 0.2734 - accuracy: 0.8994 - val_loss: 0.7690 - val_accuracy: 0.8142\n",
-      "Epoch 43/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.90 - ETA: 0s - loss: 0.3154 - accuracy: 0.87 - ETA: 0s - loss: 0.3143 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8757 - val_loss: 0.8100 - val_accuracy: 0.7876\n",
-      "Epoch 44/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.87 - ETA: 0s - loss: 0.2914 - accuracy: 0.92 - ETA: 0s - loss: 0.2265 - accuracy: 0.93 - ETA: 0s - loss: 0.2922 - accuracy: 0.90 - ETA: 0s - loss: 0.3184 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8994 - val_loss: 0.8384 - val_accuracy: 0.8319\n",
-      "Epoch 45/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.1693 - accuracy: 0.96 - ETA: 0s - loss: 0.3334 - accuracy: 0.88 - ETA: 0s - loss: 0.2989 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8994 - val_loss: 0.8613 - val_accuracy: 0.8319\n",
-      "Epoch 46/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.93 - ETA: 0s - loss: 0.2121 - accuracy: 0.95 - ETA: 0s - loss: 0.2208 - accuracy: 0.94 - ETA: 0s - loss: 0.2141 - accuracy: 0.95 - ETA: 0s - loss: 0.2053 - accuracy: 0.95 - 1s 3ms/step - loss: 0.2044 - accuracy: 0.9586 - val_loss: 0.9388 - val_accuracy: 0.8053\n",
-      "Epoch 47/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.93 - ETA: 0s - loss: 0.2067 - accuracy: 0.92 - ETA: 0s - loss: 0.2262 - accuracy: 0.91 - ETA: 0s - loss: 0.1991 - accuracy: 0.92 - ETA: 0s - loss: 0.2069 - accuracy: 0.92 - 1s 3ms/step - loss: 0.2020 - accuracy: 0.9290 - val_loss: 0.9359 - val_accuracy: 0.8053\n",
-      "Epoch 48/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.96 - ETA: 0s - loss: 0.0893 - accuracy: 0.96 - ETA: 0s - loss: 0.1339 - accuracy: 0.95 - ETA: 0s - loss: 0.1701 - accuracy: 0.93 - ETA: 0s - loss: 0.1636 - accuracy: 0.93 - 1s 3ms/step - loss: 0.1659 - accuracy: 0.9290 - val_loss: 0.8759 - val_accuracy: 0.8142\n",
-      "Epoch 49/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.93 - ETA: 0s - loss: 0.1959 - accuracy: 0.93 - ETA: 0s - loss: 0.1760 - accuracy: 0.95 - ETA: 0s - loss: 0.1967 - accuracy: 0.93 - 0s 2ms/step - loss: 0.1960 - accuracy: 0.9408 - val_loss: 0.8242 - val_accuracy: 0.7965\n",
-      "Epoch 50/50\n",
-      "169/169 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.93 - ETA: 0s - loss: 0.1210 - accuracy: 0.95 - ETA: 0s - loss: 0.1158 - accuracy: 0.96 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9467 - val_loss: 0.9015 - val_accuracy: 0.7965\n"
+      "657/657 [==============================] - ETA: 4s - loss: 0.1133 - accuracy: 0.96 - ETA: 5s - loss: 0.2007 - accuracy: 0.92 - ETA: 5s - loss: 0.2069 - accuracy: 0.92 - ETA: 7s - loss: 0.2032 - accuracy: 0.92 - ETA: 7s - loss: 0.1786 - accuracy: 0.93 - ETA: 6s - loss: 0.2122 - accuracy: 0.91 - ETA: 5s - loss: 0.2222 - accuracy: 0.91 - ETA: 5s - loss: 0.2016 - accuracy: 0.91 - ETA: 4s - loss: 0.2045 - accuracy: 0.92 - ETA: 4s - loss: 0.1902 - accuracy: 0.92 - ETA: 3s - loss: 0.1879 - accuracy: 0.93 - ETA: 3s - loss: 0.1873 - accuracy: 0.93 - ETA: 3s - loss: 0.1851 - accuracy: 0.93 - ETA: 3s - loss: 0.1907 - accuracy: 0.92 - ETA: 2s - loss: 0.1856 - accuracy: 0.93 - ETA: 2s - loss: 0.1892 - accuracy: 0.93 - ETA: 1s - loss: 0.1906 - accuracy: 0.93 - ETA: 1s - loss: 0.1938 - accuracy: 0.92 - ETA: 0s - loss: 0.2057 - accuracy: 0.92 - ETA: 0s - loss: 0.2028 - accuracy: 0.92 - 11s 17ms/step - loss: 0.1997 - accuracy: 0.9254 - val_loss: 0.5544 - val_accuracy: 0.8747\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<keras.callbacks.callbacks.History at 0x1b0da70b9c8>"
+       "<keras.callbacks.callbacks.History at 0x25a182792c8>"
       ]
      },
-     "execution_count": 9,
+     "execution_count": 51,
      "metadata": {},
      "output_type": "execute_result"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Network error resolved after 0:00:25.850209, resuming normal operation.\n"
+     ]
     }
    ],
    "source": [
@@ -426,7 +404,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 52,
    "metadata": {},
    "outputs": [
     {
@@ -452,7 +430,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 53,
    "metadata": {
     "scrolled": true
    },
@@ -466,49 +444,49 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 54,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Model: \"sequential_1\"\n",
+      "Model: \"sequential_4\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_1 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_10 (Conv2D)           (None, 126, 30, 21)       210       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_1 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_7 (MaxPooling2 (None, 63, 15, 21)        0         \n",
       "_________________________________________________________________\n",
-      "activation_1 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_16 (Activation)   (None, 63, 15, 21)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_2 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_11 (Conv2D)           (None, 61, 13, 48)        9120      \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_2 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_8 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_2 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_17 (Activation)   (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_3 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_12 (Conv2D)           (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_3 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_18 (Activation)   (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_1 (Flatten)          (None, 1296)              0         \n",
+      "flatten_4 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_1 (Dropout)          (None, 1296)              0         \n",
+      "dropout_7 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_1 (Dense)              (None, 64)                83008     \n",
+      "dense_7 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_4 (Activation)    (None, 64)                0         \n",
+      "activation_19 (Activation)   (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dropout_2 (Dropout)          (None, 64)                0         \n",
+      "dropout_8 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 7)                 455       \n",
+      "dense_8 (Dense)              (None, 7)                 455       \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 7)                 0         \n",
+      "activation_20 (Activation)   (None, 7)                 0         \n",
       "=================================================================\n",
-      "Total params: 101,079\n",
-      "Trainable params: 101,079\n",
+      "Total params: 532,905\n",
+      "Trainable params: 532,905\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -521,36 +499,37 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 55,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "[[ 7  0  0  0  0  8]\n",
-      " [ 0  1  0  0  0  2]\n",
-      " [ 0  0  1  0  0  7]\n",
-      " [ 0  0  0  0  0  1]\n",
-      " [ 0  0  0  0  3  2]\n",
-      " [ 2  0  1  0  0 36]]\n",
-      "Accuracy for class AAT : [0.46666667]\n",
-      "Accuracy for class AHV : [0.33333333]\n",
-      "Accuracy for class AMA : [0.125]\n",
-      "Accuracy for class ART : 0\n",
+      "[[ 24   0   0   0   0   0  10]\n",
+      " [  1   4   0   0   0   0   3]\n",
+      " [  0   0   5   0   0   0  16]\n",
+      " [  2   0   0   0   0   0   1]\n",
+      " [  0   0   0   0   0   0   6]\n",
+      " [  0   0   0   0   0   2   3]\n",
+      " [  1   0   1   1   0   0 194]]\n",
+      "Accuracy for class AAT : [0.70588235]\n",
+      "Accuracy for class AHV : [0.5]\n",
+      "Accuracy for class AMA : [0.23809524]\n",
+      "Accuracy for class ART : [0.]\n",
       "Accuracy for class ASI : [0.]\n",
-      "Accuracy for class AVH : [0.6]\n",
-      "Accuracy for class AVT : [0.92307692]\n",
-      "Overall Accuracy : 0.676056338028169\n"
+      "Accuracy for class AVH : [0.4]\n",
+      "Accuracy for class AVT : [0.98477157]\n",
+      "Overall Accuracy : 0.8357664233576643\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "'Accuracy for class chirping_birds : [0.8]\\nAccuracy for class crickets : [0.25]\\nAccuracy for class crow : [0.83333333]\\nAccuracy for class frog : [0.3]\\nAccuracy for class insects : [0.85714286]\\nOverall Accuracy : 0.525'"
+       "'Accuracy for class AAT : [0.46666667]\\nAccuracy for class AHV : [0.33333333]\\nAccuracy for class AMA : [0.125]\\nAccuracy for class ART : 0\\nAccuracy for class ASI : [0.]\\nAccuracy for class AVH : [0.6]\\nAccuracy for class AVT : [0.92307692]\\nOverall Accuracy : 0.676056338028169'"
       ]
      },
-     "execution_count": 16,
+     "execution_count": 55,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -571,12 +550,28 @@
     "    else : mean = \"N/A\"\n",
     "    print(\"Accuracy for class\", labels[class_i], \":\", mean)\n",
     "\n",
-    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))"
+    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))\n",
+    "\n",
+    "'''[[ 24   0   0   0   0   0  10]\n",
+    " [  1   4   0   0   0   0   3]\n",
+    " [  0   0   5   0   0   0  16]\n",
+    " [  2   0   0   0   0   0   1]\n",
+    " [  0   0   0   0   0   0   6]\n",
+    " [  0   0   0   0   0   2   3]\n",
+    " [  1   0   1   1   0   0 194]]\n",
+    "Accuracy for class AAT : [0.70588235]\n",
+    "Accuracy for class AHV : [0.5]\n",
+    "Accuracy for class AMA : [0.23809524]\n",
+    "Accuracy for class ART : [0.]\n",
+    "Accuracy for class ASI : [0.]\n",
+    "Accuracy for class AVH : [0.4]\n",
+    "Accuracy for class AVT : [0.98477157]\n",
+    "Overall Accuracy : 0.8357664233576643'''"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 56,
    "metadata": {},
    "outputs": [
     {
@@ -586,798 +581,800 @@
       "[0. 1.]\n",
       "[ 0.0000000e+00  1.5258789e-05  0.0000000e+00 ...  3.3020020e-02\n",
       "  1.2680054e-02 -8.7432861e-03]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.01039583\n",
+      " AAT :  0.00000845\n",
       "\n",
-      " AHV :  0.02998595\n",
+      " AHV :  0.00002680\n",
       "\n",
-      " AMA :  0.02692612\n",
+      " AMA :  0.00160028\n",
       "\n",
-      " ART :  0.00021275\n",
+      " ART :  0.00002374\n",
       "\n",
-      " ASI :  0.00109535\n",
+      " ASI :  0.00000634\n",
       "\n",
-      " AVH :  0.00171419\n",
+      " AVH :  0.00003628\n",
       "\n",
-      " AVT :  0.92966986\n",
+      " AVT :  0.99829823\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[1. 2.]\n",
       "[-0.03717041 -0.05769348 -0.06455994 ...  0.01766968  0.01895142\n",
       "  0.01779175]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.09363972\n",
+      " AAT :  0.12997904\n",
       "\n",
-      " AHV :  0.27155691\n",
+      " AHV :  0.04505088\n",
       "\n",
-      " AMA :  0.11288872\n",
+      " AMA :  0.00426566\n",
       "\n",
-      " ART :  0.00317154\n",
+      " ART :  0.00476746\n",
       "\n",
-      " ASI :  0.00362689\n",
+      " ASI :  0.00100627\n",
       "\n",
-      " AVH :  0.00379819\n",
+      " AVH :  0.00626362\n",
       "\n",
-      " AVT :  0.51131809\n",
+      " AVT :  0.80866700\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[2. 3.]\n",
       "[ 0.02345276  0.02101135  0.01712036 ... -0.01161194 -0.0141449\n",
       " -0.01431274]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.02302537\n",
+      " AAT :  0.00162159\n",
       "\n",
-      " AHV :  0.34436873\n",
+      " AHV :  0.02336102\n",
       "\n",
-      " AMA :  0.12974481\n",
+      " AMA :  0.02379958\n",
       "\n",
-      " ART :  0.00235426\n",
+      " ART :  0.00201543\n",
       "\n",
-      " ASI :  0.00554969\n",
+      " ASI :  0.00147080\n",
       "\n",
-      " AVH :  0.01236655\n",
+      " AVH :  0.05321468\n",
       "\n",
-      " AVT :  0.48259062\n",
-      "GUESS: Nothing\n",
+      " AVT :  0.89451694\n",
+      "\n",
+      "\n",
+      "GUESS:  AVT\n",
       "[3. 4.]\n",
       "[-0.01583862 -0.01066589 -0.00762939 ... -0.0377655  -0.03556824\n",
       " -0.02685547]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00185250\n",
+      " AAT :  0.00026622\n",
       "\n",
-      " AHV :  0.01309109\n",
+      " AHV :  0.00000569\n",
       "\n",
-      " AMA :  0.00708705\n",
+      " AMA :  0.00002566\n",
       "\n",
-      " ART :  0.00006909\n",
+      " ART :  0.00000374\n",
       "\n",
-      " ASI :  0.00058765\n",
+      " ASI :  0.00000336\n",
       "\n",
-      " AVH :  0.00124668\n",
+      " AVH :  0.00011508\n",
       "\n",
-      " AVT :  0.97606587\n",
+      " AVT :  0.99958020\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[4. 5.]\n",
       "[-0.02836609 -0.02510071 -0.02012634 ...  0.0138855  -0.00386047\n",
       " -0.00904846]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00043267\n",
+      " AAT :  0.00001108\n",
       "\n",
-      " AHV :  0.02111147\n",
+      " AHV :  0.00008323\n",
       "\n",
-      " AMA :  0.04168708\n",
+      " AMA :  0.00004823\n",
       "\n",
-      " ART :  0.00007569\n",
+      " ART :  0.00000794\n",
       "\n",
-      " ASI :  0.00024637\n",
+      " ASI :  0.00000329\n",
       "\n",
-      " AVH :  0.00020514\n",
+      " AVH :  0.00019222\n",
       "\n",
-      " AVT :  0.93624163\n",
+      " AVT :  0.99965405\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[5. 6.]\n",
       "[-0.00526428  0.00822449  0.01951599 ...  0.02729797  0.02156067\n",
       "  0.01234436]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00642328\n",
+      " AAT :  0.00061439\n",
       "\n",
-      " AHV :  0.20216547\n",
+      " AHV :  0.02362605\n",
       "\n",
-      " AMA :  0.25181180\n",
+      " AMA :  0.00448049\n",
       "\n",
-      " ART :  0.00056665\n",
+      " ART :  0.00014941\n",
       "\n",
-      " ASI :  0.00074299\n",
+      " ASI :  0.00009322\n",
       "\n",
-      " AVH :  0.00098468\n",
+      " AVH :  0.00404324\n",
       "\n",
-      " AVT :  0.53730518\n",
+      " AVT :  0.96699321\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[6. 7.]\n",
       "[ 0.00544739  0.00053406  0.00970459 ... -0.02848816 -0.01611328\n",
       " -0.01091003]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00057033\n",
+      " AAT :  0.00004080\n",
       "\n",
-      " AHV :  0.00731208\n",
+      " AHV :  0.00074152\n",
       "\n",
-      " AMA :  0.00087687\n",
+      " AMA :  0.00559893\n",
       "\n",
-      " ART :  0.00000403\n",
+      " ART :  0.00048534\n",
       "\n",
-      " ASI :  0.00005995\n",
+      " ASI :  0.00055095\n",
       "\n",
-      " AVH :  0.00019443\n",
+      " AVH :  0.00444674\n",
       "\n",
-      " AVT :  0.99098223\n",
+      " AVT :  0.98813581\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[7. 8.]\n",
       "[-0.0177002  -0.02372742 -0.02700806 ... -0.04304504 -0.04063416\n",
       " -0.03363037]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000483\n",
+      " AAT :  0.00004655\n",
       "\n",
-      " AHV :  0.00022814\n",
+      " AHV :  0.00002883\n",
       "\n",
-      " AMA :  0.00036886\n",
+      " AMA :  0.00047086\n",
       "\n",
-      " ART :  0.00000013\n",
+      " ART :  0.00004399\n",
       "\n",
-      " ASI :  0.00000414\n",
+      " ASI :  0.00001032\n",
       "\n",
-      " AVH :  0.00000038\n",
+      " AVH :  0.00005926\n",
       "\n",
-      " AVT :  0.99939358\n",
+      " AVT :  0.99934012\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[8. 9.]\n",
       "[-0.01539612 -0.00108337  0.00718689 ...  0.01161194  0.01818848\n",
       "  0.02700806]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000965\n",
+      " AAT :  0.00001865\n",
       "\n",
-      " AHV :  0.01190078\n",
+      " AHV :  0.00208160\n",
       "\n",
-      " AMA :  0.00533848\n",
+      " AMA :  0.01226574\n",
       "\n",
-      " ART :  0.00000082\n",
+      " ART :  0.00032419\n",
       "\n",
-      " ASI :  0.00000508\n",
+      " ASI :  0.00006721\n",
       "\n",
-      " AVH :  0.00000422\n",
+      " AVH :  0.00023971\n",
       "\n",
-      " AVT :  0.98274094\n",
+      " AVT :  0.98500293\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[ 9. 10.]\n",
       "[ 0.03549194  0.04856873  0.05519104 ... -0.02171326 -0.03634644\n",
       " -0.03912354]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00016367\n",
+      " AAT :  0.00033581\n",
       "\n",
-      " AHV :  0.02253050\n",
+      " AHV :  0.00389374\n",
       "\n",
-      " AMA :  0.00854565\n",
+      " AMA :  0.04916417\n",
       "\n",
-      " ART :  0.00000365\n",
+      " ART :  0.00066305\n",
       "\n",
-      " ASI :  0.00008337\n",
+      " ASI :  0.00055320\n",
       "\n",
-      " AVH :  0.00025015\n",
+      " AVH :  0.00067650\n",
       "\n",
-      " AVT :  0.96842301\n",
+      " AVT :  0.94471347\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[10. 11.]\n",
       "[-0.02934265 -0.0115509   0.00445557 ... -0.03616333 -0.03759766\n",
       " -0.0304718 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00002215\n",
+      " AAT :  0.00009942\n",
       "\n",
-      " AHV :  0.00093671\n",
+      " AHV :  0.00036154\n",
       "\n",
-      " AMA :  0.00037632\n",
+      " AMA :  0.00088362\n",
       "\n",
-      " ART :  0.00000074\n",
+      " ART :  0.00033854\n",
       "\n",
-      " ASI :  0.00000892\n",
+      " ASI :  0.00006554\n",
       "\n",
-      " AVH :  0.00000542\n",
+      " AVH :  0.00013340\n",
       "\n",
-      " AVT :  0.99864978\n",
+      " AVT :  0.99811792\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[11. 12.]\n",
       "[-0.03358459 -0.03901672 -0.03933716 ... -0.02337646 -0.02124023\n",
       " -0.02107239]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.01750475\n",
+      " AAT :  0.02913574\n",
       "\n",
-      " AHV :  0.13233173\n",
+      " AHV :  0.05098564\n",
       "\n",
-      " AMA :  0.06346360\n",
+      " AMA :  0.01407128\n",
       "\n",
-      " ART :  0.00163204\n",
+      " ART :  0.00464849\n",
       "\n",
-      " ASI :  0.00448596\n",
+      " ASI :  0.00274732\n",
       "\n",
-      " AVH :  0.00566114\n",
+      " AVH :  0.02327583\n",
       "\n",
-      " AVT :  0.77492076\n",
+      " AVT :  0.87513572\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[12. 13.]\n",
       "[-0.00846863  0.00444031  0.00852966 ... -0.00604248 -0.00845337\n",
       " -0.00497437]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00227106\n",
+      " AAT :  0.00023466\n",
       "\n",
-      " AHV :  0.03145981\n",
+      " AHV :  0.00001617\n",
       "\n",
-      " AMA :  0.01022883\n",
+      " AMA :  0.00001841\n",
       "\n",
-      " ART :  0.00009338\n",
+      " ART :  0.00000659\n",
       "\n",
-      " ASI :  0.00125850\n",
+      " ASI :  0.00000598\n",
       "\n",
-      " AVH :  0.00117838\n",
+      " AVH :  0.00001476\n",
       "\n",
-      " AVT :  0.95350999\n",
+      " AVT :  0.99970335\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[13. 14.]\n",
       "[-0.00427246 -0.00718689 -0.00811768 ... -0.01966858 -0.01296997\n",
       " -0.01628113]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.01924668\n",
+      " AAT :  0.00485625\n",
       "\n",
-      " AHV :  0.05086352\n",
+      " AHV :  0.00167193\n",
       "\n",
-      " AMA :  0.00325990\n",
+      " AMA :  0.00056236\n",
       "\n",
-      " ART :  0.00030024\n",
+      " ART :  0.00040990\n",
       "\n",
-      " ASI :  0.00159845\n",
+      " ASI :  0.00002041\n",
       "\n",
-      " AVH :  0.00078041\n",
+      " AVH :  0.00000730\n",
       "\n",
-      " AVT :  0.92395073\n",
+      " AVT :  0.99247181\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[14. 15.]\n",
       "[-0.02262878 -0.01573181 -0.00117493 ... -0.08956909 -0.0695343\n",
       " -0.04067993]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00017223\n",
+      " AAT :  0.00089134\n",
       "\n",
-      " AHV :  0.01953383\n",
+      " AHV :  0.00646266\n",
       "\n",
-      " AMA :  0.01624575\n",
+      " AMA :  0.00076925\n",
       "\n",
-      " ART :  0.00002341\n",
+      " ART :  0.00013778\n",
       "\n",
-      " ASI :  0.00031366\n",
+      " ASI :  0.00007284\n",
       "\n",
-      " AVH :  0.00020480\n",
+      " AVH :  0.00023529\n",
       "\n",
-      " AVT :  0.96350634\n",
+      " AVT :  0.99143094\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[15. 16.]\n",
       "[-0.02532959 -0.01031494 -0.00280762 ... -0.07128906 -0.07106018\n",
       " -0.05839539]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00245207\n",
+      " AAT :  0.00013731\n",
       "\n",
-      " AHV :  0.07802935\n",
+      " AHV :  0.00183650\n",
       "\n",
-      " AMA :  0.02046134\n",
+      " AMA :  0.00056936\n",
       "\n",
-      " ART :  0.00002834\n",
+      " ART :  0.00005597\n",
       "\n",
-      " ASI :  0.00016393\n",
+      " ASI :  0.00003344\n",
       "\n",
-      " AVH :  0.00104307\n",
+      " AVH :  0.00059690\n",
       "\n",
-      " AVT :  0.89782184\n",
+      " AVT :  0.99677050\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[16. 17.]\n",
       "[-0.04600525 -0.02149963  0.00523376 ... -0.02526855 -0.02735901\n",
       " -0.03106689]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00007982\n",
+      " AAT :  0.00012023\n",
       "\n",
-      " AHV :  0.00001830\n",
+      " AHV :  0.00005425\n",
       "\n",
-      " AMA :  0.00000174\n",
+      " AMA :  0.00050078\n",
       "\n",
-      " ART :  0.00000002\n",
+      " ART :  0.00001794\n",
       "\n",
-      " ASI :  0.00000414\n",
+      " ASI :  0.00000330\n",
       "\n",
-      " AVH :  0.00000063\n",
+      " AVH :  0.00000514\n",
       "\n",
-      " AVT :  0.99989533\n",
+      " AVT :  0.99929833\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[17. 18.]\n",
       "[-0.02043152 -0.01174927 -0.02088928 ...  0.10055542  0.08653259\n",
       "  0.06604004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000969\n",
+      " AAT :  0.00242063\n",
       "\n",
-      " AHV :  0.00028962\n",
+      " AHV :  0.02417476\n",
       "\n",
-      " AMA :  0.00005419\n",
+      " AMA :  0.00962186\n",
       "\n",
-      " ART :  0.00000001\n",
+      " ART :  0.00315190\n",
       "\n",
-      " ASI :  0.00000105\n",
+      " ASI :  0.00090792\n",
       "\n",
-      " AVH :  0.00000232\n",
+      " AVH :  0.00031734\n",
       "\n",
-      " AVT :  0.99964309\n",
+      " AVT :  0.95940560\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[18. 19.]\n",
       "[ 0.04153442  0.01223755 -0.00654602 ...  0.03269958  0.02374268\n",
       "  0.02774048]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000010\n",
+      " AAT :  0.00005964\n",
       "\n",
-      " AHV :  0.00004137\n",
+      " AHV :  0.00011938\n",
       "\n",
-      " AMA :  0.00000706\n",
+      " AMA :  0.00017072\n",
       "\n",
-      " ART :  0.00000000\n",
+      " ART :  0.00000962\n",
       "\n",
-      " ASI :  0.00000019\n",
+      " ASI :  0.00000644\n",
       "\n",
-      " AVH :  0.00000003\n",
+      " AVH :  0.00000328\n",
       "\n",
-      " AVT :  0.99995124\n",
+      " AVT :  0.99963093\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[19. 20.]\n",
       "[0.02185059 0.02069092 0.01451111 ... 0.03469849 0.03985596 0.04600525]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000012\n",
+      " AAT :  0.00040058\n",
       "\n",
-      " AHV :  0.00003513\n",
+      " AHV :  0.00004938\n",
       "\n",
-      " AMA :  0.00000929\n",
+      " AMA :  0.00074538\n",
       "\n",
-      " ART :  0.00000000\n",
+      " ART :  0.00003813\n",
       "\n",
-      " ASI :  0.00000009\n",
+      " ASI :  0.00007063\n",
       "\n",
-      " AVH :  0.00000006\n",
+      " AVH :  0.00003809\n",
       "\n",
-      " AVT :  0.99995530\n",
+      " AVT :  0.99865782\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[20. 21.]\n",
       "[ 0.0353241   0.01567078 -0.00102234 ...  0.1058197   0.10365295\n",
       "  0.09759521]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00005120\n",
+      " AAT :  0.00074374\n",
       "\n",
-      " AHV :  0.00411852\n",
+      " AHV :  0.00207694\n",
       "\n",
-      " AMA :  0.00619672\n",
+      " AMA :  0.00160230\n",
       "\n",
-      " ART :  0.00000305\n",
+      " ART :  0.00084336\n",
       "\n",
-      " ASI :  0.00002620\n",
+      " ASI :  0.00110702\n",
       "\n",
-      " AVH :  0.00000808\n",
+      " AVH :  0.00020138\n",
       "\n",
-      " AVT :  0.98959631\n",
+      " AVT :  0.99342537\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[21. 22.]\n",
       "[ 0.09413147  0.07905579  0.05625916 ... -0.01145935 -0.00245667\n",
       "  0.00479126]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00114753\n",
+      " AAT :  0.00031931\n",
       "\n",
-      " AHV :  0.00382689\n",
+      " AHV :  0.00102429\n",
       "\n",
-      " AMA :  0.00077986\n",
+      " AMA :  0.00634787\n",
       "\n",
-      " ART :  0.00001058\n",
+      " ART :  0.00009260\n",
       "\n",
-      " ASI :  0.00037801\n",
+      " ASI :  0.00047463\n",
       "\n",
-      " AVH :  0.00018464\n",
+      " AVH :  0.00170685\n",
       "\n",
-      " AVT :  0.99367249\n",
+      " AVT :  0.99003452\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[22. 23.]\n",
       "[ 0.0037384   0.01168823  0.01628113 ... -0.03440857 -0.05511475\n",
       " -0.08209229]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000152\n",
+      " AAT :  0.00003239\n",
       "\n",
-      " AHV :  0.00005573\n",
+      " AHV :  0.00036528\n",
       "\n",
-      " AMA :  0.00021121\n",
+      " AMA :  0.00011020\n",
       "\n",
-      " ART :  0.00000004\n",
+      " ART :  0.00007367\n",
       "\n",
-      " ASI :  0.00000169\n",
+      " ASI :  0.00001307\n",
       "\n",
-      " AVH :  0.00000060\n",
+      " AVH :  0.00047624\n",
       "\n",
-      " AVT :  0.99972910\n",
+      " AVT :  0.99892908\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[23. 24.]\n",
       "[-0.1026001  -0.12590027 -0.14944458 ...  0.03462219  0.02537537\n",
       "  0.02354431]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000002\n",
+      " AAT :  0.00000667\n",
       "\n",
-      " AHV :  0.00002918\n",
+      " AHV :  0.00000333\n",
       "\n",
-      " AMA :  0.00004254\n",
+      " AMA :  0.00001054\n",
       "\n",
-      " ART :  0.00000000\n",
+      " ART :  0.00000075\n",
       "\n",
-      " ASI :  0.00000003\n",
+      " ASI :  0.00000556\n",
       "\n",
-      " AVH :  0.00000002\n",
+      " AVH :  0.00063617\n",
       "\n",
-      " AVT :  0.99992824\n",
+      " AVT :  0.99933690\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[24. 25.]\n",
       "[ 0.0196991   0.02836609  0.03103638 ... -0.03009033 -0.03392029\n",
       " -0.03681946]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000001\n",
+      " AAT :  0.00188165\n",
       "\n",
-      " AHV :  0.00000161\n",
+      " AHV :  0.00130030\n",
       "\n",
-      " AMA :  0.00000013\n",
+      " AMA :  0.00001089\n",
       "\n",
-      " ART :  0.00000000\n",
+      " ART :  0.00022374\n",
       "\n",
-      " ASI :  0.00000000\n",
+      " ASI :  0.00001649\n",
       "\n",
-      " AVH :  0.00000001\n",
+      " AVH :  0.00002545\n",
       "\n",
-      " AVT :  0.99999821\n",
+      " AVT :  0.99654144\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[25. 26.]\n",
       "[-0.04151917 -0.03933716 -0.03703308 ...  0.05451965  0.0519104\n",
       "  0.05206299]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000180\n",
+      " AAT :  0.00122705\n",
       "\n",
-      " AHV :  0.00018425\n",
+      " AHV :  0.00029669\n",
       "\n",
-      " AMA :  0.00016731\n",
+      " AMA :  0.00071102\n",
       "\n",
-      " ART :  0.00000001\n",
+      " ART :  0.00031670\n",
       "\n",
-      " ASI :  0.00000116\n",
+      " ASI :  0.00006351\n",
       "\n",
-      " AVH :  0.00000147\n",
+      " AVH :  0.00002656\n",
       "\n",
-      " AVT :  0.99964404\n",
+      " AVT :  0.99735850\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[26. 27.]\n",
       "[ 0.05670166  0.06253052  0.07643127 ... -0.00396729  0.00715637\n",
       "  0.00585938]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.01121760\n",
+      " AAT :  0.01289045\n",
       "\n",
-      " AHV :  0.07370803\n",
+      " AHV :  0.01568283\n",
       "\n",
-      " AMA :  0.02433188\n",
+      " AMA :  0.00384673\n",
       "\n",
-      " ART :  0.00040088\n",
+      " ART :  0.00074689\n",
       "\n",
-      " ASI :  0.00094076\n",
+      " ASI :  0.00053665\n",
       "\n",
-      " AVH :  0.00137472\n",
+      " AVH :  0.00053112\n",
       "\n",
-      " AVT :  0.88802618\n",
+      " AVT :  0.96576536\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[27. 28.]\n",
       "[-0.00222778 -0.01303101 -0.02310181 ...  0.01165771  0.01649475\n",
       "  0.0194397 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00336064\n",
+      " AAT :  0.00012742\n",
       "\n",
-      " AHV :  0.08237971\n",
+      " AHV :  0.00486032\n",
       "\n",
-      " AMA :  0.02419341\n",
+      " AMA :  0.02440478\n",
       "\n",
-      " ART :  0.00006977\n",
+      " ART :  0.00109660\n",
       "\n",
-      " ASI :  0.00062790\n",
+      " ASI :  0.00052789\n",
       "\n",
-      " AVH :  0.00069706\n",
+      " AVH :  0.00020074\n",
       "\n",
-      " AVT :  0.88867158\n",
+      " AVT :  0.96878225\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[28. 29.]\n",
       "[0.01657104 0.01519775 0.00924683 ... 0.03746033 0.03282166 0.02775574]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.01933225\n",
+      " AAT :  0.00012959\n",
       "\n",
-      " AHV :  0.01141812\n",
+      " AHV :  0.00014935\n",
       "\n",
-      " AMA :  0.00131822\n",
+      " AMA :  0.00006704\n",
       "\n",
-      " ART :  0.00006861\n",
+      " ART :  0.00005216\n",
       "\n",
-      " ASI :  0.00037114\n",
+      " ASI :  0.00000344\n",
       "\n",
-      " AVH :  0.00009131\n",
+      " AVH :  0.00001072\n",
       "\n",
-      " AVT :  0.96740037\n",
+      " AVT :  0.99958771\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[29. 30.]\n",
       "[ 0.01919556  0.0135498   0.01724243 ... -0.00575256 -0.01502991\n",
       " -0.02742004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00067066\n",
+      " AAT :  0.00030832\n",
       "\n",
-      " AHV :  0.00015495\n",
+      " AHV :  0.00008630\n",
       "\n",
-      " AMA :  0.00002473\n",
+      " AMA :  0.00009714\n",
       "\n",
-      " ART :  0.00000080\n",
+      " ART :  0.00008344\n",
       "\n",
-      " ASI :  0.00005628\n",
+      " ASI :  0.00000965\n",
       "\n",
-      " AVH :  0.00000687\n",
+      " AVH :  0.00000210\n",
       "\n",
-      " AVT :  0.99908578\n",
+      " AVT :  0.99941301\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[30. 31.]\n",
       "[-0.0322876  -0.0365448  -0.03544617 ... -0.0218811  -0.02978516\n",
       " -0.04052734]\n",
-      "(50, 21)\n",
-      "PREDICTED VALUES\n",
-      "\n",
-      " AAT :  0.00000241\n",
-      "\n"
+      "(128, 32)\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      " AHV :  0.00321765\n",
+      "PREDICTED VALUES\n",
+      "\n",
+      " AAT :  0.00001738\n",
+      "\n",
+      " AHV :  0.00008997\n",
       "\n",
-      " AMA :  0.00698048\n",
+      " AMA :  0.00040408\n",
       "\n",
-      " ART :  0.00000044\n",
+      " ART :  0.00002873\n",
       "\n",
-      " ASI :  0.00000350\n",
+      " ASI :  0.00001739\n",
       "\n",
-      " AVH :  0.00000469\n",
+      " AVH :  0.00004352\n",
       "\n",
-      " AVT :  0.98979086\n",
+      " AVT :  0.99939895\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[31. 32.]\n",
       "[-0.04328918 -0.03413391 -0.03421021 ...  0.05908203  0.06370544\n",
       "  0.05949402]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00053715\n",
+      " AAT :  0.00014836\n",
       "\n",
-      " AHV :  0.00938936\n",
+      " AHV :  0.00004444\n",
       "\n",
-      " AMA :  0.00308272\n",
+      " AMA :  0.00008131\n",
       "\n",
-      " ART :  0.00004391\n",
+      " ART :  0.00006160\n",
       "\n",
-      " ASI :  0.00017127\n",
+      " ASI :  0.00001060\n",
       "\n",
-      " AVH :  0.00006393\n",
+      " AVH :  0.00000214\n",
       "\n",
-      " AVT :  0.98671162\n",
+      " AVT :  0.99965155\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[32. 33.]\n",
       "[ 0.06063843  0.06056213  0.06610107 ... -0.12741089 -0.13371277\n",
       " -0.12313843]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000053\n",
+      " AAT :  0.00000652\n",
       "\n",
-      " AHV :  0.00012500\n",
+      " AHV :  0.00017518\n",
       "\n",
-      " AMA :  0.00003068\n",
+      " AMA :  0.00026005\n",
       "\n",
-      " ART :  0.00000000\n",
+      " ART :  0.00000556\n",
       "\n",
-      " ASI :  0.00000027\n",
+      " ASI :  0.00000064\n",
       "\n",
-      " AVH :  0.00000024\n",
+      " AVH :  0.00000350\n",
       "\n",
-      " AVT :  0.99984336\n",
+      " AVT :  0.99954849\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[33. 34.]\n",
       "[-0.09968567 -0.06376648 -0.03105164 ... -0.0138092  -0.01574707\n",
       " -0.01896667]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00000420\n",
+      " AAT :  0.00003619\n",
       "\n",
-      " AHV :  0.00023515\n",
+      " AHV :  0.00004188\n",
       "\n",
-      " AMA :  0.00047268\n",
+      " AMA :  0.00008402\n",
       "\n",
-      " ART :  0.00000002\n",
+      " ART :  0.00001355\n",
       "\n",
-      " ASI :  0.00000135\n",
+      " ASI :  0.00000258\n",
       "\n",
-      " AVH :  0.00000821\n",
+      " AVH :  0.00001106\n",
       "\n",
-      " AVT :  0.99927837\n",
+      " AVT :  0.99981076\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[34. 35.]\n",
       "[-0.00811768  0.00149536  0.00953674 ... -0.004776   -0.0010376\n",
       "  0.00231934]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.00044373\n",
+      " AAT :  0.00199645\n",
       "\n",
-      " AHV :  0.00030428\n",
+      " AHV :  0.00405928\n",
       "\n",
-      " AMA :  0.00006797\n",
+      " AMA :  0.00041420\n",
       "\n",
-      " ART :  0.00000190\n",
+      " ART :  0.00131999\n",
       "\n",
-      " ASI :  0.00005193\n",
+      " ASI :  0.00015780\n",
       "\n",
-      " AVH :  0.00001423\n",
+      " AVH :  0.00013391\n",
       "\n",
-      " AVT :  0.99911588\n",
+      " AVT :  0.99191839\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
       "[35. 36.]\n",
       "[ 0.00238037  0.00236511  0.00231934 ... -0.00193787  0.0068512\n",
       "  0.00695801]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " AAT :  0.06506672\n",
+      " AAT :  0.00453538\n",
       "\n",
-      " AHV :  0.15438803\n",
+      " AHV :  0.04714373\n",
       "\n",
-      " AMA :  0.04633104\n",
+      " AMA :  0.00146411\n",
       "\n",
-      " ART :  0.00296247\n",
+      " ART :  0.00228473\n",
       "\n",
-      " ASI :  0.00386099\n",
+      " ASI :  0.00070103\n",
       "\n",
-      " AVH :  0.00411445\n",
+      " AVH :  0.00112808\n",
       "\n",
-      " AVT :  0.72327632\n",
+      " AVT :  0.94274294\n",
       "\n",
       "\n",
       "GUESS:  AVT\n",
-      "[{'class': 'AVT', 'timestamp': 0}, {'class': 'AVT', 'timestamp': 1}, {'class': 'Nothing', 'timestamp': 2}, {'class': 'AVT', 'timestamp': 3}, {'class': 'AVT', 'timestamp': 4}, {'class': 'AVT', 'timestamp': 5}, {'class': 'AVT', 'timestamp': 6}, {'class': 'AVT', 'timestamp': 7}, {'class': 'AVT', 'timestamp': 8}, {'class': 'AVT', 'timestamp': 9}, {'class': 'AVT', 'timestamp': 10}, {'class': 'AVT', 'timestamp': 11}, {'class': 'AVT', 'timestamp': 12}, {'class': 'AVT', 'timestamp': 13}, {'class': 'AVT', 'timestamp': 14}, {'class': 'AVT', 'timestamp': 15}, {'class': 'AVT', 'timestamp': 16}, {'class': 'AVT', 'timestamp': 17}, {'class': 'AVT', 'timestamp': 18}, {'class': 'AVT', 'timestamp': 19}, {'class': 'AVT', 'timestamp': 20}, {'class': 'AVT', 'timestamp': 21}, {'class': 'AVT', 'timestamp': 22}, {'class': 'AVT', 'timestamp': 23}, {'class': 'AVT', 'timestamp': 24}, {'class': 'AVT', 'timestamp': 25}, {'class': 'AVT', 'timestamp': 26}, {'class': 'AVT', 'timestamp': 27}, {'class': 'AVT', 'timestamp': 28}, {'class': 'AVT', 'timestamp': 29}, {'class': 'AVT', 'timestamp': 30}, {'class': 'AVT', 'timestamp': 31}, {'class': 'AVT', 'timestamp': 32}, {'class': 'AVT', 'timestamp': 33}, {'class': 'AVT', 'timestamp': 34}, {'class': 'AVT', 'timestamp': 35}]\n"
+      "[{'class': 'AVT', 'timestamp': 0}, {'class': 'AVT', 'timestamp': 1}, {'class': 'AVT', 'timestamp': 2}, {'class': 'AVT', 'timestamp': 3}, {'class': 'AVT', 'timestamp': 4}, {'class': 'AVT', 'timestamp': 5}, {'class': 'AVT', 'timestamp': 6}, {'class': 'AVT', 'timestamp': 7}, {'class': 'AVT', 'timestamp': 8}, {'class': 'AVT', 'timestamp': 9}, {'class': 'AVT', 'timestamp': 10}, {'class': 'AVT', 'timestamp': 11}, {'class': 'AVT', 'timestamp': 12}, {'class': 'AVT', 'timestamp': 13}, {'class': 'AVT', 'timestamp': 14}, {'class': 'AVT', 'timestamp': 15}, {'class': 'AVT', 'timestamp': 16}, {'class': 'AVT', 'timestamp': 17}, {'class': 'AVT', 'timestamp': 18}, {'class': 'AVT', 'timestamp': 19}, {'class': 'AVT', 'timestamp': 20}, {'class': 'AVT', 'timestamp': 21}, {'class': 'AVT', 'timestamp': 22}, {'class': 'AVT', 'timestamp': 23}, {'class': 'AVT', 'timestamp': 24}, {'class': 'AVT', 'timestamp': 25}, {'class': 'AVT', 'timestamp': 26}, {'class': 'AVT', 'timestamp': 27}, {'class': 'AVT', 'timestamp': 28}, {'class': 'AVT', 'timestamp': 29}, {'class': 'AVT', 'timestamp': 30}, {'class': 'AVT', 'timestamp': 31}, {'class': 'AVT', 'timestamp': 32}, {'class': 'AVT', 'timestamp': 33}, {'class': 'AVT', 'timestamp': 34}, {'class': 'AVT', 'timestamp': 35}]\n"
      ]
     }
    ],
diff --git a/ant-cnn/ant_cnn_model.h5 b/ant-cnn/ant_cnn_model.h5
index 2cb098c..57ef8af 100644
Binary files a/ant-cnn/ant_cnn_model.h5 and b/ant-cnn/ant_cnn_model.h5 differ
diff --git a/ant_cnn_model.h5 b/ant_cnn_model.h5
index 2cb098c..57ef8af 100644
Binary files a/ant_cnn_model.h5 and b/ant_cnn_model.h5 differ
diff --git a/bio-cnn/bio-cnn.ipynb b/bio-cnn/bio-cnn.ipynb
index e64372a..6affcb5 100644
--- a/bio-cnn/bio-cnn.ipynb
+++ b/bio-cnn/bio-cnn.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 16,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -19,7 +19,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 17,
    "metadata": {},
    "outputs": [
     {
@@ -28,7 +28,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/18wh70fx\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/18wh70fx</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2scjqmi0\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2scjqmi0</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -42,11 +42,11 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "Saving vectors of label - 'BAM': 100%|███████████████████████████████████████████████| 186/186 [00:01<00:00, 94.62it/s]\n",
-      "Saving vectors of label - 'BBI': 100%|█████████████████████████████████████████████| 1012/1012 [00:15<00:00, 63.87it/s]\n",
-      "Saving vectors of label - 'BIN': 100%|███████████████████████████████████████████████| 133/133 [00:01<00:00, 96.49it/s]\n",
-      "Saving vectors of label - 'BMA': 100%|████████████████████████████████████████████████| 11/11 [00:00<00:00, 129.71it/s]\n",
-      "Saving vectors of label - 'BRA': 100%|████████████████████████████████████████████████| 12/12 [00:00<00:00, 120.30it/s]\n"
+      "Saving vectors of label - 'BAM': 100%|███████████████████████████████████████████████| 564/564 [00:09<00:00, 57.61it/s]\n",
+      "Saving vectors of label - 'BBI': 100%|█████████████████████████████████████████████| 2488/2488 [00:40<00:00, 61.41it/s]\n",
+      "Saving vectors of label - 'BIN': 100%|███████████████████████████████████████████████| 480/480 [00:05<00:00, 82.26it/s]\n",
+      "Saving vectors of label - 'BMA': 100%|████████████████████████████████████████████████| 29/29 [00:00<00:00, 110.56it/s]\n",
+      "Saving vectors of label - 'BRA': 100%|████████████████████████████████████████████████| 32/32 [00:00<00:00, 145.18it/s]\n"
      ]
     }
    ],
@@ -54,8 +54,8 @@
     "wandb.init()\n",
     "config = wandb.config\n",
     "\n",
-    "config.max_len = 21\n",
-    "config.buckets = 50\n",
+    "config.max_len = 32\n",
+    "config.buckets = 128\n",
     "\n",
     "# Save data to array file first\n",
     "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
@@ -68,7 +68,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 18,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -78,13 +78,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 19,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Setting channels to 1 to generalize stereo sound to 1 channel\n",
     "channels = 1\n",
-    "config.epochs = 50\n",
+    "config.epochs = 11\n",
     "config.batch_size = 100\n",
     "\n",
     "# Number of classes\n",
@@ -98,14 +98,14 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 20,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(649, 50, 21, 1)\n"
+      "(1724, 128, 32, 1)\n"
      ]
     }
    ],
@@ -117,7 +117,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -129,7 +129,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 22,
    "metadata": {},
    "outputs": [
     {
@@ -140,38 +140,38 @@
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_4 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_3 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_5 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_4 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_6 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_6 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_7 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_2 (Flatten)          (None, 1296)              0         \n",
+      "flatten_2 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_3 (Dropout)          (None, 1296)              0         \n",
+      "dropout_3 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 64)                83008     \n",
+      "dense_3 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_8 (Activation)    (None, 64)                0         \n",
+      "activation_9 (Activation)    (None, 64)                0         \n",
       "_________________________________________________________________\n",
       "dropout_4 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_3 (Dense)              (None, 5)                 325       \n",
+      "dense_4 (Dense)              (None, 5)                 325       \n",
       "_________________________________________________________________\n",
-      "activation_9 (Activation)    (None, 5)                 0         \n",
+      "activation_10 (Activation)   (None, 5)                 0         \n",
       "=================================================================\n",
-      "Total params: 100,949\n",
-      "Trainable params: 100,949\n",
+      "Total params: 534,101\n",
+      "Trainable params: 534,101\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -182,7 +182,7 @@
        "\"model.add(Conv2D(32, (3, 3),\\n    input_shape=(config.buckets, config.max_len, channels),\\n    activation='relu'))\\n\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\""
       ]
      },
-     "execution_count": 14,
+     "execution_count": 22,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -231,7 +231,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 23,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -243,7 +243,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 24,
    "metadata": {
     "scrolled": false
    },
@@ -254,7 +254,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/y8r5mf51\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/y8r5mf51</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2rmjxl57\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2rmjxl57</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -268,145 +268,59 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(649, 5)\n",
+      "(1724, 5)\n",
       "(5,)\n",
-      "(649, 50, 21, 1)\n",
-      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
-      "\n",
-      "Train on 649 samples, validate on 434 samples\n",
-      "Epoch 1/50\n",
-      "649/649 [==============================] - ETA: 18s - loss: 4.5785 - accuracy: 0.250 - ETA: 9s - loss: 3.7529 - accuracy: 0.328 - ETA: 6s - loss: 3.4751 - accuracy: 0.43 - ETA: 4s - loss: 3.2928 - accuracy: 0.46 - ETA: 3s - loss: 2.9507 - accuracy: 0.52 - ETA: 3s - loss: 2.6885 - accuracy: 0.55 - ETA: 2s - loss: 2.4328 - accuracy: 0.58 - ETA: 2s - loss: 2.3398 - accuracy: 0.58 - ETA: 1s - loss: 2.1312 - accuracy: 0.58 - ETA: 1s - loss: 1.9995 - accuracy: 0.57 - ETA: 0s - loss: 1.9268 - accuracy: 0.57 - ETA: 0s - loss: 1.7802 - accuracy: 0.60 - ETA: 0s - loss: 1.6815 - accuracy: 0.60 - ETA: 0s - loss: 1.6417 - accuracy: 0.60 - ETA: 0s - loss: 1.6113 - accuracy: 0.60 - 2s 3ms/step - loss: 1.5630 - accuracy: 0.6225 - val_loss: 0.7253 - val_accuracy: 0.7350\n",
-      "Epoch 2/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.8659 - accuracy: 0.65 - ETA: 1s - loss: 0.8175 - accuracy: 0.70 - ETA: 0s - loss: 0.8903 - accuracy: 0.71 - ETA: 0s - loss: 0.9474 - accuracy: 0.69 - ETA: 0s - loss: 0.9626 - accuracy: 0.68 - ETA: 0s - loss: 0.9002 - accuracy: 0.70 - ETA: 0s - loss: 0.8823 - accuracy: 0.72 - ETA: 0s - loss: 0.8663 - accuracy: 0.73 - ETA: 0s - loss: 0.8366 - accuracy: 0.73 - ETA: 0s - loss: 0.8283 - accuracy: 0.74 - ETA: 0s - loss: 0.8188 - accuracy: 0.74 - ETA: 0s - loss: 0.8163 - accuracy: 0.74 - 1s 2ms/step - loss: 0.8046 - accuracy: 0.7504 - val_loss: 0.6369 - val_accuracy: 0.7949\n",
-      "Epoch 3/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.87 - ETA: 0s - loss: 0.4927 - accuracy: 0.81 - ETA: 1s - loss: 0.5692 - accuracy: 0.79 - ETA: 1s - loss: 0.5914 - accuracy: 0.79 - ETA: 1s - loss: 0.6884 - accuracy: 0.77 - ETA: 1s - loss: 0.6685 - accuracy: 0.78 - ETA: 1s - loss: 0.7050 - accuracy: 0.77 - ETA: 1s - loss: 0.7016 - accuracy: 0.78 - ETA: 1s - loss: 0.6733 - accuracy: 0.78 - ETA: 0s - loss: 0.6571 - accuracy: 0.78 - ETA: 0s - loss: 0.6611 - accuracy: 0.78 - ETA: 0s - loss: 0.6477 - accuracy: 0.78 - ETA: 0s - loss: 0.6497 - accuracy: 0.78 - ETA: 0s - loss: 0.6792 - accuracy: 0.78 - ETA: 0s - loss: 0.6864 - accuracy: 0.78 - ETA: 0s - loss: 0.7189 - accuracy: 0.78 - ETA: 0s - loss: 0.7382 - accuracy: 0.78 - ETA: 0s - loss: 0.7399 - accuracy: 0.78 - 2s 2ms/step - loss: 0.7342 - accuracy: 0.7843 - val_loss: 0.5574 - val_accuracy: 0.8433\n",
-      "Epoch 4/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.6717 - accuracy: 0.84 - ETA: 1s - loss: 0.7052 - accuracy: 0.76 - ETA: 0s - loss: 0.7503 - accuracy: 0.75 - ETA: 0s - loss: 0.7077 - accuracy: 0.76 - ETA: 0s - loss: 0.6876 - accuracy: 0.76 - ETA: 0s - loss: 0.6479 - accuracy: 0.78 - ETA: 0s - loss: 0.6751 - accuracy: 0.77 - ETA: 0s - loss: 0.6412 - accuracy: 0.78 - ETA: 0s - loss: 0.6124 - accuracy: 0.79 - ETA: 0s - loss: 0.6090 - accuracy: 0.80 - ETA: 0s - loss: 0.6033 - accuracy: 0.80 - ETA: 0s - loss: 0.6043 - accuracy: 0.80 - ETA: 0s - loss: 0.6208 - accuracy: 0.80 - ETA: 0s - loss: 0.5966 - accuracy: 0.81 - ETA: 0s - loss: 0.5869 - accuracy: 0.81 - ETA: 0s - loss: 0.5968 - accuracy: 0.81 - ETA: 0s - loss: 0.6051 - accuracy: 0.81 - ETA: 0s - loss: 0.6010 - accuracy: 0.81 - 2s 3ms/step - loss: 0.5956 - accuracy: 0.8166 - val_loss: 0.5050 - val_accuracy: 0.8410\n",
-      "Epoch 5/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.84 - ETA: 1s - loss: 0.5859 - accuracy: 0.82 - ETA: 1s - loss: 0.5940 - accuracy: 0.82 - ETA: 1s - loss: 0.5367 - accuracy: 0.84 - ETA: 0s - loss: 0.5692 - accuracy: 0.84 - ETA: 0s - loss: 0.5643 - accuracy: 0.84 - ETA: 0s - loss: 0.5627 - accuracy: 0.84 - ETA: 0s - loss: 0.5491 - accuracy: 0.85 - ETA: 0s - loss: 0.5490 - accuracy: 0.84 - ETA: 0s - loss: 0.5596 - accuracy: 0.84 - ETA: 0s - loss: 0.5766 - accuracy: 0.82 - ETA: 0s - loss: 0.5584 - accuracy: 0.83 - ETA: 0s - loss: 0.5569 - accuracy: 0.82 - ETA: 0s - loss: 0.5518 - accuracy: 0.83 - ETA: 0s - loss: 0.5511 - accuracy: 0.82 - 1s 2ms/step - loss: 0.5507 - accuracy: 0.8305 - val_loss: 0.4757 - val_accuracy: 0.8456\n",
-      "Epoch 6/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.7303 - accuracy: 0.81 - ETA: 1s - loss: 0.7337 - accuracy: 0.79 - ETA: 1s - loss: 0.6324 - accuracy: 0.82 - ETA: 1s - loss: 0.6121 - accuracy: 0.82 - ETA: 1s - loss: 0.5691 - accuracy: 0.83 - ETA: 1s - loss: 0.5424 - accuracy: 0.83 - ETA: 1s - loss: 0.4927 - accuracy: 0.86 - ETA: 0s - loss: 0.4726 - accuracy: 0.86 - ETA: 0s - loss: 0.4848 - accuracy: 0.85 - ETA: 0s - loss: 0.5001 - accuracy: 0.84 - ETA: 0s - loss: 0.4957 - accuracy: 0.84 - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.5230 - accuracy: 0.84 - ETA: 0s - loss: 0.5149 - accuracy: 0.84 - 1s 2ms/step - loss: 0.5129 - accuracy: 0.8444 - val_loss: 0.4841 - val_accuracy: 0.8433\n",
-      "Epoch 7/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.75 - ETA: 0s - loss: 0.5079 - accuracy: 0.78 - ETA: 0s - loss: 0.5222 - accuracy: 0.75 - ETA: 1s - loss: 0.4756 - accuracy: 0.78 - ETA: 1s - loss: 0.4594 - accuracy: 0.80 - ETA: 0s - loss: 0.4777 - accuracy: 0.79 - ETA: 0s - loss: 0.4621 - accuracy: 0.79 - ETA: 0s - loss: 0.4779 - accuracy: 0.79 - ETA: 0s - loss: 0.5265 - accuracy: 0.79 - ETA: 0s - loss: 0.5121 - accuracy: 0.80 - ETA: 0s - loss: 0.4991 - accuracy: 0.81 - ETA: 0s - loss: 0.4990 - accuracy: 0.81 - ETA: 0s - loss: 0.4942 - accuracy: 0.81 - ETA: 0s - loss: 0.4876 - accuracy: 0.82 - ETA: 0s - loss: 0.4842 - accuracy: 0.82 - ETA: 0s - loss: 0.4844 - accuracy: 0.82 - ETA: 0s - loss: 0.4791 - accuracy: 0.82 - 1s 2ms/step - loss: 0.4781 - accuracy: 0.8290 - val_loss: 0.4802 - val_accuracy: 0.8456\n",
-      "Epoch 8/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.4091 - accuracy: 0.84 - ETA: 1s - loss: 0.4928 - accuracy: 0.79 - ETA: 0s - loss: 0.4339 - accuracy: 0.82 - ETA: 0s - loss: 0.4299 - accuracy: 0.83 - ETA: 0s - loss: 0.4755 - accuracy: 0.82 - ETA: 0s - loss: 0.4744 - accuracy: 0.83 - ETA: 0s - loss: 0.5089 - accuracy: 0.81 - ETA: 0s - loss: 0.4922 - accuracy: 0.82 - ETA: 0s - loss: 0.4962 - accuracy: 0.82 - ETA: 0s - loss: 0.4873 - accuracy: 0.82 - ETA: 0s - loss: 0.4597 - accuracy: 0.83 - ETA: 0s - loss: 0.4597 - accuracy: 0.83 - ETA: 0s - loss: 0.4504 - accuracy: 0.84 - ETA: 0s - loss: 0.4561 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4585 - accuracy: 0.8444 - val_loss: 0.5894 - val_accuracy: 0.8341\n",
-      "Epoch 9/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.84 - ETA: 0s - loss: 0.2925 - accuracy: 0.90 - ETA: 0s - loss: 0.3366 - accuracy: 0.88 - ETA: 1s - loss: 0.3470 - accuracy: 0.88 - ETA: 0s - loss: 0.3389 - accuracy: 0.88 - ETA: 0s - loss: 0.3328 - accuracy: 0.88 - ETA: 0s - loss: 0.3836 - accuracy: 0.87 - ETA: 0s - loss: 0.3757 - accuracy: 0.86 - ETA: 0s - loss: 0.3875 - accuracy: 0.86 - ETA: 0s - loss: 0.4103 - accuracy: 0.86 - ETA: 0s - loss: 0.4211 - accuracy: 0.85 - ETA: 0s - loss: 0.4203 - accuracy: 0.86 - ETA: 0s - loss: 0.4237 - accuracy: 0.85 - ETA: 0s - loss: 0.4200 - accuracy: 0.85 - ETA: 0s - loss: 0.4386 - accuracy: 0.84 - ETA: 0s - loss: 0.4468 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4483 - accuracy: 0.8413 - val_loss: 0.4525 - val_accuracy: 0.8456\n",
-      "Epoch 10/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.81 - ETA: 1s - loss: 0.4179 - accuracy: 0.84 - ETA: 1s - loss: 0.4159 - accuracy: 0.84 - ETA: 1s - loss: 0.3844 - accuracy: 0.86 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4473 - accuracy: 0.83 - ETA: 0s - loss: 0.4378 - accuracy: 0.84 - ETA: 0s - loss: 0.4244 - accuracy: 0.84 - ETA: 0s - loss: 0.4230 - accuracy: 0.85 - ETA: 0s - loss: 0.4302 - accuracy: 0.85 - ETA: 0s - loss: 0.4255 - accuracy: 0.85 - ETA: 0s - loss: 0.4184 - accuracy: 0.85 - ETA: 0s - loss: 0.4164 - accuracy: 0.85 - ETA: 0s - loss: 0.4170 - accuracy: 0.85 - ETA: 0s - loss: 0.4172 - accuracy: 0.85 - ETA: 0s - loss: 0.4021 - accuracy: 0.86 - 1s 2ms/step - loss: 0.4048 - accuracy: 0.8598 - val_loss: 0.4856 - val_accuracy: 0.8479\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Epoch 11/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.68 - ETA: 0s - loss: 0.5325 - accuracy: 0.78 - ETA: 0s - loss: 0.5262 - accuracy: 0.78 - ETA: 0s - loss: 0.5029 - accuracy: 0.80 - ETA: 0s - loss: 0.4684 - accuracy: 0.81 - ETA: 0s - loss: 0.4503 - accuracy: 0.82 - ETA: 0s - loss: 0.4202 - accuracy: 0.84 - ETA: 0s - loss: 0.4071 - accuracy: 0.84 - ETA: 0s - loss: 0.4123 - accuracy: 0.84 - ETA: 0s - loss: 0.4001 - accuracy: 0.84 - ETA: 0s - loss: 0.3979 - accuracy: 0.85 - ETA: 0s - loss: 0.4200 - accuracy: 0.84 - ETA: 0s - loss: 0.4283 - accuracy: 0.84 - ETA: 0s - loss: 0.4254 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4225 - accuracy: 0.8444 - val_loss: 0.4427 - val_accuracy: 0.8525\n",
-      "Epoch 12/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 1s - loss: 0.2022 - accuracy: 0.96 - ETA: 1s - loss: 0.3082 - accuracy: 0.93 - ETA: 1s - loss: 0.2834 - accuracy: 0.92 - ETA: 0s - loss: 0.3418 - accuracy: 0.89 - ETA: 0s - loss: 0.3851 - accuracy: 0.86 - ETA: 0s - loss: 0.4023 - accuracy: 0.85 - ETA: 0s - loss: 0.3900 - accuracy: 0.86 - ETA: 0s - loss: 0.4120 - accuracy: 0.85 - ETA: 0s - loss: 0.4349 - accuracy: 0.85 - ETA: 0s - loss: 0.4175 - accuracy: 0.85 - ETA: 0s - loss: 0.4192 - accuracy: 0.85 - ETA: 0s - loss: 0.4113 - accuracy: 0.85 - 1s 2ms/step - loss: 0.4072 - accuracy: 0.8598 - val_loss: 0.5036 - val_accuracy: 0.8479\n",
-      "Epoch 13/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.2807 - accuracy: 0.90 - ETA: 1s - loss: 0.3285 - accuracy: 0.87 - ETA: 0s - loss: 0.3767 - accuracy: 0.86 - ETA: 0s - loss: 0.3934 - accuracy: 0.85 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - ETA: 0s - loss: 0.3790 - accuracy: 0.87 - ETA: 0s - loss: 0.3939 - accuracy: 0.86 - ETA: 0s - loss: 0.3750 - accuracy: 0.87 - ETA: 0s - loss: 0.3696 - accuracy: 0.87 - ETA: 0s - loss: 0.3675 - accuracy: 0.87 - ETA: 0s - loss: 0.3717 - accuracy: 0.87 - ETA: 0s - loss: 0.3726 - accuracy: 0.87 - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - ETA: 0s - loss: 0.3792 - accuracy: 0.87 - ETA: 0s - loss: 0.3786 - accuracy: 0.87 - ETA: 0s - loss: 0.3806 - accuracy: 0.87 - 1s 2ms/step - loss: 0.3781 - accuracy: 0.8737 - val_loss: 0.4465 - val_accuracy: 0.8664\n",
-      "Epoch 14/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.4288 - accuracy: 0.81 - ETA: 0s - loss: 0.3394 - accuracy: 0.84 - ETA: 0s - loss: 0.3669 - accuracy: 0.81 - ETA: 0s - loss: 0.3509 - accuracy: 0.83 - ETA: 0s - loss: 0.3477 - accuracy: 0.85 - ETA: 0s - loss: 0.3419 - accuracy: 0.85 - ETA: 0s - loss: 0.3530 - accuracy: 0.85 - ETA: 0s - loss: 0.3622 - accuracy: 0.84 - ETA: 0s - loss: 0.3400 - accuracy: 0.86 - ETA: 0s - loss: 0.3394 - accuracy: 0.86 - ETA: 0s - loss: 0.3650 - accuracy: 0.86 - ETA: 0s - loss: 0.3556 - accuracy: 0.87 - 1s 1ms/step - loss: 0.3632 - accuracy: 0.8690 - val_loss: 0.4390 - val_accuracy: 0.8641\n",
-      "Epoch 15/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.81 - ETA: 0s - loss: 0.3398 - accuracy: 0.85 - ETA: 0s - loss: 0.3541 - accuracy: 0.85 - ETA: 0s - loss: 0.3178 - accuracy: 0.86 - ETA: 0s - loss: 0.3324 - accuracy: 0.86 - ETA: 0s - loss: 0.3403 - accuracy: 0.87 - ETA: 0s - loss: 0.3766 - accuracy: 0.86 - ETA: 0s - loss: 0.4194 - accuracy: 0.85 - ETA: 0s - loss: 0.4123 - accuracy: 0.85 - ETA: 0s - loss: 0.4182 - accuracy: 0.84 - ETA: 0s - loss: 0.4099 - accuracy: 0.84 - ETA: 0s - loss: 0.4362 - accuracy: 0.83 - ETA: 0s - loss: 0.4465 - accuracy: 0.83 - ETA: 0s - loss: 0.4419 - accuracy: 0.83 - ETA: 0s - loss: 0.4352 - accuracy: 0.84 - ETA: 0s - loss: 0.4226 - accuracy: 0.84 - 1s 2ms/step - loss: 0.4214 - accuracy: 0.8459 - val_loss: 0.5638 - val_accuracy: 0.8548\n",
-      "Epoch 16/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.90 - ETA: 0s - loss: 0.4549 - accuracy: 0.85 - ETA: 0s - loss: 0.4302 - accuracy: 0.86 - ETA: 0s - loss: 0.3999 - accuracy: 0.88 - ETA: 0s - loss: 0.3849 - accuracy: 0.87 - ETA: 0s - loss: 0.3758 - accuracy: 0.87 - ETA: 0s - loss: 0.3771 - accuracy: 0.86 - ETA: 0s - loss: 0.3626 - accuracy: 0.87 - ETA: 0s - loss: 0.3539 - accuracy: 0.88 - ETA: 0s - loss: 0.3533 - accuracy: 0.87 - ETA: 0s - loss: 0.3434 - accuracy: 0.87 - ETA: 0s - loss: 0.3444 - accuracy: 0.87 - ETA: 0s - loss: 0.3416 - accuracy: 0.87 - ETA: 0s - loss: 0.3446 - accuracy: 0.86 - ETA: 0s - loss: 0.3419 - accuracy: 0.87 - ETA: 0s - loss: 0.3336 - accuracy: 0.87 - 1s 2ms/step - loss: 0.3373 - accuracy: 0.8721 - val_loss: 0.5021 - val_accuracy: 0.8502\n",
-      "Epoch 17/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.84 - ETA: 0s - loss: 0.4209 - accuracy: 0.85 - ETA: 0s - loss: 0.3868 - accuracy: 0.86 - ETA: 0s - loss: 0.3899 - accuracy: 0.86 - ETA: 0s - loss: 0.3427 - accuracy: 0.88 - ETA: 0s - loss: 0.3341 - accuracy: 0.88 - ETA: 0s - loss: 0.3191 - accuracy: 0.90 - ETA: 0s - loss: 0.3082 - accuracy: 0.90 - ETA: 0s - loss: 0.3161 - accuracy: 0.89 - ETA: 0s - loss: 0.3159 - accuracy: 0.90 - ETA: 0s - loss: 0.3085 - accuracy: 0.90 - ETA: 0s - loss: 0.3141 - accuracy: 0.89 - ETA: 0s - loss: 0.3144 - accuracy: 0.89 - ETA: 0s - loss: 0.3221 - accuracy: 0.89 - ETA: 0s - loss: 0.3107 - accuracy: 0.89 - 1s 2ms/step - loss: 0.3135 - accuracy: 0.8906 - val_loss: 0.4686 - val_accuracy: 0.8641\n",
-      "Epoch 18/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.90 - ETA: 0s - loss: 0.2874 - accuracy: 0.92 - ETA: 0s - loss: 0.2852 - accuracy: 0.90 - ETA: 0s - loss: 0.2846 - accuracy: 0.89 - ETA: 0s - loss: 0.3291 - accuracy: 0.87 - ETA: 0s - loss: 0.3192 - accuracy: 0.87 - ETA: 0s - loss: 0.3440 - accuracy: 0.86 - ETA: 0s - loss: 0.3232 - accuracy: 0.87 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3141 - accuracy: 0.87 - ETA: 0s - loss: 0.3156 - accuracy: 0.87 - ETA: 0s - loss: 0.3037 - accuracy: 0.87 - ETA: 0s - loss: 0.2881 - accuracy: 0.88 - ETA: 0s - loss: 0.2854 - accuracy: 0.88 - ETA: 0s - loss: 0.2830 - accuracy: 0.88 - 1s 2ms/step - loss: 0.2988 - accuracy: 0.8783 - val_loss: 0.4621 - val_accuracy: 0.8502\n",
-      "Epoch 19/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.93 - ETA: 0s - loss: 0.2138 - accuracy: 0.93 - ETA: 1s - loss: 0.2213 - accuracy: 0.93 - ETA: 1s - loss: 0.2246 - accuracy: 0.93 - ETA: 1s - loss: 0.2742 - accuracy: 0.90 - ETA: 1s - loss: 0.2610 - accuracy: 0.91 - ETA: 1s - loss: 0.2959 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2859 - accuracy: 0.88 - ETA: 1s - loss: 0.2825 - accuracy: 0.89 - ETA: 1s - loss: 0.2780 - accuracy: 0.89 - ETA: 0s - loss: 0.2761 - accuracy: 0.89 - ETA: 0s - loss: 0.2798 - accuracy: 0.88 - ETA: 0s - loss: 0.2962 - accuracy: 0.88 - ETA: 0s - loss: 0.2956 - accuracy: 0.87 - ETA: 0s - loss: 0.3031 - accuracy: 0.88 - ETA: 0s - loss: 0.3116 - accuracy: 0.87 - 2s 3ms/step - loss: 0.3118 - accuracy: 0.8752 - val_loss: 0.4765 - val_accuracy: 0.8733\n",
-      "Epoch 20/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.87 - ETA: 1s - loss: 0.2458 - accuracy: 0.85 - ETA: 1s - loss: 0.2645 - accuracy: 0.85 - ETA: 1s - loss: 0.3058 - accuracy: 0.85 - ETA: 1s - loss: 0.3428 - accuracy: 0.85 - ETA: 1s - loss: 0.3322 - accuracy: 0.85 - ETA: 1s - loss: 0.3257 - accuracy: 0.84 - ETA: 1s - loss: 0.3133 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.84 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3231 - accuracy: 0.86 - ETA: 0s - loss: 0.3056 - accuracy: 0.86 - ETA: 0s - loss: 0.2988 - accuracy: 0.87 - ETA: 0s - loss: 0.2987 - accuracy: 0.88 - ETA: 0s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2959 - accuracy: 0.88 - ETA: 0s - loss: 0.2960 - accuracy: 0.88 - ETA: 0s - loss: 0.3035 - accuracy: 0.88 - 2s 3ms/step - loss: 0.3056 - accuracy: 0.8798 - val_loss: 0.4502 - val_accuracy: 0.8733\n",
-      "Epoch 21/50\n"
+      "(1724, 128, 32, 1)\n",
+      "Train on 1724 samples, validate on 1150 samples\n",
+      "Epoch 1/11\n",
+      "1724/1724 [==============================] - ETA: 36s - loss: 2.3595 - accuracy: 0.406 - ETA: 19s - loss: 2.7648 - accuracy: 0.578 - ETA: 14s - loss: 3.0706 - accuracy: 0.541 - ETA: 11s - loss: 2.7080 - accuracy: 0.578 - ETA: 9s - loss: 2.4034 - accuracy: 0.581 - ETA: 8s - loss: 2.1790 - accuracy: 0.55 - ETA: 7s - loss: 2.0256 - accuracy: 0.55 - ETA: 6s - loss: 1.9185 - accuracy: 0.55 - ETA: 6s - loss: 1.8244 - accuracy: 0.56 - ETA: 6s - loss: 1.7247 - accuracy: 0.57 - ETA: 5s - loss: 1.6566 - accuracy: 0.59 - ETA: 5s - loss: 1.5882 - accuracy: 0.61 - ETA: 5s - loss: 1.5701 - accuracy: 0.60 - ETA: 5s - loss: 1.5059 - accuracy: 0.62 - ETA: 5s - loss: 1.4804 - accuracy: 0.61 - ETA: 4s - loss: 1.4343 - accuracy: 0.62 - ETA: 4s - loss: 1.3946 - accuracy: 0.63 - ETA: 4s - loss: 1.3560 - accuracy: 0.64 - ETA: 4s - loss: 1.3337 - accuracy: 0.64 - ETA: 4s - loss: 1.2899 - accuracy: 0.65 - ETA: 4s - loss: 1.2602 - accuracy: 0.66 - ETA: 3s - loss: 1.2327 - accuracy: 0.66 - ETA: 3s - loss: 1.2023 - accuracy: 0.67 - ETA: 3s - loss: 1.1818 - accuracy: 0.67 - ETA: 3s - loss: 1.1526 - accuracy: 0.68 - ETA: 3s - loss: 1.1283 - accuracy: 0.69 - ETA: 3s - loss: 1.1162 - accuracy: 0.69 - ETA: 3s - loss: 1.0979 - accuracy: 0.69 - ETA: 3s - loss: 1.0821 - accuracy: 0.70 - ETA: 3s - loss: 1.0586 - accuracy: 0.70 - ETA: 2s - loss: 1.0464 - accuracy: 0.71 - ETA: 2s - loss: 1.0292 - accuracy: 0.71 - ETA: 2s - loss: 1.0248 - accuracy: 0.71 - ETA: 2s - loss: 1.0142 - accuracy: 0.71 - ETA: 2s - loss: 0.9965 - accuracy: 0.72 - ETA: 2s - loss: 0.9897 - accuracy: 0.71 - ETA: 2s - loss: 0.9764 - accuracy: 0.72 - ETA: 1s - loss: 0.9598 - accuracy: 0.72 - ETA: 1s - loss: 0.9550 - accuracy: 0.72 - ETA: 1s - loss: 0.9404 - accuracy: 0.72 - ETA: 1s - loss: 0.9298 - accuracy: 0.72 - ETA: 1s - loss: 0.9251 - accuracy: 0.73 - ETA: 1s - loss: 0.9165 - accuracy: 0.73 - ETA: 1s - loss: 0.9122 - accuracy: 0.73 - ETA: 1s - loss: 0.9090 - accuracy: 0.73 - ETA: 0s - loss: 0.9061 - accuracy: 0.73 - ETA: 0s - loss: 0.9047 - accuracy: 0.73 - ETA: 0s - loss: 0.8941 - accuracy: 0.74 - ETA: 0s - loss: 0.8889 - accuracy: 0.73 - ETA: 0s - loss: 0.8792 - accuracy: 0.74 - ETA: 0s - loss: 0.8760 - accuracy: 0.74 - ETA: 0s - loss: 0.8765 - accuracy: 0.74 - ETA: 0s - loss: 0.8699 - accuracy: 0.74 - 8s 4ms/step - loss: 0.8663 - accuracy: 0.7448 - val_loss: 0.5761 - val_accuracy: 0.7748\n",
+      "Epoch 2/11\n",
+      "1724/1724 [==============================] - ETA: 7s - loss: 0.7468 - accuracy: 0.81 - ETA: 6s - loss: 0.6847 - accuracy: 0.81 - ETA: 5s - loss: 0.7795 - accuracy: 0.79 - ETA: 4s - loss: 0.7285 - accuracy: 0.78 - ETA: 4s - loss: 0.7104 - accuracy: 0.80 - ETA: 4s - loss: 0.6931 - accuracy: 0.80 - ETA: 4s - loss: 0.6639 - accuracy: 0.80 - ETA: 4s - loss: 0.6344 - accuracy: 0.81 - ETA: 4s - loss: 0.6135 - accuracy: 0.81 - ETA: 4s - loss: 0.6045 - accuracy: 0.81 - ETA: 4s - loss: 0.5995 - accuracy: 0.82 - ETA: 4s - loss: 0.6221 - accuracy: 0.81 - ETA: 4s - loss: 0.5942 - accuracy: 0.82 - ETA: 4s - loss: 0.5866 - accuracy: 0.81 - ETA: 4s - loss: 0.5690 - accuracy: 0.82 - ETA: 4s - loss: 0.5766 - accuracy: 0.82 - ETA: 4s - loss: 0.5942 - accuracy: 0.81 - ETA: 3s - loss: 0.5743 - accuracy: 0.82 - ETA: 3s - loss: 0.5661 - accuracy: 0.82 - ETA: 3s - loss: 0.5723 - accuracy: 0.82 - ETA: 3s - loss: 0.5736 - accuracy: 0.82 - ETA: 3s - loss: 0.5783 - accuracy: 0.81 - ETA: 3s - loss: 0.5734 - accuracy: 0.82 - ETA: 3s - loss: 0.5704 - accuracy: 0.82 - ETA: 3s - loss: 0.5603 - accuracy: 0.82 - ETA: 3s - loss: 0.5547 - accuracy: 0.82 - ETA: 3s - loss: 0.5681 - accuracy: 0.82 - ETA: 3s - loss: 0.5594 - accuracy: 0.82 - ETA: 3s - loss: 0.5455 - accuracy: 0.83 - ETA: 2s - loss: 0.5502 - accuracy: 0.83 - ETA: 2s - loss: 0.5460 - accuracy: 0.83 - ETA: 2s - loss: 0.5373 - accuracy: 0.83 - ETA: 2s - loss: 0.5390 - accuracy: 0.83 - ETA: 2s - loss: 0.5325 - accuracy: 0.83 - ETA: 2s - loss: 0.5227 - accuracy: 0.83 - ETA: 2s - loss: 0.5346 - accuracy: 0.83 - ETA: 2s - loss: 0.5405 - accuracy: 0.83 - ETA: 1s - loss: 0.5352 - accuracy: 0.83 - ETA: 1s - loss: 0.5375 - accuracy: 0.83 - ETA: 1s - loss: 0.5348 - accuracy: 0.83 - ETA: 1s - loss: 0.5290 - accuracy: 0.83 - ETA: 1s - loss: 0.5330 - accuracy: 0.83 - ETA: 1s - loss: 0.5413 - accuracy: 0.82 - ETA: 1s - loss: 0.5366 - accuracy: 0.82 - ETA: 1s - loss: 0.5325 - accuracy: 0.82 - ETA: 0s - loss: 0.5299 - accuracy: 0.83 - ETA: 0s - loss: 0.5326 - accuracy: 0.82 - ETA: 0s - loss: 0.5308 - accuracy: 0.82 - ETA: 0s - loss: 0.5323 - accuracy: 0.82 - ETA: 0s - loss: 0.5285 - accuracy: 0.83 - ETA: 0s - loss: 0.5287 - accuracy: 0.82 - ETA: 0s - loss: 0.5281 - accuracy: 0.82 - ETA: 0s - loss: 0.5228 - accuracy: 0.83 - 8s 5ms/step - loss: 0.5194 - accuracy: 0.8318 - val_loss: 0.4937 - val_accuracy: 0.8513\n",
+      "Epoch 3/11\n",
+      "1724/1724 [==============================] - ETA: 6s - loss: 0.4234 - accuracy: 0.84 - ETA: 5s - loss: 0.3679 - accuracy: 0.85 - ETA: 5s - loss: 0.4395 - accuracy: 0.84 - ETA: 5s - loss: 0.5246 - accuracy: 0.83 - ETA: 5s - loss: 0.5328 - accuracy: 0.82 - ETA: 5s - loss: 0.5089 - accuracy: 0.83 - ETA: 5s - loss: 0.5013 - accuracy: 0.83 - ETA: 5s - loss: 0.4835 - accuracy: 0.84 - ETA: 4s - loss: 0.4636 - accuracy: 0.85 - ETA: 4s - loss: 0.4422 - accuracy: 0.86 - ETA: 4s - loss: 0.4242 - accuracy: 0.86 - ETA: 4s - loss: 0.4079 - accuracy: 0.86 - ETA: 4s - loss: 0.4152 - accuracy: 0.86 - ETA: 4s - loss: 0.4217 - accuracy: 0.86 - ETA: 4s - loss: 0.4193 - accuracy: 0.86 - ETA: 4s - loss: 0.4256 - accuracy: 0.86 - ETA: 4s - loss: 0.4257 - accuracy: 0.86 - ETA: 4s - loss: 0.4338 - accuracy: 0.86 - ETA: 4s - loss: 0.4550 - accuracy: 0.85 - ETA: 4s - loss: 0.4658 - accuracy: 0.85 - ETA: 3s - loss: 0.4654 - accuracy: 0.85 - ETA: 3s - loss: 0.4639 - accuracy: 0.84 - ETA: 3s - loss: 0.4654 - accuracy: 0.84 - ETA: 3s - loss: 0.4790 - accuracy: 0.84 - ETA: 3s - loss: 0.4755 - accuracy: 0.84 - ETA: 3s - loss: 0.4745 - accuracy: 0.84 - ETA: 3s - loss: 0.4776 - accuracy: 0.84 - ETA: 3s - loss: 0.4744 - accuracy: 0.84 - ETA: 2s - loss: 0.4674 - accuracy: 0.84 - ETA: 2s - loss: 0.4676 - accuracy: 0.84 - ETA: 2s - loss: 0.4724 - accuracy: 0.84 - ETA: 2s - loss: 0.4680 - accuracy: 0.84 - ETA: 2s - loss: 0.4632 - accuracy: 0.85 - ETA: 2s - loss: 0.4605 - accuracy: 0.85 - ETA: 2s - loss: 0.4636 - accuracy: 0.85 - ETA: 2s - loss: 0.4616 - accuracy: 0.85 - ETA: 1s - loss: 0.4548 - accuracy: 0.85 - ETA: 1s - loss: 0.4591 - accuracy: 0.85 - ETA: 1s - loss: 0.4539 - accuracy: 0.85 - ETA: 1s - loss: 0.4541 - accuracy: 0.85 - ETA: 1s - loss: 0.4543 - accuracy: 0.85 - ETA: 1s - loss: 0.4526 - accuracy: 0.85 - ETA: 1s - loss: 0.4483 - accuracy: 0.85 - ETA: 1s - loss: 0.4511 - accuracy: 0.85 - ETA: 1s - loss: 0.4509 - accuracy: 0.85 - ETA: 0s - loss: 0.4507 - accuracy: 0.85 - ETA: 0s - loss: 0.4588 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.4541 - accuracy: 0.85 - ETA: 0s - loss: 0.4516 - accuracy: 0.85 - ETA: 0s - loss: 0.4545 - accuracy: 0.85 - ETA: 0s - loss: 0.4554 - accuracy: 0.84 - ETA: 0s - loss: 0.4524 - accuracy: 0.84 - 8s 5ms/step - loss: 0.4519 - accuracy: 0.8492 - val_loss: 0.4259 - val_accuracy: 0.8678\n",
+      "Epoch 4/11\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "649/649 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.87 - ETA: 0s - loss: 0.3177 - accuracy: 0.84 - ETA: 0s - loss: 0.2751 - accuracy: 0.86 - ETA: 0s - loss: 0.2815 - accuracy: 0.86 - ETA: 0s - loss: 0.2594 - accuracy: 0.87 - ETA: 0s - loss: 0.2449 - accuracy: 0.88 - ETA: 0s - loss: 0.2290 - accuracy: 0.89 - ETA: 0s - loss: 0.2240 - accuracy: 0.90 - ETA: 0s - loss: 0.2231 - accuracy: 0.90 - ETA: 0s - loss: 0.2160 - accuracy: 0.90 - ETA: 0s - loss: 0.2141 - accuracy: 0.90 - ETA: 0s - loss: 0.2290 - accuracy: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2472 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.89 - ETA: 0s - loss: 0.2447 - accuracy: 0.89 - ETA: 0s - loss: 0.2490 - accuracy: 0.89 - 1s 2ms/step - loss: 0.2482 - accuracy: 0.8983 - val_loss: 0.5055 - val_accuracy: 0.8825\n",
-      "Epoch 22/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.1187 - accuracy: 0.96 - ETA: 1s - loss: 0.2384 - accuracy: 0.90 - ETA: 1s - loss: 0.2301 - accuracy: 0.90 - ETA: 1s - loss: 0.1995 - accuracy: 0.92 - ETA: 1s - loss: 0.1980 - accuracy: 0.93 - ETA: 0s - loss: 0.2283 - accuracy: 0.91 - ETA: 0s - loss: 0.2145 - accuracy: 0.92 - ETA: 0s - loss: 0.2158 - accuracy: 0.91 - ETA: 0s - loss: 0.2176 - accuracy: 0.91 - ETA: 0s - loss: 0.2171 - accuracy: 0.91 - ETA: 0s - loss: 0.2294 - accuracy: 0.91 - ETA: 0s - loss: 0.2316 - accuracy: 0.91 - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.91 - ETA: 0s - loss: 0.2269 - accuracy: 0.91 - ETA: 0s - loss: 0.2259 - accuracy: 0.91 - ETA: 0s - loss: 0.2290 - accuracy: 0.91 - 2s 3ms/step - loss: 0.2353 - accuracy: 0.9137 - val_loss: 0.4667 - val_accuracy: 0.8710\n",
-      "Epoch 23/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.93 - ETA: 0s - loss: 0.1946 - accuracy: 0.95 - ETA: 0s - loss: 0.1731 - accuracy: 0.95 - ETA: 0s - loss: 0.1680 - accuracy: 0.95 - ETA: 0s - loss: 0.1765 - accuracy: 0.93 - ETA: 0s - loss: 0.2049 - accuracy: 0.93 - ETA: 0s - loss: 0.2022 - accuracy: 0.93 - ETA: 0s - loss: 0.1906 - accuracy: 0.93 - ETA: 0s - loss: 0.1827 - accuracy: 0.93 - ETA: 0s - loss: 0.2022 - accuracy: 0.92 - ETA: 0s - loss: 0.2142 - accuracy: 0.91 - ETA: 0s - loss: 0.2058 - accuracy: 0.92 - ETA: 0s - loss: 0.2074 - accuracy: 0.91 - ETA: 0s - loss: 0.2110 - accuracy: 0.91 - ETA: 0s - loss: 0.2212 - accuracy: 0.91 - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9183 - val_loss: 0.5114 - val_accuracy: 0.8756\n",
-      "Epoch 24/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.1794 - accuracy: 0.90 - ETA: 0s - loss: 0.2208 - accuracy: 0.91 - ETA: 0s - loss: 0.2081 - accuracy: 0.91 - ETA: 0s - loss: 0.2249 - accuracy: 0.92 - ETA: 0s - loss: 0.2209 - accuracy: 0.92 - ETA: 0s - loss: 0.2036 - accuracy: 0.92 - ETA: 0s - loss: 0.2077 - accuracy: 0.92 - ETA: 0s - loss: 0.2069 - accuracy: 0.91 - ETA: 0s - loss: 0.1888 - accuracy: 0.92 - ETA: 0s - loss: 0.1895 - accuracy: 0.92 - ETA: 0s - loss: 0.1833 - accuracy: 0.93 - ETA: 0s - loss: 0.1969 - accuracy: 0.92 - ETA: 0s - loss: 0.1958 - accuracy: 0.92 - 1s 2ms/step - loss: 0.2019 - accuracy: 0.9199 - val_loss: 0.4775 - val_accuracy: 0.8710\n",
-      "Epoch 25/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.87 - ETA: 0s - loss: 0.1811 - accuracy: 0.94 - ETA: 0s - loss: 0.2136 - accuracy: 0.92 - ETA: 0s - loss: 0.2182 - accuracy: 0.93 - ETA: 0s - loss: 0.2101 - accuracy: 0.93 - ETA: 0s - loss: 0.2034 - accuracy: 0.93 - ETA: 0s - loss: 0.2081 - accuracy: 0.93 - ETA: 0s - loss: 0.2104 - accuracy: 0.92 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2271 - accuracy: 0.91 - ETA: 0s - loss: 0.2202 - accuracy: 0.91 - ETA: 0s - loss: 0.2129 - accuracy: 0.92 - ETA: 0s - loss: 0.2006 - accuracy: 0.92 - ETA: 0s - loss: 0.2081 - accuracy: 0.91 - ETA: 0s - loss: 0.2078 - accuracy: 0.92 - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9214 - val_loss: 0.5941 - val_accuracy: 0.8664\n",
-      "Epoch 26/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.90 - ETA: 1s - loss: 0.2454 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.88 - ETA: 0s - loss: 0.2656 - accuracy: 0.88 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2663 - accuracy: 0.88 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2548 - accuracy: 0.88 - ETA: 0s - loss: 0.2570 - accuracy: 0.88 - ETA: 0s - loss: 0.2483 - accuracy: 0.88 - ETA: 0s - loss: 0.2462 - accuracy: 0.89 - ETA: 0s - loss: 0.2451 - accuracy: 0.89 - ETA: 0s - loss: 0.2420 - accuracy: 0.89 - ETA: 0s - loss: 0.2368 - accuracy: 0.89 - ETA: 0s - loss: 0.2320 - accuracy: 0.89 - 2s 3ms/step - loss: 0.2291 - accuracy: 0.8983 - val_loss: 0.6538 - val_accuracy: 0.8641\n",
-      "Epoch 27/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.93 - ETA: 1s - loss: 0.1587 - accuracy: 0.95 - ETA: 1s - loss: 0.1635 - accuracy: 0.95 - ETA: 1s - loss: 0.1572 - accuracy: 0.95 - ETA: 1s - loss: 0.1509 - accuracy: 0.95 - ETA: 0s - loss: 0.1442 - accuracy: 0.95 - ETA: 0s - loss: 0.1536 - accuracy: 0.94 - ETA: 0s - loss: 0.1850 - accuracy: 0.93 - ETA: 0s - loss: 0.1793 - accuracy: 0.93 - ETA: 0s - loss: 0.1777 - accuracy: 0.93 - ETA: 0s - loss: 0.1771 - accuracy: 0.93 - ETA: 0s - loss: 0.1753 - accuracy: 0.93 - ETA: 0s - loss: 0.1784 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.94 - ETA: 0s - loss: 0.1708 - accuracy: 0.94 - ETA: 0s - loss: 0.1661 - accuracy: 0.94 - ETA: 0s - loss: 0.1764 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1724 - accuracy: 0.9414 - val_loss: 0.5725 - val_accuracy: 0.8710\n",
-      "Epoch 28/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.90 - ETA: 1s - loss: 0.2402 - accuracy: 0.93 - ETA: 0s - loss: 0.2111 - accuracy: 0.93 - ETA: 1s - loss: 0.2131 - accuracy: 0.93 - ETA: 0s - loss: 0.2001 - accuracy: 0.93 - ETA: 0s - loss: 0.1892 - accuracy: 0.94 - ETA: 0s - loss: 0.1740 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1608 - accuracy: 0.95 - ETA: 0s - loss: 0.1909 - accuracy: 0.93 - ETA: 0s - loss: 0.1819 - accuracy: 0.93 - ETA: 0s - loss: 0.1729 - accuracy: 0.93 - ETA: 0s - loss: 0.1714 - accuracy: 0.93 - ETA: 0s - loss: 0.1765 - accuracy: 0.93 - ETA: 0s - loss: 0.1703 - accuracy: 0.93 - ETA: 0s - loss: 0.1685 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1739 - accuracy: 0.9322 - val_loss: 0.5412 - val_accuracy: 0.8733\n",
-      "Epoch 29/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.2795 - accuracy: 0.81 - ETA: 1s - loss: 0.2447 - accuracy: 0.87 - ETA: 1s - loss: 0.2021 - accuracy: 0.90 - ETA: 0s - loss: 0.1551 - accuracy: 0.93 - ETA: 0s - loss: 0.1509 - accuracy: 0.93 - ETA: 0s - loss: 0.1419 - accuracy: 0.93 - ETA: 0s - loss: 0.1399 - accuracy: 0.94 - ETA: 0s - loss: 0.1448 - accuracy: 0.94 - ETA: 0s - loss: 0.1456 - accuracy: 0.94 - ETA: 0s - loss: 0.1547 - accuracy: 0.93 - ETA: 0s - loss: 0.1478 - accuracy: 0.94 - ETA: 0s - loss: 0.1496 - accuracy: 0.94 - ETA: 0s - loss: 0.1581 - accuracy: 0.93 - ETA: 0s - loss: 0.1555 - accuracy: 0.93 - ETA: 0s - loss: 0.1544 - accuracy: 0.93 - 1s 2ms/step - loss: 0.1537 - accuracy: 0.9353 - val_loss: 0.7098 - val_accuracy: 0.8594\n",
-      "Epoch 30/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.90 - ETA: 1s - loss: 0.2429 - accuracy: 0.90 - ETA: 1s - loss: 0.1924 - accuracy: 0.91 - ETA: 1s - loss: 0.2077 - accuracy: 0.91 - ETA: 1s - loss: 0.2070 - accuracy: 0.91 - ETA: 1s - loss: 0.2037 - accuracy: 0.91 - ETA: 0s - loss: 0.1707 - accuracy: 0.92 - ETA: 0s - loss: 0.1633 - accuracy: 0.93 - ETA: 0s - loss: 0.1549 - accuracy: 0.93 - ETA: 0s - loss: 0.1565 - accuracy: 0.93 - ETA: 0s - loss: 0.1480 - accuracy: 0.94 - ETA: 0s - loss: 0.1458 - accuracy: 0.94 - ETA: 0s - loss: 0.1409 - accuracy: 0.94 - ETA: 0s - loss: 0.1378 - accuracy: 0.94 - ETA: 0s - loss: 0.1450 - accuracy: 0.94 - ETA: 0s - loss: 0.1760 - accuracy: 0.92 - 1s 2ms/step - loss: 0.1745 - accuracy: 0.9291 - val_loss: 0.5088 - val_accuracy: 0.8571\n",
-      "Epoch 31/50\n"
+      "1724/1724 [==============================] - ETA: 10s - loss: 0.4947 - accuracy: 0.843 - ETA: 8s - loss: 0.3784 - accuracy: 0.875 - ETA: 11s - loss: 0.4152 - accuracy: 0.854 - ETA: 10s - loss: 0.4073 - accuracy: 0.843 - ETA: 9s - loss: 0.3941 - accuracy: 0.856 - ETA: 8s - loss: 0.3931 - accuracy: 0.84 - ETA: 7s - loss: 0.3798 - accuracy: 0.84 - ETA: 6s - loss: 0.3702 - accuracy: 0.85 - ETA: 6s - loss: 0.3820 - accuracy: 0.85 - ETA: 6s - loss: 0.3743 - accuracy: 0.85 - ETA: 5s - loss: 0.3750 - accuracy: 0.85 - ETA: 5s - loss: 0.3638 - accuracy: 0.85 - ETA: 5s - loss: 0.3679 - accuracy: 0.85 - ETA: 5s - loss: 0.3704 - accuracy: 0.85 - ETA: 5s - loss: 0.3887 - accuracy: 0.85 - ETA: 5s - loss: 0.3779 - accuracy: 0.85 - ETA: 4s - loss: 0.3747 - accuracy: 0.85 - ETA: 4s - loss: 0.3747 - accuracy: 0.85 - ETA: 4s - loss: 0.3684 - accuracy: 0.86 - ETA: 4s - loss: 0.3627 - accuracy: 0.86 - ETA: 4s - loss: 0.3569 - accuracy: 0.86 - ETA: 3s - loss: 0.3563 - accuracy: 0.86 - ETA: 3s - loss: 0.3653 - accuracy: 0.85 - ETA: 3s - loss: 0.3742 - accuracy: 0.85 - ETA: 3s - loss: 0.3695 - accuracy: 0.86 - ETA: 3s - loss: 0.3642 - accuracy: 0.86 - ETA: 3s - loss: 0.3676 - accuracy: 0.86 - ETA: 2s - loss: 0.3615 - accuracy: 0.86 - ETA: 2s - loss: 0.3613 - accuracy: 0.86 - ETA: 2s - loss: 0.3668 - accuracy: 0.85 - ETA: 2s - loss: 0.3694 - accuracy: 0.85 - ETA: 2s - loss: 0.3700 - accuracy: 0.85 - ETA: 2s - loss: 0.3635 - accuracy: 0.86 - ETA: 2s - loss: 0.3642 - accuracy: 0.86 - ETA: 2s - loss: 0.3583 - accuracy: 0.86 - ETA: 1s - loss: 0.3595 - accuracy: 0.86 - ETA: 1s - loss: 0.3563 - accuracy: 0.86 - ETA: 1s - loss: 0.3559 - accuracy: 0.86 - ETA: 1s - loss: 0.3537 - accuracy: 0.86 - ETA: 1s - loss: 0.3530 - accuracy: 0.86 - ETA: 1s - loss: 0.3490 - accuracy: 0.86 - ETA: 1s - loss: 0.3520 - accuracy: 0.86 - ETA: 1s - loss: 0.3555 - accuracy: 0.86 - ETA: 1s - loss: 0.3539 - accuracy: 0.86 - ETA: 0s - loss: 0.3567 - accuracy: 0.86 - ETA: 0s - loss: 0.3603 - accuracy: 0.86 - ETA: 0s - loss: 0.3625 - accuracy: 0.86 - ETA: 0s - loss: 0.3637 - accuracy: 0.86 - ETA: 0s - loss: 0.3665 - accuracy: 0.86 - ETA: 0s - loss: 0.3674 - accuracy: 0.86 - ETA: 0s - loss: 0.3680 - accuracy: 0.85 - ETA: 0s - loss: 0.3709 - accuracy: 0.85 - ETA: 0s - loss: 0.3786 - accuracy: 0.85 - 7s 4ms/step - loss: 0.3814 - accuracy: 0.8561 - val_loss: 0.4127 - val_accuracy: 0.8670\n",
+      "Epoch 5/11\n",
+      "1724/1724 [==============================] - ETA: 9s - loss: 0.3703 - accuracy: 0.84 - ETA: 6s - loss: 0.3603 - accuracy: 0.85 - ETA: 7s - loss: 0.3071 - accuracy: 0.88 - ETA: 8s - loss: 0.3665 - accuracy: 0.86 - ETA: 8s - loss: 0.3796 - accuracy: 0.85 - ETA: 8s - loss: 0.4046 - accuracy: 0.84 - ETA: 7s - loss: 0.4030 - accuracy: 0.84 - ETA: 7s - loss: 0.4089 - accuracy: 0.85 - ETA: 7s - loss: 0.3865 - accuracy: 0.86 - ETA: 8s - loss: 0.3783 - accuracy: 0.87 - ETA: 8s - loss: 0.3775 - accuracy: 0.87 - ETA: 7s - loss: 0.3722 - accuracy: 0.87 - ETA: 7s - loss: 0.3680 - accuracy: 0.86 - ETA: 7s - loss: 0.3684 - accuracy: 0.87 - ETA: 6s - loss: 0.3773 - accuracy: 0.86 - ETA: 6s - loss: 0.3689 - accuracy: 0.87 - ETA: 6s - loss: 0.3770 - accuracy: 0.86 - ETA: 5s - loss: 0.3730 - accuracy: 0.86 - ETA: 5s - loss: 0.3762 - accuracy: 0.86 - ETA: 5s - loss: 0.3838 - accuracy: 0.86 - ETA: 5s - loss: 0.3851 - accuracy: 0.86 - ETA: 5s - loss: 0.3795 - accuracy: 0.87 - ETA: 5s - loss: 0.3894 - accuracy: 0.86 - ETA: 5s - loss: 0.3853 - accuracy: 0.86 - ETA: 4s - loss: 0.3865 - accuracy: 0.86 - ETA: 4s - loss: 0.3988 - accuracy: 0.86 - ETA: 4s - loss: 0.3944 - accuracy: 0.86 - ETA: 4s - loss: 0.3918 - accuracy: 0.86 - ETA: 4s - loss: 0.3911 - accuracy: 0.86 - ETA: 3s - loss: 0.3873 - accuracy: 0.86 - ETA: 3s - loss: 0.3872 - accuracy: 0.86 - ETA: 3s - loss: 0.3846 - accuracy: 0.86 - ETA: 3s - loss: 0.3852 - accuracy: 0.87 - ETA: 3s - loss: 0.3827 - accuracy: 0.87 - ETA: 2s - loss: 0.3885 - accuracy: 0.86 - ETA: 2s - loss: 0.3833 - accuracy: 0.87 - ETA: 2s - loss: 0.3853 - accuracy: 0.87 - ETA: 2s - loss: 0.3821 - accuracy: 0.87 - ETA: 2s - loss: 0.3812 - accuracy: 0.87 - ETA: 2s - loss: 0.3766 - accuracy: 0.87 - ETA: 1s - loss: 0.3779 - accuracy: 0.87 - ETA: 1s - loss: 0.3761 - accuracy: 0.87 - ETA: 1s - loss: 0.3724 - accuracy: 0.87 - ETA: 1s - loss: 0.3685 - accuracy: 0.87 - ETA: 1s - loss: 0.3698 - accuracy: 0.87 - ETA: 1s - loss: 0.3670 - accuracy: 0.87 - ETA: 0s - loss: 0.3633 - accuracy: 0.87 - ETA: 0s - loss: 0.3611 - accuracy: 0.87 - ETA: 0s - loss: 0.3616 - accuracy: 0.87 - ETA: 0s - loss: 0.3602 - accuracy: 0.87 - ETA: 0s - loss: 0.3571 - accuracy: 0.87 - ETA: 0s - loss: 0.3538 - accuracy: 0.87 - ETA: 0s - loss: 0.3522 - accuracy: 0.87 - 9s 5ms/step - loss: 0.3503 - accuracy: 0.8788 - val_loss: 0.4078 - val_accuracy: 0.8800\n",
+      "Epoch 6/11\n",
+      "1724/1724 [==============================] - ETA: 6s - loss: 0.1030 - accuracy: 0.96 - ETA: 5s - loss: 0.0982 - accuracy: 0.96 - ETA: 5s - loss: 0.1751 - accuracy: 0.92 - ETA: 5s - loss: 0.2203 - accuracy: 0.88 - ETA: 4s - loss: 0.2887 - accuracy: 0.87 - ETA: 4s - loss: 0.2916 - accuracy: 0.88 - ETA: 4s - loss: 0.2926 - accuracy: 0.87 - ETA: 3s - loss: 0.2922 - accuracy: 0.87 - ETA: 3s - loss: 0.2846 - accuracy: 0.88 - ETA: 3s - loss: 0.2725 - accuracy: 0.89 - ETA: 3s - loss: 0.2744 - accuracy: 0.88 - ETA: 3s - loss: 0.2824 - accuracy: 0.88 - ETA: 3s - loss: 0.2806 - accuracy: 0.88 - ETA: 3s - loss: 0.2775 - accuracy: 0.88 - ETA: 3s - loss: 0.2758 - accuracy: 0.88 - ETA: 3s - loss: 0.2753 - accuracy: 0.88 - ETA: 3s - loss: 0.2783 - accuracy: 0.88 - ETA: 3s - loss: 0.2693 - accuracy: 0.89 - ETA: 3s - loss: 0.2679 - accuracy: 0.89 - ETA: 3s - loss: 0.2672 - accuracy: 0.89 - ETA: 2s - loss: 0.2646 - accuracy: 0.89 - ETA: 2s - loss: 0.2708 - accuracy: 0.89 - ETA: 2s - loss: 0.2687 - accuracy: 0.89 - ETA: 2s - loss: 0.2673 - accuracy: 0.89 - ETA: 2s - loss: 0.2678 - accuracy: 0.89 - ETA: 2s - loss: 0.2782 - accuracy: 0.88 - ETA: 2s - loss: 0.2899 - accuracy: 0.88 - ETA: 2s - loss: 0.2891 - accuracy: 0.88 - ETA: 2s - loss: 0.2954 - accuracy: 0.88 - ETA: 2s - loss: 0.3099 - accuracy: 0.88 - ETA: 2s - loss: 0.3164 - accuracy: 0.87 - ETA: 1s - loss: 0.3206 - accuracy: 0.88 - ETA: 1s - loss: 0.3214 - accuracy: 0.87 - ETA: 1s - loss: 0.3193 - accuracy: 0.88 - ETA: 1s - loss: 0.3175 - accuracy: 0.88 - ETA: 1s - loss: 0.3175 - accuracy: 0.88 - ETA: 1s - loss: 0.3186 - accuracy: 0.88 - ETA: 1s - loss: 0.3174 - accuracy: 0.88 - ETA: 1s - loss: 0.3167 - accuracy: 0.88 - ETA: 1s - loss: 0.3122 - accuracy: 0.88 - ETA: 1s - loss: 0.3120 - accuracy: 0.88 - ETA: 1s - loss: 0.3093 - accuracy: 0.88 - ETA: 0s - loss: 0.3100 - accuracy: 0.88 - ETA: 0s - loss: 0.3111 - accuracy: 0.88 - ETA: 0s - loss: 0.3104 - accuracy: 0.88 - ETA: 0s - loss: 0.3086 - accuracy: 0.88 - ETA: 0s - loss: 0.3146 - accuracy: 0.88 - ETA: 0s - loss: 0.3121 - accuracy: 0.88 - ETA: 0s - loss: 0.3127 - accuracy: 0.88 - ETA: 0s - loss: 0.3106 - accuracy: 0.88 - ETA: 0s - loss: 0.3088 - accuracy: 0.88 - ETA: 0s - loss: 0.3079 - accuracy: 0.88 - ETA: 0s - loss: 0.3052 - accuracy: 0.88 - 6s 3ms/step - loss: 0.3051 - accuracy: 0.8892 - val_loss: 0.3952 - val_accuracy: 0.8791\n",
+      "Epoch 7/11\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "649/649 [==============================] - ETA: 1s - loss: 0.2651 - accuracy: 0.90 - ETA: 0s - loss: 0.2476 - accuracy: 0.90 - ETA: 1s - loss: 0.2378 - accuracy: 0.90 - ETA: 0s - loss: 0.2800 - accuracy: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.89 - ETA: 0s - loss: 0.2510 - accuracy: 0.90 - ETA: 0s - loss: 0.2515 - accuracy: 0.89 - ETA: 0s - loss: 0.2257 - accuracy: 0.90 - ETA: 0s - loss: 0.2191 - accuracy: 0.90 - ETA: 0s - loss: 0.2355 - accuracy: 0.90 - ETA: 0s - loss: 0.2290 - accuracy: 0.90 - ETA: 0s - loss: 0.2122 - accuracy: 0.90 - ETA: 0s - loss: 0.2070 - accuracy: 0.91 - ETA: 0s - loss: 0.2013 - accuracy: 0.91 - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9153 - val_loss: 0.5899 - val_accuracy: 0.8641\n",
-      "Epoch 32/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.93 - ETA: 0s - loss: 0.1653 - accuracy: 0.93 - ETA: 0s - loss: 0.1273 - accuracy: 0.95 - ETA: 0s - loss: 0.0969 - accuracy: 0.97 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0861 - accuracy: 0.97 - ETA: 0s - loss: 0.1017 - accuracy: 0.96 - ETA: 0s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.95 - ETA: 0s - loss: 0.1241 - accuracy: 0.95 - ETA: 0s - loss: 0.1191 - accuracy: 0.95 - ETA: 0s - loss: 0.1211 - accuracy: 0.95 - ETA: 0s - loss: 0.1235 - accuracy: 0.95 - ETA: 0s - loss: 0.1327 - accuracy: 0.95 - ETA: 0s - loss: 0.1307 - accuracy: 0.95 - ETA: 0s - loss: 0.1303 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1292 - accuracy: 0.9553 - val_loss: 0.5471 - val_accuracy: 0.8825\n",
-      "Epoch 33/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.93 - ETA: 0s - loss: 0.1146 - accuracy: 0.95 - ETA: 0s - loss: 0.1137 - accuracy: 0.95 - ETA: 1s - loss: 0.1269 - accuracy: 0.94 - ETA: 1s - loss: 0.1139 - accuracy: 0.95 - ETA: 0s - loss: 0.1047 - accuracy: 0.95 - ETA: 0s - loss: 0.1050 - accuracy: 0.95 - ETA: 0s - loss: 0.1007 - accuracy: 0.95 - ETA: 0s - loss: 0.1225 - accuracy: 0.95 - ETA: 0s - loss: 0.1227 - accuracy: 0.94 - ETA: 0s - loss: 0.1227 - accuracy: 0.95 - ETA: 0s - loss: 0.1279 - accuracy: 0.94 - ETA: 0s - loss: 0.1378 - accuracy: 0.94 - ETA: 0s - loss: 0.1407 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9492 - val_loss: 0.6552 - val_accuracy: 0.8594\n",
-      "Epoch 34/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.2541 - accuracy: 0.90 - ETA: 0s - loss: 0.2018 - accuracy: 0.92 - ETA: 0s - loss: 0.1884 - accuracy: 0.92 - ETA: 0s - loss: 0.1770 - accuracy: 0.93 - ETA: 0s - loss: 0.1635 - accuracy: 0.93 - ETA: 0s - loss: 0.1337 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1418 - accuracy: 0.95 - ETA: 0s - loss: 0.1392 - accuracy: 0.95 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1308 - accuracy: 0.95 - ETA: 0s - loss: 0.1276 - accuracy: 0.96 - ETA: 0s - loss: 0.1240 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9615 - val_loss: 0.6495 - val_accuracy: 0.8641\n",
-      "Epoch 35/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 1.00 - ETA: 0s - loss: 0.1027 - accuracy: 0.96 - ETA: 1s - loss: 0.0851 - accuracy: 0.97 - ETA: 0s - loss: 0.0722 - accuracy: 0.98 - ETA: 1s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.0884 - accuracy: 0.97 - ETA: 0s - loss: 0.1050 - accuracy: 0.96 - ETA: 0s - loss: 0.1028 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.97 - ETA: 0s - loss: 0.0931 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0930 - accuracy: 0.96 - ETA: 0s - loss: 0.1013 - accuracy: 0.96 - ETA: 0s - loss: 0.1127 - accuracy: 0.95 - ETA: 0s - loss: 0.1090 - accuracy: 0.96 - ETA: 0s - loss: 0.1060 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1083 - accuracy: 0.9599 - val_loss: 0.6482 - val_accuracy: 0.8687\n",
-      "Epoch 36/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.93 - ETA: 0s - loss: 0.0801 - accuracy: 0.95 - ETA: 0s - loss: 0.1354 - accuracy: 0.92 - ETA: 0s - loss: 0.1218 - accuracy: 0.93 - ETA: 0s - loss: 0.1529 - accuracy: 0.91 - ETA: 0s - loss: 0.1651 - accuracy: 0.91 - ETA: 0s - loss: 0.1513 - accuracy: 0.91 - ETA: 0s - loss: 0.1482 - accuracy: 0.92 - ETA: 0s - loss: 0.1346 - accuracy: 0.93 - ETA: 0s - loss: 0.1381 - accuracy: 0.93 - ETA: 0s - loss: 0.1464 - accuracy: 0.93 - ETA: 0s - loss: 0.1472 - accuracy: 0.93 - ETA: 0s - loss: 0.1416 - accuracy: 0.93 - ETA: 0s - loss: 0.1455 - accuracy: 0.93 - ETA: 0s - loss: 0.1355 - accuracy: 0.94 - ETA: 0s - loss: 0.1301 - accuracy: 0.94 - ETA: 0s - loss: 0.1285 - accuracy: 0.94 - ETA: 0s - loss: 0.1264 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9445 - val_loss: 0.6935 - val_accuracy: 0.8594\n",
-      "Epoch 37/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 1.00 - ETA: 0s - loss: 0.0606 - accuracy: 0.98 - ETA: 1s - loss: 0.0552 - accuracy: 0.97 - ETA: 1s - loss: 0.0596 - accuracy: 0.98 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0955 - accuracy: 0.96 - ETA: 0s - loss: 0.1082 - accuracy: 0.96 - ETA: 0s - loss: 0.1084 - accuracy: 0.96 - ETA: 0s - loss: 0.1095 - accuracy: 0.96 - ETA: 0s - loss: 0.1184 - accuracy: 0.95 - ETA: 0s - loss: 0.1130 - accuracy: 0.95 - ETA: 0s - loss: 0.1098 - accuracy: 0.95 - ETA: 0s - loss: 0.1110 - accuracy: 0.95 - ETA: 0s - loss: 0.1226 - accuracy: 0.95 - ETA: 0s - loss: 0.1172 - accuracy: 0.96 - ETA: 0s - loss: 0.1142 - accuracy: 0.96 - 1s 2ms/step - loss: 0.1148 - accuracy: 0.9615 - val_loss: 0.6198 - val_accuracy: 0.8710\n",
-      "Epoch 38/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 1.00 - ETA: 1s - loss: 0.0558 - accuracy: 1.00 - ETA: 1s - loss: 0.0856 - accuracy: 0.98 - ETA: 1s - loss: 0.0729 - accuracy: 0.99 - ETA: 1s - loss: 0.0734 - accuracy: 0.98 - ETA: 0s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.1166 - accuracy: 0.96 - ETA: 0s - loss: 0.1110 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.96 - ETA: 0s - loss: 0.1019 - accuracy: 0.96 - ETA: 0s - loss: 0.1095 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1089 - accuracy: 0.95 - ETA: 0s - loss: 0.1185 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1188 - accuracy: 0.9507 - val_loss: 0.6679 - val_accuracy: 0.8710\n",
-      "Epoch 39/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.96 - ETA: 1s - loss: 0.1262 - accuracy: 0.95 - ETA: 1s - loss: 0.1166 - accuracy: 0.94 - ETA: 0s - loss: 0.1601 - accuracy: 0.95 - ETA: 0s - loss: 0.1658 - accuracy: 0.94 - ETA: 0s - loss: 0.1460 - accuracy: 0.94 - ETA: 0s - loss: 0.1388 - accuracy: 0.94 - ETA: 0s - loss: 0.1313 - accuracy: 0.95 - ETA: 0s - loss: 0.1267 - accuracy: 0.95 - ETA: 0s - loss: 0.1265 - accuracy: 0.95 - ETA: 0s - loss: 0.1216 - accuracy: 0.95 - ETA: 0s - loss: 0.1205 - accuracy: 0.95 - ETA: 0s - loss: 0.1221 - accuracy: 0.95 - ETA: 0s - loss: 0.1208 - accuracy: 0.95 - ETA: 0s - loss: 0.1219 - accuracy: 0.95 - ETA: 0s - loss: 0.1170 - accuracy: 0.95 - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9522 - val_loss: 0.7204 - val_accuracy: 0.8641\n",
-      "Epoch 40/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 1s - loss: 0.2143 - accuracy: 0.90 - ETA: 0s - loss: 0.1647 - accuracy: 0.92 - ETA: 0s - loss: 0.1387 - accuracy: 0.93 - ETA: 0s - loss: 0.1354 - accuracy: 0.94 - ETA: 0s - loss: 0.1365 - accuracy: 0.94 - ETA: 0s - loss: 0.1317 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - ETA: 0s - loss: 0.1820 - accuracy: 0.94 - ETA: 0s - loss: 0.1772 - accuracy: 0.94 - ETA: 0s - loss: 0.1781 - accuracy: 0.94 - ETA: 0s - loss: 0.1872 - accuracy: 0.93 - ETA: 0s - loss: 0.1770 - accuracy: 0.94 - ETA: 0s - loss: 0.1763 - accuracy: 0.94 - ETA: 0s - loss: 0.1776 - accuracy: 0.94 - ETA: 0s - loss: 0.1804 - accuracy: 0.93 - ETA: 0s - loss: 0.1764 - accuracy: 0.94 - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9399 - val_loss: 0.5938 - val_accuracy: 0.8687\n",
-      "Epoch 41/50\n"
+      "1724/1724 [==============================] - ETA: 4s - loss: 0.3489 - accuracy: 0.87 - ETA: 4s - loss: 0.3208 - accuracy: 0.84 - ETA: 5s - loss: 0.2521 - accuracy: 0.88 - ETA: 5s - loss: 0.2059 - accuracy: 0.91 - ETA: 5s - loss: 0.2069 - accuracy: 0.91 - ETA: 4s - loss: 0.1927 - accuracy: 0.91 - ETA: 4s - loss: 0.1948 - accuracy: 0.92 - ETA: 4s - loss: 0.1996 - accuracy: 0.92 - ETA: 4s - loss: 0.2011 - accuracy: 0.92 - ETA: 4s - loss: 0.2119 - accuracy: 0.92 - ETA: 4s - loss: 0.2190 - accuracy: 0.92 - ETA: 4s - loss: 0.2183 - accuracy: 0.92 - ETA: 4s - loss: 0.2147 - accuracy: 0.92 - ETA: 4s - loss: 0.2292 - accuracy: 0.91 - ETA: 3s - loss: 0.2272 - accuracy: 0.92 - ETA: 3s - loss: 0.2250 - accuracy: 0.92 - ETA: 3s - loss: 0.2320 - accuracy: 0.92 - ETA: 3s - loss: 0.2310 - accuracy: 0.92 - ETA: 3s - loss: 0.2293 - accuracy: 0.92 - ETA: 3s - loss: 0.2312 - accuracy: 0.92 - ETA: 3s - loss: 0.2280 - accuracy: 0.91 - ETA: 3s - loss: 0.2303 - accuracy: 0.91 - ETA: 3s - loss: 0.2306 - accuracy: 0.91 - ETA: 3s - loss: 0.2329 - accuracy: 0.91 - ETA: 3s - loss: 0.2310 - accuracy: 0.92 - ETA: 3s - loss: 0.2282 - accuracy: 0.92 - ETA: 2s - loss: 0.2310 - accuracy: 0.92 - ETA: 2s - loss: 0.2289 - accuracy: 0.92 - ETA: 2s - loss: 0.2350 - accuracy: 0.92 - ETA: 2s - loss: 0.2326 - accuracy: 0.92 - ETA: 2s - loss: 0.2289 - accuracy: 0.92 - ETA: 2s - loss: 0.2276 - accuracy: 0.92 - ETA: 2s - loss: 0.2240 - accuracy: 0.92 - ETA: 2s - loss: 0.2283 - accuracy: 0.92 - ETA: 2s - loss: 0.2350 - accuracy: 0.92 - ETA: 2s - loss: 0.2333 - accuracy: 0.92 - ETA: 2s - loss: 0.2326 - accuracy: 0.92 - ETA: 1s - loss: 0.2332 - accuracy: 0.92 - ETA: 1s - loss: 0.2315 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2390 - accuracy: 0.91 - ETA: 1s - loss: 0.2394 - accuracy: 0.91 - ETA: 1s - loss: 0.2384 - accuracy: 0.91 - ETA: 1s - loss: 0.2409 - accuracy: 0.91 - ETA: 1s - loss: 0.2399 - accuracy: 0.91 - ETA: 0s - loss: 0.2370 - accuracy: 0.91 - ETA: 0s - loss: 0.2395 - accuracy: 0.91 - ETA: 0s - loss: 0.2428 - accuracy: 0.91 - ETA: 0s - loss: 0.2396 - accuracy: 0.91 - ETA: 0s - loss: 0.2406 - accuracy: 0.91 - ETA: 0s - loss: 0.2401 - accuracy: 0.90 - ETA: 0s - loss: 0.2422 - accuracy: 0.90 - ETA: 0s - loss: 0.2470 - accuracy: 0.90 - 8s 4ms/step - loss: 0.2486 - accuracy: 0.9089 - val_loss: 0.3677 - val_accuracy: 0.8704\n",
+      "Epoch 8/11\n",
+      "1724/1724 [==============================] - ETA: 3s - loss: 0.3543 - accuracy: 0.90 - ETA: 9s - loss: 0.3166 - accuracy: 0.87 - ETA: 7s - loss: 0.3175 - accuracy: 0.86 - ETA: 7s - loss: 0.2711 - accuracy: 0.89 - ETA: 7s - loss: 0.2588 - accuracy: 0.89 - ETA: 6s - loss: 0.2404 - accuracy: 0.90 - ETA: 6s - loss: 0.2240 - accuracy: 0.91 - ETA: 5s - loss: 0.2115 - accuracy: 0.91 - ETA: 5s - loss: 0.2424 - accuracy: 0.90 - ETA: 5s - loss: 0.2483 - accuracy: 0.90 - ETA: 5s - loss: 0.2486 - accuracy: 0.90 - ETA: 5s - loss: 0.2442 - accuracy: 0.90 - ETA: 4s - loss: 0.2489 - accuracy: 0.90 - ETA: 4s - loss: 0.2375 - accuracy: 0.90 - ETA: 4s - loss: 0.2317 - accuracy: 0.91 - ETA: 4s - loss: 0.2277 - accuracy: 0.91 - ETA: 4s - loss: 0.2255 - accuracy: 0.91 - ETA: 3s - loss: 0.2372 - accuracy: 0.90 - ETA: 3s - loss: 0.2323 - accuracy: 0.91 - ETA: 3s - loss: 0.2374 - accuracy: 0.90 - ETA: 3s - loss: 0.2304 - accuracy: 0.91 - ETA: 3s - loss: 0.2275 - accuracy: 0.91 - ETA: 3s - loss: 0.2240 - accuracy: 0.91 - ETA: 3s - loss: 0.2321 - accuracy: 0.91 - ETA: 3s - loss: 0.2331 - accuracy: 0.90 - ETA: 3s - loss: 0.2353 - accuracy: 0.90 - ETA: 3s - loss: 0.2322 - accuracy: 0.90 - ETA: 2s - loss: 0.2295 - accuracy: 0.90 - ETA: 2s - loss: 0.2273 - accuracy: 0.90 - ETA: 2s - loss: 0.2278 - accuracy: 0.91 - ETA: 2s - loss: 0.2291 - accuracy: 0.90 - ETA: 2s - loss: 0.2266 - accuracy: 0.91 - ETA: 2s - loss: 0.2247 - accuracy: 0.91 - ETA: 2s - loss: 0.2229 - accuracy: 0.91 - ETA: 2s - loss: 0.2237 - accuracy: 0.91 - ETA: 1s - loss: 0.2225 - accuracy: 0.91 - ETA: 1s - loss: 0.2211 - accuracy: 0.91 - ETA: 1s - loss: 0.2242 - accuracy: 0.91 - ETA: 1s - loss: 0.2232 - accuracy: 0.91 - ETA: 1s - loss: 0.2196 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2174 - accuracy: 0.91 - ETA: 1s - loss: 0.2178 - accuracy: 0.91 - ETA: 1s - loss: 0.2148 - accuracy: 0.91 - ETA: 0s - loss: 0.2131 - accuracy: 0.91 - ETA: 0s - loss: 0.2177 - accuracy: 0.91 - ETA: 0s - loss: 0.2201 - accuracy: 0.91 - ETA: 0s - loss: 0.2198 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2199 - accuracy: 0.91 - ETA: 0s - loss: 0.2227 - accuracy: 0.91 - ETA: 0s - loss: 0.2201 - accuracy: 0.91 - ETA: 0s - loss: 0.2206 - accuracy: 0.91 - 7s 4ms/step - loss: 0.2219 - accuracy: 0.9130 - val_loss: 0.4182 - val_accuracy: 0.8791\n",
+      "Epoch 9/11\n",
+      "1724/1724 [==============================] - ETA: 7s - loss: 0.0716 - accuracy: 0.96 - ETA: 6s - loss: 0.0964 - accuracy: 0.96 - ETA: 5s - loss: 0.1440 - accuracy: 0.93 - ETA: 5s - loss: 0.1362 - accuracy: 0.94 - ETA: 4s - loss: 0.1837 - accuracy: 0.92 - ETA: 4s - loss: 0.1595 - accuracy: 0.93 - ETA: 4s - loss: 0.1693 - accuracy: 0.93 - ETA: 3s - loss: 0.1577 - accuracy: 0.94 - ETA: 3s - loss: 0.1704 - accuracy: 0.93 - ETA: 3s - loss: 0.1770 - accuracy: 0.93 - ETA: 3s - loss: 0.1829 - accuracy: 0.93 - ETA: 3s - loss: 0.1824 - accuracy: 0.93 - ETA: 3s - loss: 0.1817 - accuracy: 0.93 - ETA: 3s - loss: 0.1773 - accuracy: 0.94 - ETA: 3s - loss: 0.1825 - accuracy: 0.94 - ETA: 3s - loss: 0.1825 - accuracy: 0.93 - ETA: 3s - loss: 0.1878 - accuracy: 0.93 - ETA: 2s - loss: 0.1941 - accuracy: 0.93 - ETA: 2s - loss: 0.2163 - accuracy: 0.92 - ETA: 2s - loss: 0.2210 - accuracy: 0.92 - ETA: 2s - loss: 0.2162 - accuracy: 0.92 - ETA: 2s - loss: 0.2099 - accuracy: 0.92 - ETA: 2s - loss: 0.2063 - accuracy: 0.92 - ETA: 2s - loss: 0.2100 - accuracy: 0.92 - ETA: 2s - loss: 0.2177 - accuracy: 0.92 - ETA: 2s - loss: 0.2190 - accuracy: 0.92 - ETA: 2s - loss: 0.2170 - accuracy: 0.92 - ETA: 2s - loss: 0.2193 - accuracy: 0.92 - ETA: 1s - loss: 0.2171 - accuracy: 0.92 - ETA: 1s - loss: 0.2224 - accuracy: 0.92 - ETA: 1s - loss: 0.2197 - accuracy: 0.92 - ETA: 1s - loss: 0.2183 - accuracy: 0.92 - ETA: 1s - loss: 0.2163 - accuracy: 0.92 - ETA: 1s - loss: 0.2133 - accuracy: 0.92 - ETA: 1s - loss: 0.2121 - accuracy: 0.92 - ETA: 1s - loss: 0.2082 - accuracy: 0.92 - ETA: 1s - loss: 0.2033 - accuracy: 0.92 - ETA: 1s - loss: 0.2011 - accuracy: 0.92 - ETA: 1s - loss: 0.2006 - accuracy: 0.92 - ETA: 1s - loss: 0.1992 - accuracy: 0.92 - ETA: 1s - loss: 0.1992 - accuracy: 0.92 - ETA: 1s - loss: 0.1990 - accuracy: 0.92 - ETA: 1s - loss: 0.1961 - accuracy: 0.92 - ETA: 0s - loss: 0.1984 - accuracy: 0.92 - ETA: 0s - loss: 0.1979 - accuracy: 0.92 - ETA: 0s - loss: 0.1985 - accuracy: 0.92 - ETA: 0s - loss: 0.1966 - accuracy: 0.92 - ETA: 0s - loss: 0.2001 - accuracy: 0.92 - ETA: 0s - loss: 0.2039 - accuracy: 0.92 - ETA: 0s - loss: 0.2023 - accuracy: 0.92 - ETA: 0s - loss: 0.2009 - accuracy: 0.92 - ETA: 0s - loss: 0.2005 - accuracy: 0.92 - ETA: 0s - loss: 0.2011 - accuracy: 0.92 - 7s 4ms/step - loss: 0.1997 - accuracy: 0.9234 - val_loss: 0.3574 - val_accuracy: 0.8913\n",
+      "Epoch 10/11\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "649/649 [==============================] - ETA: 1s - loss: 0.0726 - accuracy: 1.00 - ETA: 0s - loss: 0.1056 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.96 - ETA: 1s - loss: 0.1043 - accuracy: 0.95 - ETA: 0s - loss: 0.1051 - accuracy: 0.95 - ETA: 0s - loss: 0.1105 - accuracy: 0.95 - ETA: 0s - loss: 0.1095 - accuracy: 0.95 - ETA: 0s - loss: 0.1195 - accuracy: 0.94 - ETA: 0s - loss: 0.1348 - accuracy: 0.94 - ETA: 0s - loss: 0.1450 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1422 - accuracy: 0.94 - ETA: 0s - loss: 0.1377 - accuracy: 0.94 - 1s 2ms/step - loss: 0.1318 - accuracy: 0.9492 - val_loss: 0.6164 - val_accuracy: 0.8779\n",
-      "Epoch 42/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.96 - ETA: 0s - loss: 0.0564 - accuracy: 0.98 - ETA: 0s - loss: 0.0657 - accuracy: 0.98 - ETA: 0s - loss: 0.0611 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0992 - accuracy: 0.96 - ETA: 0s - loss: 0.0943 - accuracy: 0.96 - ETA: 0s - loss: 0.1057 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.96 - ETA: 0s - loss: 0.1083 - accuracy: 0.96 - ETA: 0s - loss: 0.1067 - accuracy: 0.96 - ETA: 0s - loss: 0.1035 - accuracy: 0.96 - ETA: 0s - loss: 0.0966 - accuracy: 0.96 - 1s 2ms/step - loss: 0.0955 - accuracy: 0.9676 - val_loss: 0.7090 - val_accuracy: 0.8802\n",
-      "Epoch 43/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.96 - ETA: 0s - loss: 0.0581 - accuracy: 0.96 - ETA: 0s - loss: 0.0814 - accuracy: 0.96 - ETA: 1s - loss: 0.0767 - accuracy: 0.96 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0630 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0641 - accuracy: 0.97 - ETA: 0s - loss: 0.0798 - accuracy: 0.97 - ETA: 0s - loss: 0.0779 - accuracy: 0.97 - ETA: 0s - loss: 0.0774 - accuracy: 0.97 - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9707 - val_loss: 0.6879 - val_accuracy: 0.8756\n",
-      "Epoch 44/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.93 - ETA: 0s - loss: 0.0565 - accuracy: 0.96 - ETA: 0s - loss: 0.0654 - accuracy: 0.96 - ETA: 0s - loss: 0.0680 - accuracy: 0.96 - ETA: 0s - loss: 0.0625 - accuracy: 0.96 - ETA: 0s - loss: 0.0614 - accuracy: 0.96 - ETA: 0s - loss: 0.0605 - accuracy: 0.96 - ETA: 0s - loss: 0.0612 - accuracy: 0.96 - ETA: 0s - loss: 0.0773 - accuracy: 0.96 - ETA: 0s - loss: 0.0787 - accuracy: 0.96 - ETA: 0s - loss: 0.0766 - accuracy: 0.96 - ETA: 0s - loss: 0.0758 - accuracy: 0.96 - ETA: 0s - loss: 0.0750 - accuracy: 0.96 - 1s 2ms/step - loss: 0.0724 - accuracy: 0.9676 - val_loss: 0.6215 - val_accuracy: 0.8733\n",
-      "Epoch 45/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 1.00 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.99 - ETA: 0s - loss: 0.0567 - accuracy: 0.98 - ETA: 0s - loss: 0.0543 - accuracy: 0.98 - ETA: 0s - loss: 0.0573 - accuracy: 0.98 - ETA: 0s - loss: 0.0557 - accuracy: 0.98 - ETA: 0s - loss: 0.0675 - accuracy: 0.98 - ETA: 0s - loss: 0.0701 - accuracy: 0.98 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0632 - accuracy: 0.9846 - val_loss: 0.8091 - val_accuracy: 0.8802\n",
-      "Epoch 46/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.96 - ETA: 1s - loss: 0.0486 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0711 - accuracy: 0.97 - ETA: 1s - loss: 0.0681 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0855 - accuracy: 0.96 - ETA: 0s - loss: 0.0829 - accuracy: 0.96 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0695 - accuracy: 0.97 - ETA: 0s - loss: 0.0693 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0825 - accuracy: 0.97 - ETA: 0s - loss: 0.0753 - accuracy: 0.98 - ETA: 0s - loss: 0.0741 - accuracy: 0.98 - ETA: 0s - loss: 0.0718 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0709 - accuracy: 0.9815 - val_loss: 0.7007 - val_accuracy: 0.8802\n",
-      "Epoch 47/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.0128 - accuracy: 1.00 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.99 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0494 - accuracy: 0.98 - ETA: 0s - loss: 0.0518 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0557 - accuracy: 0.98 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.8195 - val_accuracy: 0.8802\n",
-      "Epoch 48/50\n",
-      "649/649 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.96 - ETA: 1s - loss: 0.1126 - accuracy: 0.95 - ETA: 0s - loss: 0.0792 - accuracy: 0.96 - ETA: 0s - loss: 0.0835 - accuracy: 0.96 - ETA: 0s - loss: 0.0729 - accuracy: 0.97 - ETA: 0s - loss: 0.0645 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.98 - ETA: 0s - loss: 0.0632 - accuracy: 0.97 - ETA: 0s - loss: 0.0763 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0679 - accuracy: 0.97 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - 1s 1ms/step - loss: 0.0641 - accuracy: 0.9769 - val_loss: 0.6818 - val_accuracy: 0.8710\n",
-      "Epoch 49/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.3168 - accuracy: 0.93 - ETA: 1s - loss: 0.1705 - accuracy: 0.96 - ETA: 0s - loss: 0.1010 - accuracy: 0.97 - ETA: 0s - loss: 0.0860 - accuracy: 0.98 - ETA: 0s - loss: 0.0812 - accuracy: 0.98 - ETA: 0s - loss: 0.0770 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.98 - ETA: 0s - loss: 0.0654 - accuracy: 0.98 - ETA: 0s - loss: 0.0636 - accuracy: 0.98 - ETA: 0s - loss: 0.0616 - accuracy: 0.98 - ETA: 0s - loss: 0.0633 - accuracy: 0.98 - ETA: 0s - loss: 0.0608 - accuracy: 0.98 - ETA: 0s - loss: 0.0611 - accuracy: 0.98 - ETA: 0s - loss: 0.0614 - accuracy: 0.98 - ETA: 0s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0576 - accuracy: 0.98 - ETA: 0s - loss: 0.0555 - accuracy: 0.98 - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.8624 - val_accuracy: 0.8802\n",
-      "Epoch 50/50\n",
-      "649/649 [==============================] - ETA: 1s - loss: 0.0273 - accuracy: 1.00 - ETA: 0s - loss: 0.0761 - accuracy: 0.96 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0525 - accuracy: 0.97 - ETA: 0s - loss: 0.0529 - accuracy: 0.97 - ETA: 0s - loss: 0.0592 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0592 - accuracy: 0.97 - ETA: 0s - loss: 0.0534 - accuracy: 0.97 - ETA: 0s - loss: 0.0531 - accuracy: 0.97 - ETA: 0s - loss: 0.0514 - accuracy: 0.97 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0570 - accuracy: 0.97 - ETA: 0s - loss: 0.0610 - accuracy: 0.97 - 1s 2ms/step - loss: 0.0602 - accuracy: 0.9769 - val_loss: 0.7308 - val_accuracy: 0.8779\n"
+      "1724/1724 [==============================] - ETA: 6s - loss: 0.1649 - accuracy: 0.90 - ETA: 6s - loss: 0.1684 - accuracy: 0.89 - ETA: 6s - loss: 0.1324 - accuracy: 0.91 - ETA: 6s - loss: 0.1258 - accuracy: 0.92 - ETA: 5s - loss: 0.1250 - accuracy: 0.93 - ETA: 5s - loss: 0.1375 - accuracy: 0.93 - ETA: 5s - loss: 0.1356 - accuracy: 0.93 - ETA: 5s - loss: 0.1326 - accuracy: 0.94 - ETA: 4s - loss: 0.1300 - accuracy: 0.94 - ETA: 4s - loss: 0.1390 - accuracy: 0.94 - ETA: 4s - loss: 0.1340 - accuracy: 0.94 - ETA: 4s - loss: 0.1266 - accuracy: 0.95 - ETA: 4s - loss: 0.1220 - accuracy: 0.95 - ETA: 4s - loss: 0.1243 - accuracy: 0.95 - ETA: 4s - loss: 0.1195 - accuracy: 0.95 - ETA: 4s - loss: 0.1416 - accuracy: 0.95 - ETA: 4s - loss: 0.1377 - accuracy: 0.95 - ETA: 4s - loss: 0.1333 - accuracy: 0.95 - ETA: 3s - loss: 0.1328 - accuracy: 0.95 - ETA: 3s - loss: 0.1378 - accuracy: 0.95 - ETA: 3s - loss: 0.1405 - accuracy: 0.95 - ETA: 3s - loss: 0.1399 - accuracy: 0.95 - ETA: 3s - loss: 0.1391 - accuracy: 0.95 - ETA: 3s - loss: 0.1362 - accuracy: 0.95 - ETA: 3s - loss: 0.1351 - accuracy: 0.95 - ETA: 3s - loss: 0.1334 - accuracy: 0.95 - ETA: 2s - loss: 0.1387 - accuracy: 0.95 - ETA: 2s - loss: 0.1373 - accuracy: 0.95 - ETA: 2s - loss: 0.1405 - accuracy: 0.95 - ETA: 2s - loss: 0.1408 - accuracy: 0.95 - ETA: 2s - loss: 0.1391 - accuracy: 0.94 - ETA: 2s - loss: 0.1368 - accuracy: 0.95 - ETA: 2s - loss: 0.1352 - accuracy: 0.94 - ETA: 2s - loss: 0.1376 - accuracy: 0.95 - ETA: 2s - loss: 0.1371 - accuracy: 0.95 - ETA: 2s - loss: 0.1374 - accuracy: 0.94 - ETA: 1s - loss: 0.1412 - accuracy: 0.94 - ETA: 1s - loss: 0.1420 - accuracy: 0.94 - ETA: 1s - loss: 0.1421 - accuracy: 0.94 - ETA: 1s - loss: 0.1449 - accuracy: 0.94 - ETA: 1s - loss: 0.1437 - accuracy: 0.94 - ETA: 1s - loss: 0.1422 - accuracy: 0.94 - ETA: 1s - loss: 0.1396 - accuracy: 0.94 - ETA: 1s - loss: 0.1410 - accuracy: 0.94 - ETA: 1s - loss: 0.1411 - accuracy: 0.94 - ETA: 0s - loss: 0.1440 - accuracy: 0.94 - ETA: 0s - loss: 0.1509 - accuracy: 0.94 - ETA: 0s - loss: 0.1512 - accuracy: 0.94 - ETA: 0s - loss: 0.1508 - accuracy: 0.94 - ETA: 0s - loss: 0.1519 - accuracy: 0.94 - ETA: 0s - loss: 0.1504 - accuracy: 0.94 - ETA: 0s - loss: 0.1513 - accuracy: 0.94 - ETA: 0s - loss: 0.1533 - accuracy: 0.94 - 7s 4ms/step - loss: 0.1553 - accuracy: 0.9420 - val_loss: 0.3454 - val_accuracy: 0.8878\n",
+      "Epoch 11/11\n",
+      "1724/1724 [==============================] - ETA: 6s - loss: 0.1769 - accuracy: 0.90 - ETA: 5s - loss: 0.1904 - accuracy: 0.90 - ETA: 5s - loss: 0.2045 - accuracy: 0.90 - ETA: 5s - loss: 0.1816 - accuracy: 0.91 - ETA: 5s - loss: 0.1741 - accuracy: 0.91 - ETA: 5s - loss: 0.1567 - accuracy: 0.92 - ETA: 5s - loss: 0.1692 - accuracy: 0.92 - ETA: 5s - loss: 0.1813 - accuracy: 0.92 - ETA: 5s - loss: 0.1707 - accuracy: 0.93 - ETA: 5s - loss: 0.1641 - accuracy: 0.93 - ETA: 4s - loss: 0.1621 - accuracy: 0.93 - ETA: 4s - loss: 0.1662 - accuracy: 0.92 - ETA: 4s - loss: 0.1636 - accuracy: 0.93 - ETA: 4s - loss: 0.1584 - accuracy: 0.93 - ETA: 4s - loss: 0.1500 - accuracy: 0.94 - ETA: 4s - loss: 0.1461 - accuracy: 0.94 - ETA: 4s - loss: 0.1463 - accuracy: 0.94 - ETA: 3s - loss: 0.1428 - accuracy: 0.94 - ETA: 3s - loss: 0.1544 - accuracy: 0.93 - ETA: 3s - loss: 0.1542 - accuracy: 0.94 - ETA: 3s - loss: 0.1556 - accuracy: 0.94 - ETA: 3s - loss: 0.1536 - accuracy: 0.94 - ETA: 3s - loss: 0.1496 - accuracy: 0.94 - ETA: 3s - loss: 0.1474 - accuracy: 0.94 - ETA: 3s - loss: 0.1449 - accuracy: 0.94 - ETA: 3s - loss: 0.1449 - accuracy: 0.94 - ETA: 3s - loss: 0.1436 - accuracy: 0.94 - ETA: 3s - loss: 0.1486 - accuracy: 0.94 - ETA: 3s - loss: 0.1475 - accuracy: 0.94 - ETA: 3s - loss: 0.1463 - accuracy: 0.94 - ETA: 2s - loss: 0.1435 - accuracy: 0.94 - ETA: 2s - loss: 0.1436 - accuracy: 0.94 - ETA: 2s - loss: 0.1418 - accuracy: 0.94 - ETA: 2s - loss: 0.1406 - accuracy: 0.94 - ETA: 2s - loss: 0.1434 - accuracy: 0.94 - ETA: 2s - loss: 0.1452 - accuracy: 0.94 - ETA: 2s - loss: 0.1483 - accuracy: 0.94 - ETA: 2s - loss: 0.1514 - accuracy: 0.94 - ETA: 1s - loss: 0.1513 - accuracy: 0.94 - ETA: 1s - loss: 0.1532 - accuracy: 0.94 - ETA: 1s - loss: 0.1552 - accuracy: 0.94 - ETA: 1s - loss: 0.1544 - accuracy: 0.94 - ETA: 1s - loss: 0.1536 - accuracy: 0.94 - ETA: 1s - loss: 0.1526 - accuracy: 0.94 - ETA: 1s - loss: 0.1555 - accuracy: 0.94 - ETA: 0s - loss: 0.1537 - accuracy: 0.94 - ETA: 0s - loss: 0.1518 - accuracy: 0.94 - ETA: 0s - loss: 0.1563 - accuracy: 0.94 - ETA: 0s - loss: 0.1568 - accuracy: 0.94 - ETA: 0s - loss: 0.1557 - accuracy: 0.94 - ETA: 0s - loss: 0.1591 - accuracy: 0.94 - ETA: 0s - loss: 0.1590 - accuracy: 0.93 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - 8s 5ms/step - loss: 0.1608 - accuracy: 0.9397 - val_loss: 0.4344 - val_accuracy: 0.8583\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<keras.callbacks.callbacks.History at 0x1c6d6201688>"
+       "<keras.callbacks.callbacks.History at 0x182456011c8>"
       ]
      },
-     "execution_count": 16,
+     "execution_count": 24,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -424,7 +338,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 25,
    "metadata": {},
    "outputs": [
     {
@@ -450,7 +364,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 26,
    "metadata": {
     "scrolled": true
    },
@@ -459,12 +373,12 @@
     "from keras.models import load_model\n",
     "\n",
     "# Load the model\n",
-    "loaded_model = load_model('ant_cnn_model.h5')"
+    "loaded_model = load_model('bio_cnn_model.h5')"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 27,
    "metadata": {},
    "outputs": [
     {
@@ -475,38 +389,38 @@
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_4 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_3 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_5 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_4 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_6 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_6 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_7 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_2 (Flatten)          (None, 1296)              0         \n",
+      "flatten_2 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_3 (Dropout)          (None, 1296)              0         \n",
+      "dropout_3 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 64)                83008     \n",
+      "dense_3 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_8 (Activation)    (None, 64)                0         \n",
+      "activation_9 (Activation)    (None, 64)                0         \n",
       "_________________________________________________________________\n",
       "dropout_4 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_3 (Dense)              (None, 5)                 325       \n",
+      "dense_4 (Dense)              (None, 5)                 325       \n",
       "_________________________________________________________________\n",
-      "activation_9 (Activation)    (None, 5)                 0         \n",
+      "activation_10 (Activation)   (None, 5)                 0         \n",
       "=================================================================\n",
-      "Total params: 100,949\n",
-      "Trainable params: 100,949\n",
+      "Total params: 534,101\n",
+      "Trainable params: 534,101\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -519,25 +433,35 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 28,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "[[ 28   4   0   0   0]\n",
-      " [  0 202   5   0   0]\n",
-      " [  0  10  13   0   0]\n",
-      " [  0   0   1   0   0]\n",
-      " [  0   7   1   0   0]]\n",
-      "Accuracy for class BRA : [0.875]\n",
-      "Accuracy for class BAM : [0.97584541]\n",
-      "Accuracy for class BBI : [0.56521739]\n",
+      "[[125  10   0   0   0]\n",
+      " [  3 430  45   0   0]\n",
+      " [  0  17  78   0   0]\n",
+      " [  0   5   0   0   0]\n",
+      " [  0   4   0   0   2]]\n",
+      "Accuracy for class BRA : [0.92592593]\n",
+      "Accuracy for class BAM : [0.89958159]\n",
+      "Accuracy for class BBI : [0.82105263]\n",
       "Accuracy for class BMA : [0.]\n",
-      "Accuracy for class BIN : [0.]\n",
-      "Overall Accuracy : 0.8966789667896679\n"
+      "Accuracy for class BIN : [0.33333333]\n",
+      "Overall Accuracy : 0.8831710709318498\n"
      ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "'[[126   9   0   0   0]\\n [  2 461  15   0   0]\\n [  0  29  66   0   0]\\n [  0   5   0   0   0]\\n [  0   0   0   0   6]]\\nAccuracy for class BRA : [0.93333333]\\nAccuracy for class BAM : [0.96443515]\\nAccuracy for class BBI : [0.69473684]\\nAccuracy for class BMA : [0.]\\nAccuracy for class BIN : [1.]\\nOverall Accuracy : 0.9165507649513213'"
+      ]
+     },
+     "execution_count": 28,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
@@ -556,12 +480,24 @@
     "    else : mean = \"N/A\"\n",
     "    print(\"Accuracy for class\", labels[class_i], \":\", mean)\n",
     "\n",
-    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))"
+    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))\n",
+    "\n",
+    "'''[[125  10   0   0   0]\n",
+    " [  3 430  45   0   0]\n",
+    " [  0  17  78   0   0]\n",
+    " [  0   5   0   0   0]\n",
+    " [  0   4   0   0   2]]\n",
+    "Accuracy for class BRA : [0.92592593]\n",
+    "Accuracy for class BAM : [0.89958159]\n",
+    "Accuracy for class BBI : [0.82105263]\n",
+    "Accuracy for class BMA : [0.]\n",
+    "Accuracy for class BIN : [0.33333333]\n",
+    "Overall Accuracy : 0.8831710709318498'''"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 21,
+   "execution_count": 29,
    "metadata": {},
    "outputs": [
     {
@@ -571,142 +507,142 @@
       "[0. 1.]\n",
       "[ 0.0000000e+00  1.5258789e-05  0.0000000e+00 ...  3.3020020e-02\n",
       "  1.2680054e-02 -8.7432861e-03]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000002\n",
+      " BRA :  0.00010891\n",
       "\n",
-      " BAM :  0.99999845\n",
+      " BAM :  0.99858701\n",
       "\n",
-      " BBI :  0.00000156\n",
+      " BBI :  0.00105622\n",
       "\n",
-      " BMA :  0.00000001\n",
+      " BMA :  0.00023835\n",
       "\n",
-      " BIN :  0.00000000\n",
+      " BIN :  0.00000958\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[1. 2.]\n",
       "[-0.03717041 -0.05769348 -0.06455994 ...  0.01766968  0.01895142\n",
       "  0.01779175]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000011\n",
+      " BRA :  0.00014382\n",
       "\n",
-      " BAM :  0.65600646\n",
+      " BAM :  0.85032731\n",
       "\n",
-      " BBI :  0.34398925\n",
+      " BBI :  0.14928448\n",
       "\n",
-      " BMA :  0.00000400\n",
+      " BMA :  0.00024211\n",
       "\n",
-      " BIN :  0.00000018\n",
+      " BIN :  0.00000233\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[2. 3.]\n",
       "[ 0.02345276  0.02101135  0.01712036 ... -0.01161194 -0.0141449\n",
       " -0.01431274]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000021\n",
+      " BRA :  0.00000749\n",
       "\n",
-      " BAM :  0.99949670\n",
+      " BAM :  0.99568462\n",
       "\n",
-      " BBI :  0.00050283\n",
+      " BBI :  0.00429150\n",
       "\n",
-      " BMA :  0.00000016\n",
+      " BMA :  0.00001622\n",
       "\n",
-      " BIN :  0.00000001\n",
+      " BIN :  0.00000025\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[3. 4.]\n",
       "[-0.01583862 -0.01066589 -0.00762939 ... -0.0377655  -0.03556824\n",
       " -0.02685547]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00034440\n",
+      " BRA :  0.00104451\n",
       "\n",
-      " BAM :  0.99944335\n",
+      " BAM :  0.99785441\n",
       "\n",
-      " BBI :  0.00012089\n",
+      " BBI :  0.00019401\n",
       "\n",
-      " BMA :  0.00007492\n",
+      " BMA :  0.00089407\n",
       "\n",
-      " BIN :  0.00001626\n",
+      " BIN :  0.00001295\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[4. 5.]\n",
       "[-0.02836609 -0.02510071 -0.02012634 ...  0.0138855  -0.00386047\n",
       " -0.00904846]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000000\n",
+      " BRA :  0.00064268\n",
       "\n",
-      " BAM :  1.00000000\n",
+      " BAM :  0.99867272\n",
       "\n",
-      " BBI :  0.00000000\n",
+      " BBI :  0.00047300\n",
       "\n",
-      " BMA :  0.00000000\n",
+      " BMA :  0.00020845\n",
       "\n",
-      " BIN :  0.00000000\n",
+      " BIN :  0.00000323\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[5. 6.]\n",
       "[-0.00526428  0.00822449  0.01951599 ...  0.02729797  0.02156067\n",
       "  0.01234436]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00185306\n",
+      " BRA :  0.00000077\n",
       "\n",
-      " BAM :  0.97584927\n",
+      " BAM :  0.99999666\n",
       "\n",
-      " BBI :  0.00809248\n",
+      " BBI :  0.00000220\n",
       "\n",
-      " BMA :  0.01294072\n",
+      " BMA :  0.00000035\n",
       "\n",
-      " BIN :  0.00126453\n",
+      " BIN :  0.00000011\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[6. 7.]\n",
       "[ 0.00544739  0.00053406  0.00970459 ... -0.02848816 -0.01611328\n",
       " -0.01091003]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000024\n",
+      " BRA :  0.00000281\n",
       "\n",
-      " BAM :  0.99988234\n",
+      " BAM :  0.99997854\n",
       "\n",
-      " BBI :  0.00011659\n",
+      " BBI :  0.00001035\n",
       "\n",
-      " BMA :  0.00000065\n",
+      " BMA :  0.00000824\n",
       "\n",
-      " BIN :  0.00000008\n",
+      " BIN :  0.00000003\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[7. 8.]\n",
       "[-0.0177002  -0.02372742 -0.02700806 ... -0.04304504 -0.04063416\n",
       " -0.03363037]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000038\n",
+      " BRA :  0.00000000\n",
       "\n",
-      " BAM :  0.99999917\n",
+      " BAM :  1.00000000\n",
       "\n",
-      " BBI :  0.00000042\n",
+      " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000002\n",
+      " BMA :  0.00000000\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -715,34 +651,34 @@
       "[8. 9.]\n",
       "[-0.01539612 -0.00108337  0.00718689 ...  0.01161194  0.01818848\n",
       "  0.02700806]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00001630\n",
+      " BRA :  0.00000011\n",
       "\n",
-      " BAM :  0.99997389\n",
+      " BAM :  0.99999988\n",
       "\n",
-      " BBI :  0.00000005\n",
+      " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000962\n",
+      " BMA :  0.00000001\n",
       "\n",
-      " BIN :  0.00000009\n",
+      " BIN :  0.00000000\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[ 9. 10.]\n",
       "[ 0.03549194  0.04856873  0.05519104 ... -0.02171326 -0.03634644\n",
       " -0.03912354]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000001\n",
+      " BRA :  0.00000004\n",
       "\n",
       " BAM :  1.00000000\n",
       "\n",
       " BBI :  0.00000001\n",
       "\n",
-      " BMA :  0.00000000\n",
+      " BMA :  0.00000005\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -751,122 +687,122 @@
       "[10. 11.]\n",
       "[-0.02934265 -0.0115509   0.00445557 ... -0.03616333 -0.03759766\n",
       " -0.0304718 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000013\n",
+      " BRA :  0.00000056\n",
       "\n",
-      " BAM :  0.99996102\n",
+      " BAM :  0.99999881\n",
       "\n",
-      " BBI :  0.00003862\n",
+      " BBI :  0.00000035\n",
       "\n",
-      " BMA :  0.00000027\n",
+      " BMA :  0.00000023\n",
       "\n",
-      " BIN :  0.00000001\n",
+      " BIN :  0.00000003\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[11. 12.]\n",
       "[-0.03358459 -0.03901672 -0.03933716 ... -0.02337646 -0.02124023\n",
       " -0.02107239]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000306\n",
+      " BRA :  0.00001431\n",
       "\n",
-      " BAM :  0.99999297\n",
+      " BAM :  0.99988461\n",
       "\n",
-      " BBI :  0.00000327\n",
+      " BBI :  0.00006046\n",
       "\n",
-      " BMA :  0.00000053\n",
+      " BMA :  0.00004009\n",
       "\n",
-      " BIN :  0.00000006\n",
+      " BIN :  0.00000051\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[12. 13.]\n",
       "[-0.00846863  0.00444031  0.00852966 ... -0.00604248 -0.00845337\n",
       " -0.00497437]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00555177\n",
+      " BRA :  0.00025713\n",
       "\n",
-      " BAM :  0.99345279\n",
+      " BAM :  0.98849314\n",
       "\n",
-      " BBI :  0.00021171\n",
+      " BBI :  0.01096764\n",
       "\n",
-      " BMA :  0.00052347\n",
+      " BMA :  0.00024913\n",
       "\n",
-      " BIN :  0.00026016\n",
+      " BIN :  0.00003295\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[13. 14.]\n",
       "[-0.00427246 -0.00718689 -0.00811768 ... -0.01966858 -0.01296997\n",
       " -0.01628113]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00228493\n",
+      " BRA :  0.00093799\n",
       "\n",
-      " BAM :  0.62795508\n",
+      " BAM :  0.99158257\n",
       "\n",
-      " BBI :  0.34584466\n",
+      " BBI :  0.00518909\n",
       "\n",
-      " BMA :  0.02155693\n",
+      " BMA :  0.00213042\n",
       "\n",
-      " BIN :  0.00235841\n",
+      " BIN :  0.00015992\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[14. 15.]\n",
       "[-0.02262878 -0.01573181 -0.00117493 ... -0.08956909 -0.0695343\n",
       " -0.04067993]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00019938\n",
+      " BRA :  0.00020277\n",
       "\n",
-      " BAM :  0.97783411\n",
+      " BAM :  0.99944395\n",
       "\n",
-      " BBI :  0.02189673\n",
+      " BBI :  0.00022603\n",
       "\n",
-      " BMA :  0.00006556\n",
+      " BMA :  0.00012033\n",
       "\n",
-      " BIN :  0.00000426\n",
+      " BIN :  0.00000688\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[15. 16.]\n",
       "[-0.02532959 -0.01031494 -0.00280762 ... -0.07128906 -0.07106018\n",
       " -0.05839539]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000299\n",
+      " BRA :  0.00001111\n",
       "\n",
-      " BAM :  0.99999702\n",
+      " BAM :  0.99998546\n",
       "\n",
-      " BBI :  0.00000001\n",
+      " BBI :  0.00000003\n",
       "\n",
-      " BMA :  0.00000001\n",
+      " BMA :  0.00000348\n",
       "\n",
-      " BIN :  0.00000000\n",
+      " BIN :  0.00000006\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[16. 17.]\n",
       "[-0.04600525 -0.02149963  0.00523376 ... -0.02526855 -0.02735901\n",
       " -0.03106689]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000888\n",
+      " BRA :  0.00000000\n",
       "\n",
-      " BAM :  0.99997985\n",
+      " BAM :  1.00000000\n",
       "\n",
-      " BBI :  0.00001121\n",
+      " BBI :  0.00000000\n",
       "\n",
       " BMA :  0.00000000\n",
       "\n",
@@ -877,28 +813,28 @@
       "[17. 18.]\n",
       "[-0.02043152 -0.01174927 -0.02088928 ...  0.10055542  0.08653259\n",
       "  0.06604004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000032\n",
+      " BRA :  0.00000001\n",
       "\n",
-      " BAM :  0.99999928\n",
+      " BAM :  1.00000000\n",
       "\n",
-      " BBI :  0.00000033\n",
+      " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000004\n",
+      " BMA :  0.00000001\n",
       "\n",
-      " BIN :  0.00000002\n",
+      " BIN :  0.00000000\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[18. 19.]\n",
       "[ 0.04153442  0.01223755 -0.00654602 ...  0.03269958  0.02374268\n",
       "  0.02774048]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000001\n",
+      " BRA :  0.00000000\n",
       "\n",
       " BAM :  1.00000000\n",
       "\n",
@@ -912,16 +848,16 @@
       "GUESS:  BAM\n",
       "[19. 20.]\n",
       "[0.02185059 0.02069092 0.01451111 ... 0.03469849 0.03985596 0.04600525]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00007026\n",
+      " BRA :  0.00000002\n",
       "\n",
-      " BAM :  0.99992979\n",
+      " BAM :  1.00000000\n",
       "\n",
       " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000006\n",
+      " BMA :  0.00000001\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -930,43 +866,43 @@
       "[20. 21.]\n",
       "[ 0.0353241   0.01567078 -0.00102234 ...  0.1058197   0.10365295\n",
       "  0.09759521]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000927\n",
+      " BRA :  0.00000130\n",
       "\n",
-      " BAM :  0.99992955\n",
+      " BAM :  0.99999797\n",
       "\n",
-      " BBI :  0.00005708\n",
+      " BBI :  0.00000012\n",
       "\n",
-      " BMA :  0.00000321\n",
+      " BMA :  0.00000049\n",
       "\n",
-      " BIN :  0.00000085\n",
+      " BIN :  0.00000013\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[21. 22.]\n",
       "[ 0.09413147  0.07905579  0.05625916 ... -0.01145935 -0.00245667\n",
       "  0.00479126]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00001921\n",
+      " BRA :  0.00000010\n",
       "\n",
-      " BAM :  0.99991000\n",
+      " BAM :  0.99999964\n",
       "\n",
-      " BBI :  0.00007006\n",
+      " BBI :  0.00000002\n",
       "\n",
-      " BMA :  0.00000078\n",
+      " BMA :  0.00000018\n",
       "\n",
-      " BIN :  0.00000001\n",
+      " BIN :  0.00000000\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[22. 23.]\n",
       "[ 0.0037384   0.01168823  0.01628113 ... -0.03440857 -0.05511475\n",
       " -0.08209229]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
       " BRA :  0.00000000\n",
@@ -984,7 +920,7 @@
       "[23. 24.]\n",
       "[-0.1026001  -0.12590027 -0.14944458 ...  0.03462219  0.02537537\n",
       "  0.02354431]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
       " BRA :  0.00000000\n",
@@ -1002,7 +938,7 @@
       "[24. 25.]\n",
       "[ 0.0196991   0.02836609  0.03103638 ... -0.03009033 -0.03392029\n",
       " -0.03681946]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
       " BRA :  0.00000000\n",
@@ -1020,69 +956,69 @@
       "[25. 26.]\n",
       "[-0.04151917 -0.03933716 -0.03703308 ...  0.05451965  0.0519104\n",
       "  0.05206299]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000080\n",
+      " BRA :  0.00000066\n",
       "\n",
-      " BAM :  0.99998569\n",
+      " BAM :  0.99999893\n",
       "\n",
-      " BBI :  0.00001264\n",
+      " BBI :  0.00000028\n",
       "\n",
-      " BMA :  0.00000066\n",
+      " BMA :  0.00000007\n",
       "\n",
-      " BIN :  0.00000009\n",
+      " BIN :  0.00000000\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[26. 27.]\n",
       "[ 0.05670166  0.06253052  0.07643127 ... -0.00396729  0.00715637\n",
       "  0.00585938]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.01487411\n",
+      " BRA :  0.00020419\n",
       "\n",
-      " BAM :  0.51351196\n",
+      " BAM :  0.99962115\n",
       "\n",
-      " BBI :  0.44204229\n",
+      " BBI :  0.00006228\n",
       "\n",
-      " BMA :  0.01676478\n",
+      " BMA :  0.00010320\n",
       "\n",
-      " BIN :  0.01280690\n",
+      " BIN :  0.00000904\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[27. 28.]\n",
       "[-0.00222778 -0.01303101 -0.02310181 ...  0.01165771  0.01649475\n",
       "  0.0194397 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000007\n",
+      " BRA :  0.00000249\n",
       "\n",
-      " BAM :  0.99999762\n",
+      " BAM :  0.97886640\n",
       "\n",
-      " BBI :  0.00000168\n",
+      " BBI :  0.02111143\n",
       "\n",
-      " BMA :  0.00000055\n",
+      " BMA :  0.00001957\n",
       "\n",
-      " BIN :  0.00000000\n",
+      " BIN :  0.00000016\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[28. 29.]\n",
       "[0.01657104 0.01519775 0.00924683 ... 0.03746033 0.03282166 0.02775574]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000000\n",
+      " BRA :  0.00000001\n",
       "\n",
-      " BAM :  1.00000000\n",
+      " BAM :  0.99999988\n",
       "\n",
       " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000000\n",
+      " BMA :  0.00000009\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -1091,61 +1027,61 @@
       "[29. 30.]\n",
       "[ 0.01919556  0.0135498   0.01724243 ... -0.00575256 -0.01502991\n",
       " -0.02742004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000002\n",
+      " BRA :  0.00000054\n",
       "\n",
-      " BAM :  1.00000000\n",
+      " BAM :  0.99999917\n",
       "\n",
-      " BBI :  0.00000004\n",
+      " BBI :  0.00000003\n",
       "\n",
-      " BMA :  0.00000000\n",
+      " BMA :  0.00000019\n",
       "\n",
-      " BIN :  0.00000000\n",
+      " BIN :  0.00000001\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[30. 31.]\n",
       "[-0.0322876  -0.0365448  -0.03544617 ... -0.0218811  -0.02978516\n",
       " -0.04052734]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00162166\n",
+      " BRA :  0.00000001\n",
       "\n",
-      " BAM :  0.99825948\n",
+      " BAM :  1.00000000\n",
       "\n",
-      " BBI :  0.00002736\n",
+      " BBI :  0.00000001\n",
       "\n",
-      " BMA :  0.00007135\n",
+      " BMA :  0.00000000\n",
       "\n",
-      " BIN :  0.00002006\n",
+      " BIN :  0.00000000\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[31. 32.]\n",
       "[-0.04328918 -0.03413391 -0.03421021 ...  0.05908203  0.06370544\n",
       "  0.05949402]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00003307\n",
+      " BRA :  0.00000008\n",
       "\n",
-      " BAM :  0.99972826\n",
+      " BAM :  0.99999702\n",
       "\n",
-      " BBI :  0.00022972\n",
+      " BBI :  0.00000195\n",
       "\n",
-      " BMA :  0.00000743\n",
+      " BMA :  0.00000086\n",
       "\n",
-      " BIN :  0.00000154\n",
+      " BIN :  0.00000002\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
       "[32. 33.]\n",
       "[ 0.06063843  0.06056213  0.06610107 ... -0.12741089 -0.13371277\n",
       " -0.12313843]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
       " BRA :  0.00000000\n",
@@ -1163,16 +1099,16 @@
       "[33. 34.]\n",
       "[-0.09968567 -0.06376648 -0.03105164 ... -0.0138092  -0.01574707\n",
       " -0.01896667]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000002\n",
+      " BRA :  0.00000003\n",
       "\n",
       " BAM :  1.00000000\n",
       "\n",
-      " BBI :  0.00000001\n",
+      " BBI :  0.00000000\n",
       "\n",
-      " BMA :  0.00000000\n",
+      " BMA :  0.00000003\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -1181,16 +1117,16 @@
       "[34. 35.]\n",
       "[-0.00811768  0.00149536  0.00953674 ... -0.004776   -0.0010376\n",
       "  0.00231934]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00000147\n",
+      " BRA :  0.00000002\n",
       "\n",
-      " BAM :  0.99999702\n",
+      " BAM :  0.99999988\n",
       "\n",
-      " BBI :  0.00000150\n",
+      " BBI :  0.00000005\n",
       "\n",
-      " BMA :  0.00000001\n",
+      " BMA :  0.00000007\n",
       "\n",
       " BIN :  0.00000000\n",
       "\n",
@@ -1199,16 +1135,12 @@
       "[35. 36.]\n",
       "[ 0.00238037  0.00236511  0.00231934 ... -0.00193787  0.0068512\n",
       "  0.00695801]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " BRA :  0.00021959\n",
-      "\n",
-      " BAM :  0.99587798\n",
+      " BRA :  0.00005464\n",
       "\n",
-      " BBI :  0.00373552\n",
-      "\n",
-      " BMA :  0.00012462\n",
+      " BAM :  0.99956018\n",
       "\n"
      ]
     },
@@ -1216,7 +1148,11 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      " BIN :  0.00004231\n",
+      " BBI :  0.00021886\n",
+      "\n",
+      " BMA :  0.00016426\n",
+      "\n",
+      " BIN :  0.00000198\n",
       "\n",
       "\n",
       "GUESS:  BAM\n",
diff --git a/bio-cnn/bio_cnn_model.h5 b/bio-cnn/bio_cnn_model.h5
index 8de4188..bfdb4db 100644
Binary files a/bio-cnn/bio_cnn_model.h5 and b/bio-cnn/bio_cnn_model.h5 differ
diff --git a/bio_cnn_model.h5 b/bio_cnn_model.h5
index 8de4188..bfdb4db 100644
Binary files a/bio_cnn_model.h5 and b/bio_cnn_model.h5 differ
diff --git a/geo-cnn/geo-cnn.ipynb b/geo-cnn/geo-cnn.ipynb
index 73ecfea..8b53884 100644
--- a/geo-cnn/geo-cnn.ipynb
+++ b/geo-cnn/geo-cnn.ipynb
@@ -2,17 +2,9 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 15,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "Using TensorFlow backend.\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "from preprocess import *\n",
     "import keras\n",
@@ -27,7 +19,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 16,
    "metadata": {},
    "outputs": [
     {
@@ -36,7 +28,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/1p0xwnhq\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/1p0xwnhq</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/15ugcco4\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/15ugcco4</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -50,11 +42,11 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "Saving vectors of label - 'GOC': 100%|█████████████████████████████████████████████████| 15/15 [00:00<00:00, 28.56it/s]\n",
-      "Saving vectors of label - 'GRA': 100%|███████████████████████████████████████████████| 133/133 [00:03<00:00, 40.92it/s]\n",
-      "Saving vectors of label - 'GST': 100%|█████████████████████████████████████████████████| 20/20 [00:00<00:00, 76.07it/s]\n",
-      "Saving vectors of label - 'GWC': 100%|███████████████████████████████████████████████| 100/100 [00:01<00:00, 59.24it/s]\n",
-      "Saving vectors of label - 'GWG': 100%|█████████████████████████████████████████████████| 52/52 [00:01<00:00, 37.26it/s]\n"
+      "Saving vectors of label - 'GOC': 100%|█████████████████████████████████████████████████| 91/91 [00:01<00:00, 68.76it/s]\n",
+      "Saving vectors of label - 'GRA': 100%|███████████████████████████████████████████████| 623/623 [00:06<00:00, 93.69it/s]\n",
+      "Saving vectors of label - 'GST': 100%|█████████████████████████████████████████████████| 85/85 [00:01<00:00, 80.33it/s]\n",
+      "Saving vectors of label - 'GWC': 100%|███████████████████████████████████████████████| 464/464 [00:05<00:00, 78.34it/s]\n",
+      "Saving vectors of label - 'GWG': 100%|██████████████████████████████████████████████| 203/203 [00:02<00:00, 101.32it/s]\n"
      ]
     }
    ],
@@ -62,8 +54,8 @@
     "wandb.init()\n",
     "config = wandb.config\n",
     "\n",
-    "config.max_len = 21\n",
-    "config.buckets = 50\n",
+    "config.max_len = 32\n",
+    "config.buckets = 128\n",
     "\n",
     "# Save data to array file first\n",
     "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
@@ -76,7 +68,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 17,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -86,18 +78,26 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 18,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(703, 128, 32)\n"
+     ]
+    }
+   ],
    "source": [
     "# Setting channels to 1 to generalize stereo sound to 1 channel\n",
     "channels = 1\n",
-    "config.epochs = 50\n",
+    "config.epochs = 20\n",
     "config.batch_size = 100\n",
     "\n",
     "# Number of classes\n",
     "num_classes = 5\n",
-    "\n",
+    "print(X_train.shape)\n",
     "# Reshape X_train and X_test to include a 4th dimension (channels)\n",
     "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len, channels)\n",
     "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len, channels)\n",
@@ -106,26 +106,48 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 19,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(153, 50, 21, 1)\n"
+      "(703, 128, 32, 1)\n"
      ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<matplotlib.image.AxesImage at 0x1dd77bd2b48>"
+      ]
+     },
+     "execution_count": 19,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
     }
    ],
    "source": [
     "# Spectrogram visualized of 0th element\n",
     "print(X_train.shape)\n",
-    "#plt.imshow(X_train[500, :, :, 0])"
+    "plt.imshow(X_train[10, :, :, 0])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 20,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -137,51 +159,49 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
-      "\n",
-      "Model: \"sequential_1\"\n",
+      "Model: \"sequential_2\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_1 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_1 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "activation_1 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_2 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_2 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_2 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_3 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_3 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_1 (Flatten)          (None, 1296)              0         \n",
+      "flatten_2 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_1 (Dropout)          (None, 1296)              0         \n",
+      "dropout_3 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_1 (Dense)              (None, 64)                83008     \n",
+      "dense_3 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_4 (Activation)    (None, 64)                0         \n",
+      "activation_9 (Activation)    (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dropout_2 (Dropout)          (None, 64)                0         \n",
+      "dropout_4 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 5)                 325       \n",
+      "dense_4 (Dense)              (None, 5)                 325       \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 5)                 0         \n",
+      "activation_10 (Activation)   (None, 5)                 0         \n",
       "=================================================================\n",
-      "Total params: 100,949\n",
-      "Trainable params: 100,949\n",
+      "Total params: 534,101\n",
+      "Trainable params: 534,101\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -192,7 +212,7 @@
        "\"model.add(Conv2D(32, (3, 3),\\n    input_shape=(config.buckets, config.max_len, channels),\\n    activation='relu'))\\n\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\""
       ]
      },
-     "execution_count": 7,
+     "execution_count": 21,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -241,7 +261,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 22,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -253,7 +273,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 23,
    "metadata": {
     "scrolled": false
    },
@@ -264,7 +284,7 @@
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2vo4ed9m\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2vo4ed9m</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2fhx4pil\" target=\"_blank\">https://app.wandb.ai/joshekruse/intellichirp-snaw-NN/runs/2fhx4pil</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -278,127 +298,71 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "(153, 5)\n",
+      "(703, 5)\n",
       "(5,)\n",
-      "(153, 50, 21, 1)\n",
-      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
-      "\n",
-      "Train on 153 samples, validate on 103 samples\n",
-      "Epoch 1/50\n",
-      "153/153 [==============================] - ETA: 4s - loss: 6.3866 - accuracy: 0.25 - ETA: 1s - loss: 5.2013 - accuracy: 0.32 - ETA: 0s - loss: 4.6251 - accuracy: 0.32 - 1s 10ms/step - loss: 3.9702 - accuracy: 0.3137 - val_loss: 1.6140 - val_accuracy: 0.4466\n",
-      "Epoch 2/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 2.3227 - accuracy: 0.37 - ETA: 0s - loss: 2.1149 - accuracy: 0.35 - ETA: 0s - loss: 2.0037 - accuracy: 0.36 - 0s 2ms/step - loss: 2.0069 - accuracy: 0.3529 - val_loss: 1.3922 - val_accuracy: 0.4078\n",
-      "Epoch 3/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.5084 - accuracy: 0.43 - ETA: 0s - loss: 1.4560 - accuracy: 0.45 - ETA: 0s - loss: 1.4355 - accuracy: 0.46 - 0s 2ms/step - loss: 1.4195 - accuracy: 0.4837 - val_loss: 1.3732 - val_accuracy: 0.4078\n",
-      "Epoch 4/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.4537 - accuracy: 0.43 - ETA: 0s - loss: 1.3506 - accuracy: 0.43 - ETA: 0s - loss: 1.4725 - accuracy: 0.39 - 0s 2ms/step - loss: 1.4542 - accuracy: 0.4118 - val_loss: 1.3443 - val_accuracy: 0.4078\n",
-      "Epoch 5/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.3869 - accuracy: 0.40 - ETA: 0s - loss: 1.4816 - accuracy: 0.40 - ETA: 0s - loss: 1.4119 - accuracy: 0.42 - 0s 2ms/step - loss: 1.4027 - accuracy: 0.4314 - val_loss: 1.3292 - val_accuracy: 0.4757\n",
-      "Epoch 6/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.3862 - accuracy: 0.46 - ETA: 0s - loss: 1.3544 - accuracy: 0.46 - ETA: 0s - loss: 1.3542 - accuracy: 0.47 - 0s 2ms/step - loss: 1.3211 - accuracy: 0.4706 - val_loss: 1.3037 - val_accuracy: 0.4466\n",
-      "Epoch 7/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.3200 - accuracy: 0.34 - ETA: 0s - loss: 1.3224 - accuracy: 0.42 - 0s 1ms/step - loss: 1.2395 - accuracy: 0.4837 - val_loss: 1.2936 - val_accuracy: 0.4369\n",
-      "Epoch 8/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.3623 - accuracy: 0.50 - ETA: 0s - loss: 1.3141 - accuracy: 0.46 - ETA: 0s - loss: 1.2266 - accuracy: 0.51 - 0s 1ms/step - loss: 1.2209 - accuracy: 0.5163 - val_loss: 1.2187 - val_accuracy: 0.5146\n",
-      "Epoch 9/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.1934 - accuracy: 0.34 - ETA: 0s - loss: 1.1643 - accuracy: 0.39 - 0s 1ms/step - loss: 1.1357 - accuracy: 0.4575 - val_loss: 1.1976 - val_accuracy: 0.5049\n",
-      "Epoch 10/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.1130 - accuracy: 0.56 - ETA: 0s - loss: 1.1591 - accuracy: 0.54 - ETA: 0s - loss: 1.1097 - accuracy: 0.58 - 0s 2ms/step - loss: 1.0830 - accuracy: 0.5882 - val_loss: 1.1705 - val_accuracy: 0.5340\n",
-      "Epoch 11/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.2887 - accuracy: 0.37 - ETA: 0s - loss: 1.0803 - accuracy: 0.45 - ETA: 0s - loss: 1.1370 - accuracy: 0.45 - 0s 1ms/step - loss: 1.1050 - accuracy: 0.5033 - val_loss: 1.1343 - val_accuracy: 0.5728\n",
-      "Epoch 12/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.2405 - accuracy: 0.40 - ETA: 0s - loss: 1.1210 - accuracy: 0.46 - ETA: 0s - loss: 1.1123 - accuracy: 0.50 - ETA: 0s - loss: 1.0951 - accuracy: 0.50 - 0s 2ms/step - loss: 1.0832 - accuracy: 0.5294 - val_loss: 1.0968 - val_accuracy: 0.5922\n",
-      "Epoch 13/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.65 - ETA: 0s - loss: 0.9774 - accuracy: 0.63 - ETA: 0s - loss: 1.0336 - accuracy: 0.59 - 0s 2ms/step - loss: 1.0565 - accuracy: 0.5817 - val_loss: 1.1216 - val_accuracy: 0.4854\n",
-      "Epoch 14/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.2312 - accuracy: 0.59 - ETA: 0s - loss: 1.0978 - accuracy: 0.54 - ETA: 0s - loss: 1.0304 - accuracy: 0.57 - 0s 2ms/step - loss: 1.0210 - accuracy: 0.5882 - val_loss: 1.0382 - val_accuracy: 0.6214\n",
-      "Epoch 15/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.8759 - accuracy: 0.59 - ETA: 0s - loss: 0.9756 - accuracy: 0.57 - ETA: 0s - loss: 0.9771 - accuracy: 0.58 - 0s 2ms/step - loss: 0.9759 - accuracy: 0.5817 - val_loss: 0.9818 - val_accuracy: 0.6602\n",
-      "Epoch 16/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.9569 - accuracy: 0.59 - ETA: 0s - loss: 0.8810 - accuracy: 0.62 - 0s 1ms/step - loss: 0.8879 - accuracy: 0.6209 - val_loss: 0.9492 - val_accuracy: 0.6214\n",
-      "Epoch 17/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 1.0282 - accuracy: 0.62 - ETA: 0s - loss: 0.9504 - accuracy: 0.60 - ETA: 0s - loss: 0.9344 - accuracy: 0.61 - 0s 2ms/step - loss: 0.8968 - accuracy: 0.6536 - val_loss: 0.9202 - val_accuracy: 0.6311\n",
-      "Epoch 18/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.9575 - accuracy: 0.59 - ETA: 0s - loss: 0.9697 - accuracy: 0.60 - ETA: 0s - loss: 0.9030 - accuracy: 0.65 - 0s 2ms/step - loss: 0.8963 - accuracy: 0.6405 - val_loss: 0.9100 - val_accuracy: 0.6408\n",
-      "Epoch 19/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.75 - ETA: 0s - loss: 0.7136 - accuracy: 0.71 - ETA: 0s - loss: 0.7317 - accuracy: 0.68 - ETA: 0s - loss: 0.7683 - accuracy: 0.67 - 0s 2ms/step - loss: 0.8042 - accuracy: 0.6405 - val_loss: 0.9191 - val_accuracy: 0.6408\n",
-      "Epoch 20/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.65 - ETA: 0s - loss: 0.8270 - accuracy: 0.66 - ETA: 0s - loss: 0.8451 - accuracy: 0.63 - 0s 2ms/step - loss: 0.8113 - accuracy: 0.6536 - val_loss: 0.8860 - val_accuracy: 0.6699\n",
-      "Epoch 21/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.9631 - accuracy: 0.62 - ETA: 0s - loss: 0.8104 - accuracy: 0.66 - ETA: 0s - loss: 0.7721 - accuracy: 0.67 - 0s 2ms/step - loss: 0.8146 - accuracy: 0.6536 - val_loss: 0.8761 - val_accuracy: 0.6214\n",
-      "Epoch 22/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.71 - ETA: 0s - loss: 0.6684 - accuracy: 0.67 - ETA: 0s - loss: 0.7393 - accuracy: 0.68 - 0s 1ms/step - loss: 0.7830 - accuracy: 0.6601 - val_loss: 0.7972 - val_accuracy: 0.6796\n",
-      "Epoch 23/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.71 - ETA: 0s - loss: 0.7172 - accuracy: 0.70 - 0s 2ms/step - loss: 0.6911 - accuracy: 0.7255 - val_loss: 0.9297 - val_accuracy: 0.5922\n",
-      "Epoch 24/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.65 - ETA: 0s - loss: 0.7964 - accuracy: 0.62 - ETA: 0s - loss: 0.7546 - accuracy: 0.65 - ETA: 0s - loss: 0.7473 - accuracy: 0.67 - 0s 2ms/step - loss: 0.7155 - accuracy: 0.7124 - val_loss: 0.8498 - val_accuracy: 0.6796\n",
-      "Epoch 25/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.8153 - accuracy: 0.75 - ETA: 0s - loss: 0.7639 - accuracy: 0.73 - ETA: 0s - loss: 0.6701 - accuracy: 0.77 - ETA: 0s - loss: 0.6613 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6630 - accuracy: 0.7712 - val_loss: 0.9295 - val_accuracy: 0.5922\n",
-      "Epoch 26/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.68 - ETA: 0s - loss: 0.6413 - accuracy: 0.69 - ETA: 0s - loss: 0.6783 - accuracy: 0.68 - 0s 2ms/step - loss: 0.6419 - accuracy: 0.7190 - val_loss: 0.8407 - val_accuracy: 0.6602\n",
-      "Epoch 27/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.75 - ETA: 0s - loss: 0.6384 - accuracy: 0.71 - ETA: 0s - loss: 0.6218 - accuracy: 0.72 - ETA: 0s - loss: 0.5947 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6059 - accuracy: 0.7516 - val_loss: 0.8170 - val_accuracy: 0.6990\n",
-      "Epoch 28/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.68 - ETA: 0s - loss: 0.6670 - accuracy: 0.71 - ETA: 0s - loss: 0.6598 - accuracy: 0.71 - ETA: 0s - loss: 0.6217 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6362 - accuracy: 0.7451 - val_loss: 0.8114 - val_accuracy: 0.6990\n",
-      "Epoch 29/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.87 - ETA: 0s - loss: 0.5034 - accuracy: 0.81 - ETA: 0s - loss: 0.5310 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5846 - accuracy: 0.7974 - val_loss: 0.8053 - val_accuracy: 0.6990\n",
-      "Epoch 30/50\n"
+      "(703, 128, 32, 1)\n",
+      "Train on 703 samples, validate on 469 samples\n",
+      "Epoch 1/20\n",
+      "703/703 [==============================] - ETA: 31s - loss: 2.6549 - accuracy: 0.156 - ETA: 15s - loss: 2.9842 - accuracy: 0.296 - ETA: 10s - loss: 3.0744 - accuracy: 0.270 - ETA: 8s - loss: 2.7303 - accuracy: 0.273 - ETA: 6s - loss: 2.4843 - accuracy: 0.30 - ETA: 5s - loss: 2.3492 - accuracy: 0.29 - ETA: 4s - loss: 2.2642 - accuracy: 0.29 - ETA: 4s - loss: 2.1689 - accuracy: 0.30 - ETA: 3s - loss: 2.1129 - accuracy: 0.29 - ETA: 3s - loss: 2.0378 - accuracy: 0.31 - ETA: 2s - loss: 1.9852 - accuracy: 0.31 - ETA: 2s - loss: 1.9340 - accuracy: 0.33 - ETA: 2s - loss: 1.9016 - accuracy: 0.33 - ETA: 1s - loss: 1.8562 - accuracy: 0.34 - ETA: 1s - loss: 1.8084 - accuracy: 0.36 - ETA: 1s - loss: 1.7684 - accuracy: 0.36 - ETA: 1s - loss: 1.7591 - accuracy: 0.37 - ETA: 0s - loss: 1.7334 - accuracy: 0.37 - ETA: 0s - loss: 1.7094 - accuracy: 0.38 - ETA: 0s - loss: 1.6844 - accuracy: 0.39 - ETA: 0s - loss: 1.6595 - accuracy: 0.40 - 5s 7ms/step - loss: 1.6474 - accuracy: 0.4040 - val_loss: 1.1659 - val_accuracy: 0.5714\n",
+      "Epoch 2/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 1.1689 - accuracy: 0.53 - ETA: 1s - loss: 1.1853 - accuracy: 0.53 - ETA: 1s - loss: 1.2146 - accuracy: 0.53 - ETA: 1s - loss: 1.1864 - accuracy: 0.54 - ETA: 1s - loss: 1.1878 - accuracy: 0.53 - ETA: 1s - loss: 1.1766 - accuracy: 0.54 - ETA: 1s - loss: 1.1397 - accuracy: 0.54 - ETA: 1s - loss: 1.0926 - accuracy: 0.57 - ETA: 1s - loss: 1.1273 - accuracy: 0.55 - ETA: 1s - loss: 1.1230 - accuracy: 0.56 - ETA: 1s - loss: 1.1209 - accuracy: 0.55 - ETA: 1s - loss: 1.1409 - accuracy: 0.54 - ETA: 1s - loss: 1.1405 - accuracy: 0.55 - ETA: 0s - loss: 1.1359 - accuracy: 0.54 - ETA: 0s - loss: 1.1361 - accuracy: 0.54 - ETA: 0s - loss: 1.1366 - accuracy: 0.53 - ETA: 0s - loss: 1.1285 - accuracy: 0.54 - ETA: 0s - loss: 1.1155 - accuracy: 0.54 - ETA: 0s - loss: 1.1064 - accuracy: 0.54 - ETA: 0s - loss: 1.1103 - accuracy: 0.54 - ETA: 0s - loss: 1.1088 - accuracy: 0.54 - 3s 4ms/step - loss: 1.1050 - accuracy: 0.5505 - val_loss: 0.8259 - val_accuracy: 0.6951\n",
+      "Epoch 3/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 1.1657 - accuracy: 0.62 - ETA: 1s - loss: 1.0297 - accuracy: 0.65 - ETA: 2s - loss: 0.9726 - accuracy: 0.63 - ETA: 2s - loss: 0.9639 - accuracy: 0.64 - ETA: 2s - loss: 0.8999 - accuracy: 0.67 - ETA: 1s - loss: 0.8855 - accuracy: 0.69 - ETA: 1s - loss: 0.8684 - accuracy: 0.69 - ETA: 1s - loss: 0.8930 - accuracy: 0.68 - ETA: 1s - loss: 0.8792 - accuracy: 0.69 - ETA: 1s - loss: 0.8761 - accuracy: 0.68 - ETA: 1s - loss: 0.9006 - accuracy: 0.66 - ETA: 1s - loss: 0.9117 - accuracy: 0.65 - ETA: 1s - loss: 0.9005 - accuracy: 0.65 - ETA: 0s - loss: 0.9088 - accuracy: 0.65 - ETA: 0s - loss: 0.9208 - accuracy: 0.64 - ETA: 0s - loss: 0.9158 - accuracy: 0.63 - ETA: 0s - loss: 0.9094 - accuracy: 0.64 - ETA: 0s - loss: 0.9108 - accuracy: 0.63 - ETA: 0s - loss: 0.9111 - accuracy: 0.63 - ETA: 0s - loss: 0.8972 - accuracy: 0.64 - ETA: 0s - loss: 0.8848 - accuracy: 0.65 - 3s 4ms/step - loss: 0.8768 - accuracy: 0.6615 - val_loss: 0.6931 - val_accuracy: 0.7207\n",
+      "Epoch 4/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.7302 - accuracy: 0.71 - ETA: 2s - loss: 0.8262 - accuracy: 0.62 - ETA: 2s - loss: 0.7754 - accuracy: 0.66 - ETA: 2s - loss: 0.8438 - accuracy: 0.63 - ETA: 2s - loss: 0.8224 - accuracy: 0.64 - ETA: 2s - loss: 0.8558 - accuracy: 0.63 - ETA: 1s - loss: 0.8644 - accuracy: 0.63 - ETA: 1s - loss: 0.8531 - accuracy: 0.63 - ETA: 1s - loss: 0.8553 - accuracy: 0.64 - ETA: 1s - loss: 0.8430 - accuracy: 0.65 - ETA: 1s - loss: 0.8547 - accuracy: 0.65 - ETA: 1s - loss: 0.8600 - accuracy: 0.66 - ETA: 1s - loss: 0.8458 - accuracy: 0.66 - ETA: 1s - loss: 0.8280 - accuracy: 0.66 - ETA: 0s - loss: 0.8251 - accuracy: 0.67 - ETA: 0s - loss: 0.8243 - accuracy: 0.66 - ETA: 0s - loss: 0.8126 - accuracy: 0.67 - ETA: 0s - loss: 0.8111 - accuracy: 0.67 - ETA: 0s - loss: 0.8034 - accuracy: 0.67 - ETA: 0s - loss: 0.8006 - accuracy: 0.67 - ETA: 0s - loss: 0.7956 - accuracy: 0.68 - 3s 4ms/step - loss: 0.7886 - accuracy: 0.6828 - val_loss: 0.6377 - val_accuracy: 0.7527\n",
+      "Epoch 5/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.7483 - accuracy: 0.65 - ETA: 2s - loss: 0.7772 - accuracy: 0.67 - ETA: 2s - loss: 0.7717 - accuracy: 0.67 - ETA: 2s - loss: 0.7714 - accuracy: 0.65 - ETA: 2s - loss: 0.7747 - accuracy: 0.66 - ETA: 2s - loss: 0.7828 - accuracy: 0.66 - ETA: 1s - loss: 0.7560 - accuracy: 0.67 - ETA: 1s - loss: 0.7632 - accuracy: 0.67 - ETA: 1s - loss: 0.7435 - accuracy: 0.67 - ETA: 1s - loss: 0.7254 - accuracy: 0.68 - ETA: 1s - loss: 0.7383 - accuracy: 0.68 - ETA: 1s - loss: 0.7453 - accuracy: 0.68 - ETA: 1s - loss: 0.7375 - accuracy: 0.68 - ETA: 0s - loss: 0.7438 - accuracy: 0.68 - ETA: 0s - loss: 0.7291 - accuracy: 0.68 - ETA: 0s - loss: 0.7334 - accuracy: 0.68 - ETA: 0s - loss: 0.7219 - accuracy: 0.69 - ETA: 0s - loss: 0.7123 - accuracy: 0.69 - ETA: 0s - loss: 0.7190 - accuracy: 0.68 - ETA: 0s - loss: 0.7165 - accuracy: 0.69 - ETA: 0s - loss: 0.7038 - accuracy: 0.70 - 3s 4ms/step - loss: 0.6996 - accuracy: 0.7070 - val_loss: 0.5041 - val_accuracy: 0.8081\n",
+      "Epoch 6/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.4186 - accuracy: 0.84 - ETA: 1s - loss: 0.4938 - accuracy: 0.82 - ETA: 1s - loss: 0.5886 - accuracy: 0.79 - ETA: 1s - loss: 0.5768 - accuracy: 0.78 - ETA: 1s - loss: 0.5932 - accuracy: 0.78 - ETA: 1s - loss: 0.6154 - accuracy: 0.76 - ETA: 1s - loss: 0.5818 - accuracy: 0.78 - ETA: 1s - loss: 0.5586 - accuracy: 0.79 - ETA: 1s - loss: 0.5578 - accuracy: 0.79 - ETA: 1s - loss: 0.5580 - accuracy: 0.78 - ETA: 1s - loss: 0.5580 - accuracy: 0.79 - ETA: 1s - loss: 0.5687 - accuracy: 0.78 - ETA: 0s - loss: 0.5542 - accuracy: 0.78 - ETA: 0s - loss: 0.5590 - accuracy: 0.78 - ETA: 0s - loss: 0.5549 - accuracy: 0.78 - ETA: 0s - loss: 0.5587 - accuracy: 0.78 - ETA: 0s - loss: 0.5531 - accuracy: 0.79 - ETA: 0s - loss: 0.5718 - accuracy: 0.78 - ETA: 0s - loss: 0.5740 - accuracy: 0.78 - ETA: 0s - loss: 0.5730 - accuracy: 0.78 - ETA: 0s - loss: 0.5701 - accuracy: 0.78 - 3s 4ms/step - loss: 0.5677 - accuracy: 0.7852 - val_loss: 0.4942 - val_accuracy: 0.7953\n",
+      "Epoch 7/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.5380 - accuracy: 0.78 - ETA: 2s - loss: 0.5388 - accuracy: 0.76 - ETA: 2s - loss: 0.5438 - accuracy: 0.77 - ETA: 2s - loss: 0.5308 - accuracy: 0.78 - ETA: 2s - loss: 0.5148 - accuracy: 0.76 - ETA: 1s - loss: 0.5688 - accuracy: 0.75 - ETA: 1s - loss: 0.5506 - accuracy: 0.75 - ETA: 1s - loss: 0.5409 - accuracy: 0.76 - ETA: 1s - loss: 0.5528 - accuracy: 0.76 - ETA: 1s - loss: 0.5465 - accuracy: 0.76 - ETA: 1s - loss: 0.5389 - accuracy: 0.77 - ETA: 1s - loss: 0.5316 - accuracy: 0.77 - ETA: 1s - loss: 0.5243 - accuracy: 0.78 - ETA: 0s - loss: 0.5203 - accuracy: 0.78 - ETA: 0s - loss: 0.5279 - accuracy: 0.78 - ETA: 0s - loss: 0.5226 - accuracy: 0.78 - ETA: 0s - loss: 0.5156 - accuracy: 0.78 - ETA: 0s - loss: 0.5114 - accuracy: 0.78 - ETA: 0s - loss: 0.5075 - accuracy: 0.79 - ETA: 0s - loss: 0.5139 - accuracy: 0.79 - ETA: 0s - loss: 0.5146 - accuracy: 0.79 - 3s 4ms/step - loss: 0.5199 - accuracy: 0.7923 - val_loss: 0.4109 - val_accuracy: 0.8486\n",
+      "Epoch 8/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.6302 - accuracy: 0.68 - ETA: 1s - loss: 0.5093 - accuracy: 0.76 - ETA: 1s - loss: 0.4595 - accuracy: 0.79 - ETA: 1s - loss: 0.4537 - accuracy: 0.80 - ETA: 1s - loss: 0.5101 - accuracy: 0.78 - ETA: 1s - loss: 0.5269 - accuracy: 0.77 - ETA: 1s - loss: 0.5585 - accuracy: 0.76 - ETA: 1s - loss: 0.5607 - accuracy: 0.75 - ETA: 1s - loss: 0.5315 - accuracy: 0.76 - ETA: 1s - loss: 0.5156 - accuracy: 0.77 - ETA: 1s - loss: 0.5094 - accuracy: 0.77 - ETA: 1s - loss: 0.5043 - accuracy: 0.77 - ETA: 0s - loss: 0.5121 - accuracy: 0.78 - ETA: 0s - loss: 0.5207 - accuracy: 0.77 - ETA: 0s - loss: 0.5084 - accuracy: 0.78 - ETA: 0s - loss: 0.5173 - accuracy: 0.78 - ETA: 0s - loss: 0.5047 - accuracy: 0.78 - ETA: 0s - loss: 0.4943 - accuracy: 0.79 - ETA: 0s - loss: 0.5012 - accuracy: 0.79 - ETA: 0s - loss: 0.5003 - accuracy: 0.79 - ETA: 0s - loss: 0.4982 - accuracy: 0.79 - 3s 4ms/step - loss: 0.4998 - accuracy: 0.7937 - val_loss: 0.4046 - val_accuracy: 0.8230\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "153/153 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.87 - ETA: 0s - loss: 0.6072 - accuracy: 0.79 - ETA: 0s - loss: 0.5735 - accuracy: 0.81 - ETA: 0s - loss: 0.5855 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6218 - accuracy: 0.7843 - val_loss: 0.8452 - val_accuracy: 0.6214\n",
-      "Epoch 31/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.84 - ETA: 0s - loss: 0.4928 - accuracy: 0.82 - ETA: 0s - loss: 0.4840 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4901 - accuracy: 0.8301 - val_loss: 0.8403 - val_accuracy: 0.6505\n",
-      "Epoch 32/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.5543 - accuracy: 0.78 - ETA: 0s - loss: 0.4730 - accuracy: 0.81 - ETA: 0s - loss: 0.5190 - accuracy: 0.79 - ETA: 0s - loss: 0.5050 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7582 - val_loss: 0.8160 - val_accuracy: 0.6505\n",
-      "Epoch 33/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.87 - ETA: 0s - loss: 0.5673 - accuracy: 0.79 - ETA: 0s - loss: 0.5003 - accuracy: 0.82 - ETA: 0s - loss: 0.4495 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4471 - accuracy: 0.8235 - val_loss: 0.8013 - val_accuracy: 0.6699\n",
-      "Epoch 34/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.87 - ETA: 0s - loss: 0.3633 - accuracy: 0.89 - ETA: 0s - loss: 0.4177 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8170 - val_loss: 0.7904 - val_accuracy: 0.6893\n",
-      "Epoch 35/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.78 - ETA: 0s - loss: 0.4697 - accuracy: 0.76 - ETA: 0s - loss: 0.4518 - accuracy: 0.78 - ETA: 0s - loss: 0.4722 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7974 - val_loss: 0.9129 - val_accuracy: 0.6311\n",
-      "Epoch 36/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.78 - ETA: 0s - loss: 0.5228 - accuracy: 0.79 - ETA: 0s - loss: 0.4156 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8562 - val_loss: 0.7916 - val_accuracy: 0.6893\n",
-      "Epoch 37/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.81 - ETA: 0s - loss: 0.4407 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8235 - val_loss: 0.8457 - val_accuracy: 0.6505\n",
-      "Epoch 38/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.81 - ETA: 0s - loss: 0.2833 - accuracy: 0.89 - ETA: 0s - loss: 0.3223 - accuracy: 0.86 - ETA: 0s - loss: 0.3587 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8693 - val_loss: 0.8708 - val_accuracy: 0.6505\n",
-      "Epoch 39/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.90 - ETA: 0s - loss: 0.3264 - accuracy: 0.87 - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8562 - val_loss: 0.8935 - val_accuracy: 0.6311\n",
-      "Epoch 40/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.93 - ETA: 0s - loss: 0.3152 - accuracy: 0.89 - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8954 - val_loss: 0.8991 - val_accuracy: 0.6019\n",
-      "Epoch 41/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.90 - ETA: 0s - loss: 0.4166 - accuracy: 0.85 - ETA: 0s - loss: 0.4115 - accuracy: 0.83 - ETA: 0s - loss: 0.3855 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8889 - val_loss: 0.9376 - val_accuracy: 0.6311\n",
-      "Epoch 42/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.96 - ETA: 0s - loss: 0.2204 - accuracy: 0.96 - ETA: 0s - loss: 0.2729 - accuracy: 0.93 - ETA: 0s - loss: 0.2830 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2918 - accuracy: 0.9346 - val_loss: 0.9197 - val_accuracy: 0.6505\n",
-      "Epoch 43/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.90 - ETA: 0s - loss: 0.2709 - accuracy: 0.93 - ETA: 0s - loss: 0.3022 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3081 - accuracy: 0.9020 - val_loss: 0.9597 - val_accuracy: 0.6019\n",
-      "Epoch 44/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.84 - ETA: 0s - loss: 0.2597 - accuracy: 0.90 - 0s 1ms/step - loss: 0.2773 - accuracy: 0.8954 - val_loss: 1.1554 - val_accuracy: 0.6214\n",
-      "Epoch 45/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3322 - accuracy: 0.87 - ETA: 0s - loss: 0.2935 - accuracy: 0.86 - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9020 - val_loss: 0.9416 - val_accuracy: 0.6408\n",
-      "Epoch 46/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.87 - ETA: 0s - loss: 0.2180 - accuracy: 0.93 - ETA: 0s - loss: 0.2860 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2890 - accuracy: 0.8954 - val_loss: 0.9346 - val_accuracy: 0.6214\n",
-      "Epoch 47/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.96 - ETA: 0s - loss: 0.1778 - accuracy: 0.96 - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9739 - val_loss: 0.9422 - val_accuracy: 0.6311\n",
-      "Epoch 48/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.3781 - accuracy: 0.87 - ETA: 0s - loss: 0.2801 - accuracy: 0.92 - ETA: 0s - loss: 0.2451 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2557 - accuracy: 0.9281 - val_loss: 0.8760 - val_accuracy: 0.6990\n",
-      "Epoch 49/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.96 - ETA: 0s - loss: 0.2024 - accuracy: 0.93 - ETA: 0s - loss: 0.2545 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9150 - val_loss: 0.8919 - val_accuracy: 0.6796\n",
-      "Epoch 50/50\n",
-      "153/153 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 1.00 - ETA: 0s - loss: 0.2337 - accuracy: 0.93 - ETA: 0s - loss: 0.2107 - accuracy: 0.94 - 0s 2ms/step - loss: 0.2187 - accuracy: 0.9412 - val_loss: 1.0747 - val_accuracy: 0.6408\n"
+      "Epoch 9/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.4845 - accuracy: 0.75 - ETA: 2s - loss: 0.4677 - accuracy: 0.78 - ETA: 2s - loss: 0.4696 - accuracy: 0.80 - ETA: 2s - loss: 0.4628 - accuracy: 0.81 - ETA: 1s - loss: 0.4336 - accuracy: 0.81 - ETA: 1s - loss: 0.4494 - accuracy: 0.82 - ETA: 1s - loss: 0.4543 - accuracy: 0.81 - ETA: 1s - loss: 0.4611 - accuracy: 0.80 - ETA: 1s - loss: 0.4484 - accuracy: 0.81 - ETA: 1s - loss: 0.4424 - accuracy: 0.82 - ETA: 1s - loss: 0.4411 - accuracy: 0.82 - ETA: 1s - loss: 0.4719 - accuracy: 0.81 - ETA: 1s - loss: 0.4809 - accuracy: 0.80 - ETA: 0s - loss: 0.4852 - accuracy: 0.80 - ETA: 0s - loss: 0.4700 - accuracy: 0.81 - ETA: 0s - loss: 0.4808 - accuracy: 0.80 - ETA: 0s - loss: 0.4666 - accuracy: 0.81 - ETA: 0s - loss: 0.4851 - accuracy: 0.80 - ETA: 0s - loss: 0.4920 - accuracy: 0.80 - ETA: 0s - loss: 0.4855 - accuracy: 0.80 - ETA: 0s - loss: 0.4795 - accuracy: 0.81 - 3s 4ms/step - loss: 0.4820 - accuracy: 0.8094 - val_loss: 0.4047 - val_accuracy: 0.8124\n",
+      "Epoch 10/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.3916 - accuracy: 0.87 - ETA: 2s - loss: 0.3660 - accuracy: 0.87 - ETA: 2s - loss: 0.3773 - accuracy: 0.86 - ETA: 2s - loss: 0.4001 - accuracy: 0.85 - ETA: 2s - loss: 0.3903 - accuracy: 0.85 - ETA: 2s - loss: 0.3829 - accuracy: 0.86 - ETA: 1s - loss: 0.3896 - accuracy: 0.85 - ETA: 1s - loss: 0.3891 - accuracy: 0.85 - ETA: 1s - loss: 0.3916 - accuracy: 0.85 - ETA: 1s - loss: 0.3775 - accuracy: 0.85 - ETA: 1s - loss: 0.3794 - accuracy: 0.86 - ETA: 1s - loss: 0.3743 - accuracy: 0.86 - ETA: 1s - loss: 0.3642 - accuracy: 0.87 - ETA: 1s - loss: 0.3599 - accuracy: 0.87 - ETA: 0s - loss: 0.3582 - accuracy: 0.87 - ETA: 0s - loss: 0.3602 - accuracy: 0.87 - ETA: 0s - loss: 0.3793 - accuracy: 0.86 - ETA: 0s - loss: 0.4010 - accuracy: 0.85 - ETA: 0s - loss: 0.4015 - accuracy: 0.85 - ETA: 0s - loss: 0.3929 - accuracy: 0.85 - ETA: 0s - loss: 0.3934 - accuracy: 0.85 - 3s 5ms/step - loss: 0.3922 - accuracy: 0.8592 - val_loss: 0.3622 - val_accuracy: 0.8699\n",
+      "Epoch 11/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.4113 - accuracy: 0.87 - ETA: 1s - loss: 0.3553 - accuracy: 0.87 - ETA: 1s - loss: 0.3829 - accuracy: 0.86 - ETA: 2s - loss: 0.3465 - accuracy: 0.88 - ETA: 2s - loss: 0.3565 - accuracy: 0.86 - ETA: 2s - loss: 0.3519 - accuracy: 0.85 - ETA: 2s - loss: 0.3343 - accuracy: 0.87 - ETA: 2s - loss: 0.4087 - accuracy: 0.85 - ETA: 1s - loss: 0.3958 - accuracy: 0.85 - ETA: 1s - loss: 0.4004 - accuracy: 0.84 - ETA: 1s - loss: 0.3891 - accuracy: 0.84 - ETA: 1s - loss: 0.3870 - accuracy: 0.85 - ETA: 1s - loss: 0.3787 - accuracy: 0.85 - ETA: 1s - loss: 0.3815 - accuracy: 0.85 - ETA: 1s - loss: 0.3927 - accuracy: 0.85 - ETA: 0s - loss: 0.3890 - accuracy: 0.85 - ETA: 0s - loss: 0.3885 - accuracy: 0.85 - ETA: 0s - loss: 0.3870 - accuracy: 0.85 - ETA: 0s - loss: 0.3833 - accuracy: 0.85 - ETA: 0s - loss: 0.3781 - accuracy: 0.85 - ETA: 0s - loss: 0.3750 - accuracy: 0.85 - 4s 6ms/step - loss: 0.3777 - accuracy: 0.8549 - val_loss: 0.3595 - val_accuracy: 0.8742\n",
+      "Epoch 12/20\n",
+      "703/703 [==============================] - ETA: 3s - loss: 0.3841 - accuracy: 0.81 - ETA: 2s - loss: 0.4127 - accuracy: 0.84 - ETA: 2s - loss: 0.4122 - accuracy: 0.84 - ETA: 2s - loss: 0.3829 - accuracy: 0.84 - ETA: 2s - loss: 0.3869 - accuracy: 0.84 - ETA: 2s - loss: 0.3867 - accuracy: 0.86 - ETA: 2s - loss: 0.3801 - accuracy: 0.86 - ETA: 2s - loss: 0.3770 - accuracy: 0.85 - ETA: 1s - loss: 0.3931 - accuracy: 0.84 - ETA: 1s - loss: 0.3823 - accuracy: 0.85 - ETA: 1s - loss: 0.3746 - accuracy: 0.85 - ETA: 1s - loss: 0.3640 - accuracy: 0.85 - ETA: 1s - loss: 0.3766 - accuracy: 0.85 - ETA: 0s - loss: 0.3792 - accuracy: 0.84 - ETA: 0s - loss: 0.3734 - accuracy: 0.84 - ETA: 0s - loss: 0.3676 - accuracy: 0.84 - ETA: 0s - loss: 0.3708 - accuracy: 0.84 - ETA: 0s - loss: 0.3666 - accuracy: 0.84 - ETA: 0s - loss: 0.3632 - accuracy: 0.85 - ETA: 0s - loss: 0.3540 - accuracy: 0.85 - ETA: 0s - loss: 0.3436 - accuracy: 0.86 - 3s 5ms/step - loss: 0.3428 - accuracy: 0.8620 - val_loss: 0.2976 - val_accuracy: 0.8806\n",
+      "Epoch 13/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.0931 - accuracy: 0.96 - ETA: 2s - loss: 0.2314 - accuracy: 0.89 - ETA: 2s - loss: 0.2322 - accuracy: 0.88 - ETA: 2s - loss: 0.2098 - accuracy: 0.90 - ETA: 2s - loss: 0.2262 - accuracy: 0.90 - ETA: 2s - loss: 0.2380 - accuracy: 0.89 - ETA: 2s - loss: 0.2444 - accuracy: 0.89 - ETA: 2s - loss: 0.2267 - accuracy: 0.90 - ETA: 1s - loss: 0.2296 - accuracy: 0.90 - ETA: 1s - loss: 0.2357 - accuracy: 0.90 - ETA: 1s - loss: 0.2746 - accuracy: 0.88 - ETA: 1s - loss: 0.2888 - accuracy: 0.87 - ETA: 1s - loss: 0.2861 - accuracy: 0.87 - ETA: 1s - loss: 0.2749 - accuracy: 0.88 - ETA: 0s - loss: 0.2702 - accuracy: 0.88 - ETA: 0s - loss: 0.2643 - accuracy: 0.89 - ETA: 0s - loss: 0.2651 - accuracy: 0.89 - ETA: 0s - loss: 0.2652 - accuracy: 0.89 - ETA: 0s - loss: 0.2767 - accuracy: 0.88 - ETA: 0s - loss: 0.2743 - accuracy: 0.88 - ETA: 0s - loss: 0.2719 - accuracy: 0.88 - 3s 5ms/step - loss: 0.2763 - accuracy: 0.8862 - val_loss: 0.3465 - val_accuracy: 0.8571\n",
+      "Epoch 14/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.3396 - accuracy: 0.87 - ETA: 1s - loss: 0.3321 - accuracy: 0.87 - ETA: 1s - loss: 0.2989 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2650 - accuracy: 0.90 - ETA: 1s - loss: 0.2664 - accuracy: 0.90 - ETA: 1s - loss: 0.2753 - accuracy: 0.89 - ETA: 1s - loss: 0.2679 - accuracy: 0.90 - ETA: 1s - loss: 0.2793 - accuracy: 0.89 - ETA: 1s - loss: 0.2694 - accuracy: 0.89 - ETA: 1s - loss: 0.2679 - accuracy: 0.89 - ETA: 1s - loss: 0.2611 - accuracy: 0.90 - ETA: 1s - loss: 0.2625 - accuracy: 0.89 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2593 - accuracy: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2538 - accuracy: 0.90 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2544 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.90 - ETA: 0s - loss: 0.2691 - accuracy: 0.89 - 3s 4ms/step - loss: 0.2748 - accuracy: 0.8905 - val_loss: 0.2620 - val_accuracy: 0.8913\n",
+      "Epoch 15/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.2962 - accuracy: 0.87 - ETA: 2s - loss: 0.3433 - accuracy: 0.84 - ETA: 2s - loss: 0.3125 - accuracy: 0.86 - ETA: 2s - loss: 0.3087 - accuracy: 0.87 - ETA: 2s - loss: 0.2851 - accuracy: 0.88 - ETA: 2s - loss: 0.2723 - accuracy: 0.89 - ETA: 1s - loss: 0.2781 - accuracy: 0.89 - ETA: 1s - loss: 0.2752 - accuracy: 0.89 - ETA: 1s - loss: 0.2694 - accuracy: 0.90 - ETA: 1s - loss: 0.2705 - accuracy: 0.91 - ETA: 1s - loss: 0.2786 - accuracy: 0.91 - ETA: 1s - loss: 0.2819 - accuracy: 0.91 - ETA: 1s - loss: 0.2761 - accuracy: 0.91 - ETA: 1s - loss: 0.2753 - accuracy: 0.91 - ETA: 0s - loss: 0.2641 - accuracy: 0.92 - ETA: 0s - loss: 0.2561 - accuracy: 0.92 - ETA: 0s - loss: 0.2649 - accuracy: 0.92 - ETA: 0s - loss: 0.2697 - accuracy: 0.91 - ETA: 0s - loss: 0.2690 - accuracy: 0.91 - ETA: 0s - loss: 0.2702 - accuracy: 0.91 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - 3s 4ms/step - loss: 0.2660 - accuracy: 0.9118 - val_loss: 0.2809 - val_accuracy: 0.8763\n",
+      "Epoch 16/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.2073 - accuracy: 0.87 - ETA: 2s - loss: 0.2524 - accuracy: 0.85 - ETA: 1s - loss: 0.2867 - accuracy: 0.85 - ETA: 1s - loss: 0.2501 - accuracy: 0.87 - ETA: 1s - loss: 0.2704 - accuracy: 0.88 - ETA: 1s - loss: 0.2697 - accuracy: 0.87 - ETA: 1s - loss: 0.2504 - accuracy: 0.88 - ETA: 1s - loss: 0.2425 - accuracy: 0.89 - ETA: 1s - loss: 0.2301 - accuracy: 0.90 - ETA: 1s - loss: 0.2360 - accuracy: 0.90 - ETA: 1s - loss: 0.2324 - accuracy: 0.90 - ETA: 1s - loss: 0.2269 - accuracy: 0.90 - ETA: 0s - loss: 0.2152 - accuracy: 0.91 - ETA: 0s - loss: 0.2151 - accuracy: 0.91 - ETA: 0s - loss: 0.2126 - accuracy: 0.91 - ETA: 0s - loss: 0.2134 - accuracy: 0.91 - ETA: 0s - loss: 0.2123 - accuracy: 0.91 - ETA: 0s - loss: 0.2169 - accuracy: 0.91 - ETA: 0s - loss: 0.2122 - accuracy: 0.91 - ETA: 0s - loss: 0.2132 - accuracy: 0.91 - ETA: 0s - loss: 0.2165 - accuracy: 0.91 - 3s 4ms/step - loss: 0.2229 - accuracy: 0.9147 - val_loss: 0.2815 - val_accuracy: 0.8742\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 17/20\n",
+      "703/703 [==============================] - ETA: 2s - loss: 0.0761 - accuracy: 1.00 - ETA: 2s - loss: 0.0978 - accuracy: 0.98 - ETA: 2s - loss: 0.1218 - accuracy: 0.95 - ETA: 2s - loss: 0.1368 - accuracy: 0.94 - ETA: 2s - loss: 0.1530 - accuracy: 0.93 - ETA: 2s - loss: 0.1898 - accuracy: 0.92 - ETA: 2s - loss: 0.1810 - accuracy: 0.93 - ETA: 2s - loss: 0.1851 - accuracy: 0.93 - ETA: 1s - loss: 0.2138 - accuracy: 0.93 - ETA: 1s - loss: 0.2094 - accuracy: 0.93 - ETA: 1s - loss: 0.2079 - accuracy: 0.93 - ETA: 1s - loss: 0.1979 - accuracy: 0.93 - ETA: 1s - loss: 0.2033 - accuracy: 0.93 - ETA: 1s - loss: 0.2072 - accuracy: 0.92 - ETA: 0s - loss: 0.2051 - accuracy: 0.92 - ETA: 0s - loss: 0.2008 - accuracy: 0.93 - ETA: 0s - loss: 0.1977 - accuracy: 0.93 - ETA: 0s - loss: 0.2074 - accuracy: 0.92 - ETA: 0s - loss: 0.2194 - accuracy: 0.92 - ETA: 0s - loss: 0.2159 - accuracy: 0.92 - ETA: 0s - loss: 0.2109 - accuracy: 0.93 - 3s 5ms/step - loss: 0.2068 - accuracy: 0.9317 - val_loss: 0.2568 - val_accuracy: 0.8870\n",
+      "Epoch 18/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.2396 - accuracy: 0.81 - ETA: 2s - loss: 0.2274 - accuracy: 0.89 - ETA: 2s - loss: 0.2077 - accuracy: 0.89 - ETA: 2s - loss: 0.1903 - accuracy: 0.91 - ETA: 2s - loss: 0.1785 - accuracy: 0.91 - ETA: 2s - loss: 0.1704 - accuracy: 0.92 - ETA: 1s - loss: 0.1823 - accuracy: 0.91 - ETA: 1s - loss: 0.1723 - accuracy: 0.92 - ETA: 1s - loss: 0.1623 - accuracy: 0.93 - ETA: 1s - loss: 0.1524 - accuracy: 0.93 - ETA: 1s - loss: 0.1414 - accuracy: 0.94 - ETA: 1s - loss: 0.1466 - accuracy: 0.94 - ETA: 1s - loss: 0.1635 - accuracy: 0.92 - ETA: 0s - loss: 0.1613 - accuracy: 0.92 - ETA: 0s - loss: 0.1602 - accuracy: 0.92 - ETA: 0s - loss: 0.1611 - accuracy: 0.93 - ETA: 0s - loss: 0.1583 - accuracy: 0.93 - ETA: 0s - loss: 0.1558 - accuracy: 0.93 - ETA: 0s - loss: 0.1629 - accuracy: 0.93 - ETA: 0s - loss: 0.1574 - accuracy: 0.93 - ETA: 0s - loss: 0.1612 - accuracy: 0.93 - 3s 4ms/step - loss: 0.1574 - accuracy: 0.9388 - val_loss: 0.2297 - val_accuracy: 0.9041\n",
+      "Epoch 19/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.2134 - accuracy: 0.90 - ETA: 2s - loss: 0.1912 - accuracy: 0.93 - ETA: 2s - loss: 0.1658 - accuracy: 0.94 - ETA: 2s - loss: 0.1487 - accuracy: 0.95 - ETA: 2s - loss: 0.1577 - accuracy: 0.94 - ETA: 1s - loss: 0.1700 - accuracy: 0.94 - ETA: 1s - loss: 0.1669 - accuracy: 0.94 - ETA: 1s - loss: 0.1695 - accuracy: 0.94 - ETA: 1s - loss: 0.1748 - accuracy: 0.93 - ETA: 1s - loss: 0.1761 - accuracy: 0.93 - ETA: 1s - loss: 0.1749 - accuracy: 0.93 - ETA: 1s - loss: 0.1706 - accuracy: 0.93 - ETA: 1s - loss: 0.1651 - accuracy: 0.93 - ETA: 1s - loss: 0.1596 - accuracy: 0.94 - ETA: 0s - loss: 0.1659 - accuracy: 0.94 - ETA: 0s - loss: 0.1604 - accuracy: 0.94 - ETA: 0s - loss: 0.1601 - accuracy: 0.94 - ETA: 0s - loss: 0.1546 - accuracy: 0.94 - ETA: 0s - loss: 0.1610 - accuracy: 0.94 - ETA: 0s - loss: 0.1590 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - 3s 4ms/step - loss: 0.1609 - accuracy: 0.9445 - val_loss: 0.2845 - val_accuracy: 0.8998\n",
+      "Epoch 20/20\n",
+      "703/703 [==============================] - ETA: 1s - loss: 0.2891 - accuracy: 0.87 - ETA: 2s - loss: 0.2990 - accuracy: 0.87 - ETA: 2s - loss: 0.2143 - accuracy: 0.91 - ETA: 1s - loss: 0.1812 - accuracy: 0.92 - ETA: 1s - loss: 0.1767 - accuracy: 0.93 - ETA: 1s - loss: 0.1552 - accuracy: 0.94 - ETA: 1s - loss: 0.1531 - accuracy: 0.94 - ETA: 1s - loss: 0.1448 - accuracy: 0.94 - ETA: 1s - loss: 0.1562 - accuracy: 0.94 - ETA: 1s - loss: 0.1540 - accuracy: 0.94 - ETA: 1s - loss: 0.1491 - accuracy: 0.94 - ETA: 1s - loss: 0.1528 - accuracy: 0.94 - ETA: 1s - loss: 0.1484 - accuracy: 0.94 - ETA: 0s - loss: 0.1449 - accuracy: 0.95 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - ETA: 0s - loss: 0.1554 - accuracy: 0.94 - ETA: 0s - loss: 0.1582 - accuracy: 0.93 - ETA: 0s - loss: 0.1533 - accuracy: 0.94 - ETA: 0s - loss: 0.1489 - accuracy: 0.94 - ETA: 0s - loss: 0.1470 - accuracy: 0.94 - ETA: 0s - loss: 0.1441 - accuracy: 0.94 - 3s 4ms/step - loss: 0.1417 - accuracy: 0.9502 - val_loss: 0.2835 - val_accuracy: 0.8742\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<keras.callbacks.callbacks.History at 0x1c113933bc8>"
+       "<keras.callbacks.callbacks.History at 0x1dd64399688>"
       ]
      },
-     "execution_count": 9,
+     "execution_count": 23,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -416,7 +380,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 24,
    "metadata": {},
    "outputs": [
     {
@@ -442,7 +406,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 25,
    "metadata": {
     "scrolled": true
    },
@@ -451,54 +415,54 @@
     "from keras.models import load_model\n",
     "\n",
     "# Load the model\n",
-    "loaded_model = load_model('ant_cnn_model.h5')"
+    "loaded_model = load_model('geo_cnn_model.h5')"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 26,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Model: \"sequential_1\"\n",
+      "Model: \"sequential_2\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "conv2d_1 (Conv2D)            (None, 48, 19, 24)        240       \n",
+      "conv2d_4 (Conv2D)            (None, 126, 30, 24)       240       \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_1 (MaxPooling2 (None, 24, 9, 24)         0         \n",
+      "max_pooling2d_3 (MaxPooling2 (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "activation_1 (Activation)    (None, 24, 9, 24)         0         \n",
+      "activation_6 (Activation)    (None, 63, 15, 24)        0         \n",
       "_________________________________________________________________\n",
-      "conv2d_2 (Conv2D)            (None, 22, 7, 48)         10416     \n",
+      "conv2d_5 (Conv2D)            (None, 61, 13, 48)        10416     \n",
       "_________________________________________________________________\n",
-      "max_pooling2d_2 (MaxPooling2 (None, 11, 3, 48)         0         \n",
+      "max_pooling2d_4 (MaxPooling2 (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "activation_2 (Activation)    (None, 11, 3, 48)         0         \n",
+      "activation_7 (Activation)    (None, 30, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "conv2d_3 (Conv2D)            (None, 9, 3, 48)          6960      \n",
+      "conv2d_6 (Conv2D)            (None, 28, 6, 48)         6960      \n",
       "_________________________________________________________________\n",
-      "activation_3 (Activation)    (None, 9, 3, 48)          0         \n",
+      "activation_8 (Activation)    (None, 28, 6, 48)         0         \n",
       "_________________________________________________________________\n",
-      "flatten_1 (Flatten)          (None, 1296)              0         \n",
+      "flatten_2 (Flatten)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dropout_1 (Dropout)          (None, 1296)              0         \n",
+      "dropout_3 (Dropout)          (None, 8064)              0         \n",
       "_________________________________________________________________\n",
-      "dense_1 (Dense)              (None, 64)                83008     \n",
+      "dense_3 (Dense)              (None, 64)                516160    \n",
       "_________________________________________________________________\n",
-      "activation_4 (Activation)    (None, 64)                0         \n",
+      "activation_9 (Activation)    (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dropout_2 (Dropout)          (None, 64)                0         \n",
+      "dropout_4 (Dropout)          (None, 64)                0         \n",
       "_________________________________________________________________\n",
-      "dense_2 (Dense)              (None, 5)                 325       \n",
+      "dense_4 (Dense)              (None, 5)                 325       \n",
       "_________________________________________________________________\n",
-      "activation_5 (Activation)    (None, 5)                 0         \n",
+      "activation_10 (Activation)   (None, 5)                 0         \n",
       "=================================================================\n",
-      "Total params: 100,949\n",
-      "Trainable params: 100,949\n",
+      "Total params: 534,101\n",
+      "Trainable params: 534,101\n",
       "Non-trainable params: 0\n",
       "_________________________________________________________________\n"
      ]
@@ -511,25 +475,35 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 27,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "[[ 4  0  0  0  0]\n",
-      " [ 0 21  0  5  0]\n",
-      " [ 0  2  0  1  0]\n",
-      " [ 0  9  0 10  4]\n",
-      " [ 0  2  0  2  4]]\n",
+      "[[ 19   0   0   0   0]\n",
+      " [  1 112   0  10   0]\n",
+      " [  0   0  14   0   0]\n",
+      " [  1  12   1  67  14]\n",
+      " [  0   1   0   3  39]]\n",
       "Accuracy for class GOC : [1.]\n",
-      "Accuracy for class GRA : [0.80769231]\n",
-      "Accuracy for class GST : [0.]\n",
-      "Accuracy for class GWG : [0.43478261]\n",
-      "Accuracy for class GWC : [0.5]\n",
-      "Overall Accuracy : 0.609375\n"
+      "Accuracy for class GRA : [0.91056911]\n",
+      "Accuracy for class GST : [1.]\n",
+      "Accuracy for class GWG : [0.70526316]\n",
+      "Accuracy for class GWC : [0.90697674]\n",
+      "Overall Accuracy : 0.8537414965986394\n"
      ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "'[[ 19   0   0   0   0]\\n [  0 119   0   4   0]\\n [  0   0  14   0   0]\\n [  2  19   0  67   7]\\n [  0   4   0   3  36]]\\nAccuracy for class GOC : [1.]\\nAccuracy for class GRA : [0.96747967]\\nAccuracy for class GST : [1.]\\nAccuracy for class GWG : [0.70526316]\\nAccuracy for class GWC : [0.8372093]\\nOverall Accuracy : 0.8673469387755102'"
+      ]
+     },
+     "execution_count": 27,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
@@ -548,12 +522,24 @@
     "    else : mean = \"N/A\"\n",
     "    print(\"Accuracy for class\", labels[class_i], \":\", mean)\n",
     "\n",
-    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))"
+    "print(\"Overall Accuracy :\", np.mean(y_test == y_pred_labels))\n",
+    "\n",
+    "'''[[ 19   0   0   0   0]\n",
+    " [  1 112   0  10   0]\n",
+    " [  0   0  14   0   0]\n",
+    " [  1  12   1  67  14]\n",
+    " [  0   1   0   3  39]]\n",
+    "Accuracy for class GOC : [1.]\n",
+    "Accuracy for class GRA : [0.91056911]\n",
+    "Accuracy for class GST : [1.]\n",
+    "Accuracy for class GWG : [0.70526316]\n",
+    "Accuracy for class GWC : [0.90697674]\n",
+    "Overall Accuracy : 0.8537414965986394'''"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 28,
    "metadata": {},
    "outputs": [
     {
@@ -563,632 +549,634 @@
       "[0. 1.]\n",
       "[ 0.0000000e+00  1.5258789e-05  0.0000000e+00 ...  3.3020020e-02\n",
       "  1.2680054e-02 -8.7432861e-03]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00110971\n",
+      " GOC :  0.00000176\n",
       "\n",
-      " GRA :  0.00687433\n",
+      " GRA :  0.38465884\n",
       "\n",
-      " GST :  0.00060519\n",
+      " GST :  0.00000003\n",
       "\n",
-      " GWG :  0.28046575\n",
+      " GWG :  0.00648697\n",
       "\n",
-      " GWC :  0.71094495\n",
+      " GWC :  0.60885239\n",
       "\n",
       "\n",
       "GUESS:  GWC\n",
       "[1. 2.]\n",
       "[-0.03717041 -0.05769348 -0.06455994 ...  0.01766968  0.01895142\n",
       "  0.01779175]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000150\n",
+      " GOC :  0.00000044\n",
       "\n",
-      " GRA :  0.66413796\n",
+      " GRA :  0.00247770\n",
       "\n",
-      " GST :  0.00009404\n",
+      " GST :  0.00000003\n",
       "\n",
-      " GWG :  0.00712661\n",
+      " GWG :  0.01955874\n",
       "\n",
-      " GWC :  0.32863984\n",
+      " GWC :  0.97796303\n",
       "\n",
       "\n",
-      "GUESS:  GRA\n",
+      "GUESS:  GWC\n",
       "[2. 3.]\n",
       "[ 0.02345276  0.02101135  0.01712036 ... -0.01161194 -0.0141449\n",
       " -0.01431274]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00008299\n",
+      " GOC :  0.00002324\n",
       "\n",
-      " GRA :  0.22556210\n",
+      " GRA :  0.00770301\n",
       "\n",
-      " GST :  0.00302639\n",
+      " GST :  0.00000020\n",
       "\n",
-      " GWG :  0.10296817\n",
+      " GWG :  0.01101210\n",
       "\n",
-      " GWC :  0.66836035\n",
+      " GWC :  0.98126149\n",
       "\n",
       "\n",
       "GUESS:  GWC\n",
       "[3. 4.]\n",
       "[-0.01583862 -0.01066589 -0.00762939 ... -0.0377655  -0.03556824\n",
       " -0.02685547]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00076802\n",
+      " GOC :  0.00001256\n",
       "\n",
-      " GRA :  0.19427927\n",
+      " GRA :  0.94493687\n",
       "\n",
-      " GST :  0.00463253\n",
+      " GST :  0.00001222\n",
       "\n",
-      " GWG :  0.03950845\n",
+      " GWG :  0.03799357\n",
       "\n",
-      " GWC :  0.76081169\n",
+      " GWC :  0.01704480\n",
       "\n",
       "\n",
-      "GUESS:  GWC\n",
+      "GUESS:  GRA\n",
       "[4. 5.]\n",
       "[-0.02836609 -0.02510071 -0.02012634 ...  0.0138855  -0.00386047\n",
       " -0.00904846]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00053637\n",
+      " GOC :  0.00009305\n",
       "\n",
-      " GRA :  0.19687678\n",
+      " GRA :  0.07313888\n",
       "\n",
-      " GST :  0.01669122\n",
+      " GST :  0.00000047\n",
       "\n",
-      " GWG :  0.42333746\n",
+      " GWG :  0.15516256\n",
       "\n",
-      " GWC :  0.36255816\n",
-      "GUESS: Nothing\n",
+      " GWC :  0.77160501\n",
+      "\n",
+      "\n",
+      "GUESS:  GWC\n",
       "[5. 6.]\n",
       "[-0.00526428  0.00822449  0.01951599 ...  0.02729797  0.02156067\n",
       "  0.01234436]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00001735\n",
+      " GOC :  0.00002247\n",
       "\n",
-      " GRA :  0.67453504\n",
+      " GRA :  0.26627952\n",
       "\n",
-      " GST :  0.00068860\n",
+      " GST :  0.00000522\n",
       "\n",
-      " GWG :  0.01727903\n",
+      " GWG :  0.01112610\n",
       "\n",
-      " GWC :  0.30747995\n",
+      " GWC :  0.72256666\n",
       "\n",
       "\n",
-      "GUESS:  GRA\n",
+      "GUESS:  GWC\n",
       "[6. 7.]\n",
       "[ 0.00544739  0.00053406  0.00970459 ... -0.02848816 -0.01611328\n",
       " -0.01091003]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00041052\n",
+      " GOC :  0.00000011\n",
       "\n",
-      " GRA :  0.01402403\n",
+      " GRA :  0.00877954\n",
       "\n",
-      " GST :  0.00038692\n",
+      " GST :  0.00000002\n",
       "\n",
-      " GWG :  0.17212571\n",
+      " GWG :  0.00733439\n",
       "\n",
-      " GWC :  0.81305289\n",
+      " GWC :  0.98388600\n",
       "\n",
       "\n",
       "GUESS:  GWC\n",
       "[7. 8.]\n",
       "[-0.0177002  -0.02372742 -0.02700806 ... -0.04304504 -0.04063416\n",
       " -0.03363037]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000991\n",
+      " GOC :  0.00000136\n",
       "\n",
-      " GRA :  0.94457138\n",
+      " GRA :  0.99270928\n",
       "\n",
-      " GST :  0.00616840\n",
+      " GST :  0.00002075\n",
       "\n",
-      " GWG :  0.03336105\n",
+      " GWG :  0.00483087\n",
       "\n",
-      " GWC :  0.01588919\n",
+      " GWC :  0.00243783\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[8. 9.]\n",
       "[-0.01539612 -0.00108337  0.00718689 ...  0.01161194  0.01818848\n",
       "  0.02700806]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00008726\n",
+      " GOC :  0.00000364\n",
       "\n",
-      " GRA :  0.81412381\n",
+      " GRA :  0.89666098\n",
       "\n",
-      " GST :  0.01158650\n",
+      " GST :  0.00000704\n",
       "\n",
-      " GWG :  0.09921984\n",
+      " GWG :  0.00706323\n",
       "\n",
-      " GWC :  0.07498255\n",
+      " GWC :  0.09626509\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[ 9. 10.]\n",
       "[ 0.03549194  0.04856873  0.05519104 ... -0.02171326 -0.03634644\n",
       " -0.03912354]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00191176\n",
+      " GOC :  0.00001260\n",
       "\n",
-      " GRA :  0.09809754\n",
+      " GRA :  0.15989006\n",
       "\n",
-      " GST :  0.00213318\n",
+      " GST :  0.00000509\n",
       "\n",
-      " GWG :  0.23002826\n",
+      " GWG :  0.05977093\n",
       "\n",
-      " GWC :  0.66782933\n",
+      " GWC :  0.78032130\n",
       "\n",
       "\n",
       "GUESS:  GWC\n",
       "[10. 11.]\n",
       "[-0.02934265 -0.0115509   0.00445557 ... -0.03616333 -0.03759766\n",
       " -0.0304718 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000013\n",
+      " GOC :  0.00000484\n",
       "\n",
-      " GRA :  0.99877483\n",
+      " GRA :  0.62774366\n",
       "\n",
-      " GST :  0.00003083\n",
+      " GST :  0.00000141\n",
       "\n",
-      " GWG :  0.00113827\n",
+      " GWG :  0.05731521\n",
       "\n",
-      " GWC :  0.00005597\n",
+      " GWC :  0.31493491\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[11. 12.]\n",
       "[-0.03358459 -0.03901672 -0.03933716 ... -0.02337646 -0.02124023\n",
       " -0.02107239]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00022642\n",
+      " GOC :  0.00001070\n",
       "\n",
-      " GRA :  0.13672589\n",
+      " GRA :  0.02227188\n",
       "\n",
-      " GST :  0.00204764\n",
+      " GST :  0.00000205\n",
       "\n",
-      " GWG :  0.66154647\n",
+      " GWG :  0.06705441\n",
       "\n",
-      " GWC :  0.19945353\n",
+      " GWC :  0.91066092\n",
       "\n",
       "\n",
-      "GUESS:  GWG\n",
+      "GUESS:  GWC\n",
       "[12. 13.]\n",
       "[-0.00846863  0.00444031  0.00852966 ... -0.00604248 -0.00845337\n",
       " -0.00497437]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00307768\n",
+      " GOC :  0.00000028\n",
       "\n",
-      " GRA :  0.21987155\n",
+      " GRA :  0.21407339\n",
       "\n",
-      " GST :  0.01730751\n",
+      " GST :  0.00000426\n",
       "\n",
-      " GWG :  0.38038778\n",
+      " GWG :  0.48406833\n",
       "\n",
-      " GWC :  0.37935549\n",
+      " GWC :  0.30185366\n",
       "GUESS: Nothing\n",
       "[13. 14.]\n",
       "[-0.00427246 -0.00718689 -0.00811768 ... -0.01966858 -0.01296997\n",
       " -0.01628113]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00033389\n",
+      " GOC :  0.00000002\n",
       "\n",
-      " GRA :  0.24240135\n",
+      " GRA :  0.04747614\n",
       "\n",
-      " GST :  0.01909669\n",
+      " GST :  0.00000026\n",
       "\n",
-      " GWG :  0.65060490\n",
+      " GWG :  0.28232297\n",
       "\n",
-      " GWC :  0.08756324\n",
+      " GWC :  0.67020059\n",
       "\n",
       "\n",
-      "GUESS:  GWG\n",
+      "GUESS:  GWC\n",
       "[14. 15.]\n",
       "[-0.02262878 -0.01573181 -0.00117493 ... -0.08956909 -0.0695343\n",
       " -0.04067993]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00019831\n",
+      " GOC :  0.00000068\n",
       "\n",
-      " GRA :  0.09028011\n",
+      " GRA :  0.03221902\n",
       "\n",
-      " GST :  0.00338028\n",
+      " GST :  0.00000030\n",
       "\n",
-      " GWG :  0.46894377\n",
+      " GWG :  0.06798044\n",
       "\n",
-      " GWC :  0.43719757\n",
-      "GUESS: Nothing\n",
+      " GWC :  0.89979953\n",
+      "\n",
+      "\n",
+      "GUESS:  GWC\n",
       "[15. 16.]\n",
       "[-0.02532959 -0.01031494 -0.00280762 ... -0.07128906 -0.07106018\n",
       " -0.05839539]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00002741\n",
+      " GOC :  0.00000002\n",
       "\n",
-      " GRA :  0.01345812\n",
+      " GRA :  0.86727339\n",
       "\n",
-      " GST :  0.00017061\n",
+      " GST :  0.00000005\n",
       "\n",
-      " GWG :  0.00623927\n",
+      " GWG :  0.00011261\n",
       "\n",
-      " GWC :  0.98010468\n",
+      " GWC :  0.13261394\n",
       "\n",
       "\n",
-      "GUESS:  GWC\n",
+      "GUESS:  GRA\n",
       "[16. 17.]\n",
       "[-0.04600525 -0.02149963  0.00523376 ... -0.02526855 -0.02735901\n",
       " -0.03106689]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000000\n",
+      " GOC :  0.00000016\n",
       "\n",
-      " GRA :  0.99911612\n",
+      " GRA :  0.10364556\n",
       "\n",
-      " GST :  0.00000119\n",
+      " GST :  0.00000006\n",
       "\n",
-      " GWG :  0.00087264\n",
+      " GWG :  0.04509518\n",
       "\n",
-      " GWC :  0.00001005\n",
+      " GWC :  0.85125911\n",
       "\n",
       "\n",
-      "GUESS:  GRA\n",
+      "GUESS:  GWC\n",
       "[17. 18.]\n",
       "[-0.02043152 -0.01174927 -0.02088928 ...  0.10055542  0.08653259\n",
       "  0.06604004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00001004\n",
-      "\n",
-      " GRA :  0.98751462\n",
+      " GOC :  0.00009542\n",
       "\n",
-      " GST :  0.00235241\n",
+      " GRA :  0.34512478\n",
       "\n",
-      " GWG :  0.00240187\n",
+      " GST :  0.00000620\n",
       "\n",
-      " GWC :  0.00772112\n",
+      " GWG :  0.19678749\n",
       "\n",
-      "\n",
-      "GUESS:  GRA\n",
+      " GWC :  0.45798609\n",
+      "GUESS: Nothing\n",
       "[18. 19.]\n",
       "[ 0.04153442  0.01223755 -0.00654602 ...  0.03269958  0.02374268\n",
       "  0.02774048]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000010\n",
+      " GOC :  0.00000035\n",
       "\n",
-      " GRA :  0.99923599\n",
+      " GRA :  0.76191986\n",
       "\n",
-      " GST :  0.00000878\n",
+      " GST :  0.00000023\n",
       "\n",
-      " GWG :  0.00006613\n",
+      " GWG :  0.08037928\n",
       "\n",
-      " GWC :  0.00068900\n",
+      " GWC :  0.15770023\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[19. 20.]\n",
       "[0.02185059 0.02069092 0.01451111 ... 0.03469849 0.03985596 0.04600525]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000007\n",
+      " GOC :  0.00047749\n",
       "\n",
-      " GRA :  0.99936324\n",
+      " GRA :  0.76327378\n",
       "\n",
-      " GST :  0.00004567\n",
+      " GST :  0.00000129\n",
       "\n",
-      " GWG :  0.00001874\n",
+      " GWG :  0.03341111\n",
       "\n",
-      " GWC :  0.00057223\n",
+      " GWC :  0.20283636\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[20. 21.]\n",
       "[ 0.0353241   0.01567078 -0.00102234 ...  0.1058197   0.10365295\n",
       "  0.09759521]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00001078\n",
+      " GOC :  0.00002144\n",
       "\n",
-      " GRA :  0.97431314\n",
+      " GRA :  0.63609004\n",
       "\n",
-      " GST :  0.00149824\n",
+      " GST :  0.00001051\n",
       "\n",
-      " GWG :  0.00516652\n",
+      " GWG :  0.30752742\n",
       "\n",
-      " GWC :  0.01901135\n",
+      " GWC :  0.05635052\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[21. 22.]\n",
       "[ 0.09413147  0.07905579  0.05625916 ... -0.01145935 -0.00245667\n",
       "  0.00479126]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00001107\n",
+      " GOC :  0.00000066\n",
       "\n",
-      " GRA :  0.85995126\n",
+      " GRA :  0.99761701\n",
       "\n",
-      " GST :  0.01212410\n",
+      " GST :  0.00000020\n",
       "\n",
-      " GWG :  0.12441237\n",
+      " GWG :  0.00054236\n",
       "\n",
-      " GWC :  0.00350112\n",
+      " GWC :  0.00183976\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[22. 23.]\n",
       "[ 0.0037384   0.01168823  0.01628113 ... -0.03440857 -0.05511475\n",
       " -0.08209229]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000007\n",
+      " GOC :  0.00001949\n",
       "\n",
-      " GRA :  0.99817920\n",
+      " GRA :  0.87093616\n",
       "\n",
-      " GST :  0.00064762\n",
+      " GST :  0.00000191\n",
       "\n",
-      " GWG :  0.00114116\n",
+      " GWG :  0.04734631\n",
       "\n",
-      " GWC :  0.00003188\n",
+      " GWC :  0.08169603\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[23. 24.]\n",
       "[-0.1026001  -0.12590027 -0.14944458 ...  0.03462219  0.02537537\n",
       "  0.02354431]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000003\n",
+      " GOC :  0.00000000\n",
       "\n",
-      " GRA :  0.99974483\n",
+      " GRA :  0.79179174\n",
       "\n",
-      " GST :  0.00003395\n",
+      " GST :  0.00000004\n",
       "\n",
-      " GWG :  0.00001006\n",
+      " GWG :  0.19508553\n",
       "\n",
-      " GWC :  0.00021120\n",
+      " GWC :  0.01312272\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[24. 25.]\n",
       "[ 0.0196991   0.02836609  0.03103638 ... -0.03009033 -0.03392029\n",
       " -0.03681946]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000000\n",
+      " GOC :  0.00000019\n",
       "\n",
-      " GRA :  0.99998176\n",
+      " GRA :  0.99032038\n",
       "\n",
-      " GST :  0.00001182\n",
+      " GST :  0.00000008\n",
       "\n",
-      " GWG :  0.00000516\n",
+      " GWG :  0.00241051\n",
       "\n",
-      " GWC :  0.00000131\n",
+      " GWC :  0.00726884\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[25. 26.]\n",
       "[-0.04151917 -0.03933716 -0.03703308 ...  0.05451965  0.0519104\n",
       "  0.05206299]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00003238\n",
+      " GOC :  0.00000147\n",
       "\n",
-      " GRA :  0.97408438\n",
+      " GRA :  0.94179332\n",
       "\n",
-      " GST :  0.00137138\n",
+      " GST :  0.00000139\n",
       "\n",
-      " GWG :  0.00180984\n",
+      " GWG :  0.05180373\n",
       "\n",
-      " GWC :  0.02270207\n",
+      " GWC :  0.00640011\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[26. 27.]\n",
       "[ 0.05670166  0.06253052  0.07643127 ... -0.00396729  0.00715637\n",
       "  0.00585938]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00031376\n",
+      " GOC :  0.00000093\n",
       "\n",
-      " GRA :  0.18696703\n",
+      " GRA :  0.06941112\n",
       "\n",
-      " GST :  0.00742342\n",
+      " GST :  0.00000640\n",
       "\n",
-      " GWG :  0.34027183\n",
+      " GWG :  0.14259055\n",
       "\n",
-      " GWC :  0.46502385\n",
-      "GUESS: Nothing\n",
+      " GWC :  0.78799099\n",
+      "\n",
+      "\n",
+      "GUESS:  GWC\n",
       "[27. 28.]\n",
       "[-0.00222778 -0.01303101 -0.02310181 ...  0.01165771  0.01649475\n",
       "  0.0194397 ]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00005648\n",
+      " GOC :  0.00000000\n",
       "\n",
-      " GRA :  0.13421611\n",
+      " GRA :  0.01543347\n",
       "\n",
-      " GST :  0.00141173\n",
+      " GST :  0.00000000\n",
       "\n",
-      " GWG :  0.47746155\n",
+      " GWG :  0.04449097\n",
       "\n",
-      " GWC :  0.38685405\n",
-      "GUESS: Nothing\n",
+      " GWC :  0.94007552\n",
+      "\n",
+      "\n",
+      "GUESS:  GWC\n",
       "[28. 29.]\n",
       "[0.01657104 0.01519775 0.00924683 ... 0.03746033 0.03282166 0.02775574]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00001463\n",
+      " GOC :  0.00000003\n",
       "\n",
-      " GRA :  0.36573434\n",
+      " GRA :  0.33569953\n",
       "\n",
-      " GST :  0.00021427\n",
+      " GST :  0.00000165\n",
       "\n",
-      " GWG :  0.02339190\n",
+      " GWG :  0.14353959\n",
       "\n",
-      " GWC :  0.61064482\n",
+      " GWC :  0.52075917\n",
       "\n",
       "\n",
       "GUESS:  GWC\n",
       "[29. 30.]\n",
       "[ 0.01919556  0.0135498   0.01724243 ... -0.00575256 -0.01502991\n",
       " -0.02742004]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000170\n",
+      " GOC :  0.00000223\n",
       "\n",
-      " GRA :  0.99406528\n",
+      " GRA :  0.93829304\n",
       "\n",
-      " GST :  0.00014316\n",
+      " GST :  0.00000170\n",
       "\n",
-      " GWG :  0.00414455\n",
+      " GWG :  0.00578377\n",
       "\n",
-      " GWC :  0.00164524\n",
+      " GWC :  0.05591923\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[30. 31.]\n",
       "[-0.0322876  -0.0365448  -0.03544617 ... -0.0218811  -0.02978516\n",
       " -0.04052734]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00073814\n",
+      " GOC :  0.00000810\n",
       "\n",
-      " GRA :  0.86860937\n",
+      " GRA :  0.60701579\n",
       "\n",
-      " GST :  0.00153848\n",
+      " GST :  0.00000535\n",
       "\n",
-      " GWG :  0.04801571\n",
+      " GWG :  0.21953745\n",
       "\n",
-      " GWC :  0.08109826\n",
+      " GWC :  0.17343327\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[31. 32.]\n",
       "[-0.04328918 -0.03413391 -0.03421021 ...  0.05908203  0.06370544\n",
       "  0.05949402]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00002089\n",
-      "\n",
-      " GRA :  0.80240065\n",
-      "\n",
-      " GST :  0.00016713\n",
+      " GOC :  0.00000032\n",
       "\n",
-      " GWG :  0.10498895\n",
+      " GRA :  0.07600122\n",
       "\n",
-      " GWC :  0.09242238\n",
+      " GST :  0.00000019\n",
       "\n",
+      " GWG :  0.49096382\n",
       "\n",
-      "GUESS:  GRA\n",
+      " GWC :  0.43303445\n",
+      "GUESS: Nothing\n",
       "[32. 33.]\n",
       "[ 0.06063843  0.06056213  0.06610107 ... -0.12741089 -0.13371277\n",
       " -0.12313843]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000001\n",
+      " GOC :  0.00000140\n",
       "\n",
-      " GRA :  0.99997139\n",
+      " GRA :  0.12951134\n",
       "\n",
-      " GST :  0.00000195\n",
+      " GST :  0.00000031\n",
       "\n",
-      " GWG :  0.00000346\n",
+      " GWG :  0.58525521\n",
       "\n",
-      " GWC :  0.00002330\n",
+      " GWC :  0.28523183\n",
       "\n",
       "\n",
-      "GUESS:  GRA\n",
+      "GUESS:  GWG\n",
       "[33. 34.]\n",
       "[-0.09968567 -0.06376648 -0.03105164 ... -0.0138092  -0.01574707\n",
       " -0.01896667]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000411\n",
+      " GOC :  0.00000170\n",
       "\n",
-      " GRA :  0.98667425\n",
+      " GRA :  0.86591440\n",
       "\n",
-      " GST :  0.00016910\n",
+      " GST :  0.00000117\n",
       "\n",
-      " GWG :  0.00259384\n",
+      " GWG :  0.13115266\n",
       "\n",
-      " GWC :  0.01055874\n",
+      " GWC :  0.00293008\n",
       "\n",
       "\n",
       "GUESS:  GRA\n",
       "[34. 35.]\n",
       "[-0.00811768  0.00149536  0.00953674 ... -0.004776   -0.0010376\n",
       "  0.00231934]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00000309\n",
+      " GOC :  0.00000166\n",
       "\n",
-      " GRA :  0.97966915\n",
+      " GRA :  0.07164631\n",
       "\n",
-      " GST :  0.00049880\n",
+      " GST :  0.00000392\n",
       "\n",
-      " GWG :  0.01601271\n",
+      " GWG :  0.40353966\n",
       "\n",
-      " GWC :  0.00381629\n",
+      " GWC :  0.52480853\n",
       "\n",
       "\n",
-      "GUESS:  GRA\n",
+      "GUESS:  GWC\n",
       "[35. 36.]\n",
       "[ 0.00238037  0.00236511  0.00231934 ... -0.00193787  0.0068512\n",
       "  0.00695801]\n",
-      "(50, 21)\n",
+      "(128, 32)\n",
       "PREDICTED VALUES\n",
       "\n",
-      " GOC :  0.00061579\n",
+      " GOC :  0.00000096\n",
       "\n",
-      " GRA :  0.24607882\n",
-      "\n",
-      " GST :  0.00648576\n",
+      " GRA :  0.02603642\n",
       "\n"
      ]
     },
@@ -1196,11 +1184,15 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      " GWG :  0.40232626\n",
+      " GST :  0.00000121\n",
       "\n",
-      " GWC :  0.34449333\n",
-      "GUESS: Nothing\n",
-      "[{'class': 'GWC', 'timestamp': 0}, {'class': 'GRA', 'timestamp': 1}, {'class': 'GWC', 'timestamp': 2}, {'class': 'GWC', 'timestamp': 3}, {'class': 'Nothing', 'timestamp': 4}, {'class': 'GRA', 'timestamp': 5}, {'class': 'GWC', 'timestamp': 6}, {'class': 'GRA', 'timestamp': 7}, {'class': 'GRA', 'timestamp': 8}, {'class': 'GWC', 'timestamp': 9}, {'class': 'GRA', 'timestamp': 10}, {'class': 'GWG', 'timestamp': 11}, {'class': 'Nothing', 'timestamp': 12}, {'class': 'GWG', 'timestamp': 13}, {'class': 'Nothing', 'timestamp': 14}, {'class': 'GWC', 'timestamp': 15}, {'class': 'GRA', 'timestamp': 16}, {'class': 'GRA', 'timestamp': 17}, {'class': 'GRA', 'timestamp': 18}, {'class': 'GRA', 'timestamp': 19}, {'class': 'GRA', 'timestamp': 20}, {'class': 'GRA', 'timestamp': 21}, {'class': 'GRA', 'timestamp': 22}, {'class': 'GRA', 'timestamp': 23}, {'class': 'GRA', 'timestamp': 24}, {'class': 'GRA', 'timestamp': 25}, {'class': 'Nothing', 'timestamp': 26}, {'class': 'Nothing', 'timestamp': 27}, {'class': 'GWC', 'timestamp': 28}, {'class': 'GRA', 'timestamp': 29}, {'class': 'GRA', 'timestamp': 30}, {'class': 'GRA', 'timestamp': 31}, {'class': 'GRA', 'timestamp': 32}, {'class': 'GRA', 'timestamp': 33}, {'class': 'GRA', 'timestamp': 34}, {'class': 'Nothing', 'timestamp': 35}]\n"
+      " GWG :  0.05616241\n",
+      "\n",
+      " GWC :  0.91779894\n",
+      "\n",
+      "\n",
+      "GUESS:  GWC\n",
+      "[{'class': 'GWC', 'timestamp': 0}, {'class': 'GWC', 'timestamp': 1}, {'class': 'GWC', 'timestamp': 2}, {'class': 'GRA', 'timestamp': 3}, {'class': 'GWC', 'timestamp': 4}, {'class': 'GWC', 'timestamp': 5}, {'class': 'GWC', 'timestamp': 6}, {'class': 'GRA', 'timestamp': 7}, {'class': 'GRA', 'timestamp': 8}, {'class': 'GWC', 'timestamp': 9}, {'class': 'GRA', 'timestamp': 10}, {'class': 'GWC', 'timestamp': 11}, {'class': 'Nothing', 'timestamp': 12}, {'class': 'GWC', 'timestamp': 13}, {'class': 'GWC', 'timestamp': 14}, {'class': 'GRA', 'timestamp': 15}, {'class': 'GWC', 'timestamp': 16}, {'class': 'Nothing', 'timestamp': 17}, {'class': 'GRA', 'timestamp': 18}, {'class': 'GRA', 'timestamp': 19}, {'class': 'GRA', 'timestamp': 20}, {'class': 'GRA', 'timestamp': 21}, {'class': 'GRA', 'timestamp': 22}, {'class': 'GRA', 'timestamp': 23}, {'class': 'GRA', 'timestamp': 24}, {'class': 'GRA', 'timestamp': 25}, {'class': 'GWC', 'timestamp': 26}, {'class': 'GWC', 'timestamp': 27}, {'class': 'GWC', 'timestamp': 28}, {'class': 'GRA', 'timestamp': 29}, {'class': 'GRA', 'timestamp': 30}, {'class': 'Nothing', 'timestamp': 31}, {'class': 'GWG', 'timestamp': 32}, {'class': 'GRA', 'timestamp': 33}, {'class': 'GWC', 'timestamp': 34}, {'class': 'GWC', 'timestamp': 35}]\n"
      ]
     }
    ],
diff --git a/geo-cnn/geo_cnn_model.h5 b/geo-cnn/geo_cnn_model.h5
index 04d8060..be79ac9 100644
Binary files a/geo-cnn/geo_cnn_model.h5 and b/geo-cnn/geo_cnn_model.h5 differ
diff --git a/geo_cnn_model.h5 b/geo_cnn_model.h5
index 04d8060..be79ac9 100644
Binary files a/geo_cnn_model.h5 and b/geo_cnn_model.h5 differ
